{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to project root\n",
    "PROJECT_ROOT = Path(\"__file__\").resolve().parent.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add scripts/ to sys.path so bare imports like `from phase1.db import ...` work\n",
    "scripts_path = str(PROJECT_ROOT / \"scripts\")\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "from phase1.build_memories import build_memories\n",
    "from phase1.db import (\n",
    "    rebuild_database,\n",
    "    search_memories,\n",
    "    get_memory_count,\n",
    "    get_random_sample_memories,\n",
    "    DEFAULT_DB_PATH,\n",
    ")\n",
    "from phase1.experiment import (\n",
    "    run_experiment,\n",
    "    run_all_experiments,\n",
    "    DEFAULT_TEST_CASES_DIR,\n",
    "    DEFAULT_RESULTS_DIR,\n",
    ")\n",
    "from common.test_cases import build_test_cases\n",
    "from common.io import load_json\n",
    "\n",
    "# Verify API key\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and experiments will fail.\")\n",
    "else:\n",
    "    print(\"OPENROUTER_API_KEY is set.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration\n",
    "PROMPT_VERSION = \"v2\"                          # \"v1\" or \"v2\"\n",
    "MODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"   # LLM for memory extraction\n",
    "MODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\" # LLM for query generation\n",
    "\n",
    "RAW_DATA_DIR = \"data/review_data\"\n",
    "MEMORIES_DIR = \"data/phase1/memories\"\n",
    "DB_PATH = \"data/phase1/memories/memories.db\"\n",
    "TEST_CASES_DIR = \"data/phase1/test_cases\"\n",
    "RESULTS_DIR = \"data/phase1/results\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"  Model (memories): {MODEL_MEMORIES}\")\n",
    "print(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\n",
    "print(f\"  Raw data dir: {RAW_DATA_DIR}\")\n",
    "print(f\"  Memories dir: {MEMORIES_DIR}\")\n",
    "print(f\"  DB path: {DB_PATH}\")\n",
    "print(f\"  Test cases dir: {TEST_CASES_DIR}\")\n",
    "print(f\"  Results dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words describing the code pattern/issue) and an **actionable lesson** (imperative guidance, max 160 chars).\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Build Memories: Single File\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} raw data files:\")\n",
    "for i, f in enumerate(raw_files):\n",
    "    print(f\"  [{i}] {f.name}\")\n",
    "\n",
    "if raw_files:\n",
    "    # Process the first file (change index to pick a different one)\n",
    "    target_file = raw_files[0]\n",
    "    print(f\"\\nProcessing: {target_file.name}\")\n",
    "    output_path = build_memories(\n",
    "        raw_path=str(target_file),\n",
    "        out_dir=MEMORIES_DIR,\n",
    "        model=MODEL_MEMORIES,\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "    )\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No raw data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Build Memories: All Files\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing all {len(raw_files)} raw data files...\\n\")\n",
    "\n",
    "results = []\n",
    "for f in raw_files:\n",
    "    print(f\"Processing: {f.name}\")\n",
    "    try:\n",
    "        output_path = build_memories(\n",
    "            raw_path=str(f),\n",
    "            out_dir=MEMORIES_DIR,\n",
    "            model=MODEL_MEMORIES,\n",
    "            prompt_version=PROMPT_VERSION,\n",
    "        )\n",
    "        results.append({\"file\": f.name, \"output\": output_path, \"status\": \"ok\"})\n",
    "    except Exception as e:\n",
    "        results.append({\"file\": f.name, \"output\": None, \"status\": str(e)})\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nSummary: {sum(1 for r in results if r['status'] == 'ok')}/{len(results)} files processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rebuild-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Rebuild Database\n",
    "print(\"Rebuilding database...\")\n",
    "rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n",
    "\n",
    "count = get_memory_count(DB_PATH)\n",
    "print(f\"Database rebuilt. Total memories indexed: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Verify Database: Sample Search\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f\"Sample search: \\\"{sample_query}\\\"\\n\")\n",
    "\n",
    "results = search_memories(db_path=DB_PATH, query=sample_query, limit=5)\n",
    "\n",
    "if results:\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"--- Result {i + 1} (distance: {r.get('distance', 'N/A'):.4f}) ---\")\n",
    "        print(f\"  ID: {r.get('id', 'N/A')}\")\n",
    "        print(f\"  Situation: {r.get('situation_description', 'N/A')}\")\n",
    "        print(f\"  Lesson: {r.get('lesson', 'N/A')}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found. Check that the database is populated and Ollama is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-test-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Build Test Cases\n",
    "print(\"Building test cases...\\n\")\n",
    "build_test_cases(\n",
    "    raw_dir=RAW_DATA_DIR,\n",
    "    memories_dir=MEMORIES_DIR,\n",
    "    output_dir=TEST_CASES_DIR,\n",
    ")\n",
    "\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "print(f\"\\nGenerated {len(test_case_files)} test cases:\")\n",
    "for f in test_case_files:\n",
    "    tc = load_json(str(f))\n",
    "    gt_count = tc.get(\"ground_truth_count\", len(tc.get(\"ground_truth_memory_ids\", [])))\n",
    "    print(f\"  {f.name} — {gt_count} ground truth memories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 — Run Experiments\n",
    "\n",
    "For each test case, the experiment:\n",
    "1. Generates search queries from the PR context and diff via LLM\n",
    "2. Runs vector similarity search against the database\n",
    "3. Computes **recall**, **precision**, and **F1** against the ground truth\n",
    "\n",
    "Requires both `OPENROUTER_API_KEY` and Ollama with `mxbai-embed-large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-single-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Run Single Experiment\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "\n",
    "if test_case_files:\n",
    "    target = test_case_files[0]\n",
    "    print(f\"Running experiment on: {target.name}\\n\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        test_case_path=str(target),\n",
    "        db_path=DB_PATH,\n",
    "        model=MODEL_EXPERIMENT,\n",
    "        results_dir=RESULTS_DIR,\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "    )\n",
    "\n",
    "    print(f\"Recall:    {result.get('recall', 'N/A')}\")\n",
    "    print(f\"Precision: {result.get('precision', 'N/A')}\")\n",
    "    print(f\"F1:        {result.get('f1', 'N/A')}\")\n",
    "    print(f\"\\nQueries generated: {len(result.get('queries', []))}\")\n",
    "    print(f\"Unique memories retrieved: {len(result.get('retrieved_memory_ids', []))}\")\n",
    "    print(f\"Ground truth count: {result.get('ground_truth_count', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No test cases found. Run Step 3 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Run All Experiments\n",
    "print(\"Running all experiments...\\n\")\n",
    "\n",
    "all_results = run_all_experiments(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    db_path=DB_PATH,\n",
    "    model=MODEL_EXPERIMENT,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    prompt_version=PROMPT_VERSION,\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} experiments.\\n\")\n",
    "print(f\"{'Test Case':<40} {'Recall':>8} {'Precision':>10} {'F1':>8}\")\n",
    "print(\"-\" * 70)\n",
    "for r in all_results:\n",
    "    name = r.get(\"test_case_id\", r.get(\"test_case_path\", \"?\"))[:40]\n",
    "    print(f\"{name:<40} {r.get('recall', 0):>8.3f} {r.get('precision', 0):>10.3f} {r.get('f1', 0):>8.3f}\")\n",
    "\n",
    "if all_results:\n",
    "    avg_recall = sum(r.get(\"recall\", 0) for r in all_results) / len(all_results)\n",
    "    avg_precision = sum(r.get(\"precision\", 0) for r in all_results) / len(all_results)\n",
    "    avg_f1 = sum(r.get(\"f1\", 0) for r in all_results) / len(all_results)\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'AVERAGE':<40} {avg_recall:>8.3f} {avg_precision:>10.3f} {avg_f1:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Results Summary\n",
    "results_path = Path(RESULTS_DIR)\n",
    "result_files = sorted(results_path.glob(\"*.json\"))\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No result files found. Run experiments first.\")\n",
    "else:\n",
    "    all_data = [load_json(str(f)) for f in result_files]\n",
    "\n",
    "    # Aggregate metrics\n",
    "    recalls = [d.get(\"recall\", 0) for d in all_data]\n",
    "    precisions = [d.get(\"precision\", 0) for d in all_data]\n",
    "    f1s = [d.get(\"f1\", 0) for d in all_data]\n",
    "\n",
    "    print(f\"Results from {len(all_data)} experiments:\\n\")\n",
    "    print(f\"  Avg Recall:    {sum(recalls) / len(recalls):.3f}\")\n",
    "    print(f\"  Avg Precision: {sum(precisions) / len(precisions):.3f}\")\n",
    "    print(f\"  Avg F1:        {sum(f1s) / len(f1s):.3f}\")\n",
    "    print(f\"  Min Recall:    {min(recalls):.3f}\")\n",
    "    print(f\"  Max Recall:    {max(recalls):.3f}\")\n",
    "\n",
    "    # Per-query analysis\n",
    "    all_queries = []\n",
    "    for d in all_data:\n",
    "        for q in d.get(\"queries\", []):\n",
    "            if isinstance(q, str):\n",
    "                all_queries.append(q)\n",
    "            elif isinstance(q, dict):\n",
    "                all_queries.append(q.get(\"query\", \"\"))\n",
    "\n",
    "    if all_queries:\n",
    "        word_counts = [len(q.split()) for q in all_queries]\n",
    "        print(f\"\\nQuery analysis ({len(all_queries)} total queries):\")\n",
    "        print(f\"  Avg words per query: {sum(word_counts) / len(word_counts):.1f}\")\n",
    "        print(f\"  Min words: {min(word_counts)}\")\n",
    "        print(f\"  Max words: {max(word_counts)}\")\n",
    "\n",
    "    # Best / worst experiments by recall\n",
    "    sorted_by_recall = sorted(all_data, key=lambda d: d.get(\"recall\", 0), reverse=True)\n",
    "    print(\"\\nBest experiment by recall:\")\n",
    "    best = sorted_by_recall[0]\n",
    "    print(f\"  {best.get('test_case_id', '?')} — recall: {best.get('recall', 0):.3f}\")\n",
    "\n",
    "    print(\"Worst experiment by recall:\")\n",
    "    worst = sorted_by_recall[-1]\n",
    "    print(f\"  {worst.get('test_case_id', '?')} — recall: {worst.get('recall', 0):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
