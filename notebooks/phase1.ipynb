{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENROUTER_API_KEY is set.\n",
      "Project root: /Users/mayk/Projects/private/crm-memory-retrieval-research\n",
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT == PROJECT_ROOT.parent:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add scripts/ to sys.path so bare imports like `from phase1.db import ...` work\n",
    "scripts_path = str(PROJECT_ROOT / \"scripts\")\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "from phase1.build_memories import build_memories\n",
    "from phase1.db import (\n",
    "    rebuild_database,\n",
    "    search_memories,\n",
    "    get_memory_count,\n",
    "    get_random_sample_memories,\n",
    "    DEFAULT_DB_PATH,\n",
    ")\n",
    "from phase1.experiment import (\n",
    "    run_experiment,\n",
    "    run_all_experiments,\n",
    "    DEFAULT_TEST_CASES_DIR,\n",
    "    DEFAULT_RESULTS_DIR,\n",
    ")\n",
    "from common.test_cases import build_test_cases\n",
    "from common.io import load_json\n",
    "\n",
    "# Verify API key\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and experiments will fail.\")\n",
    "else:\n",
    "    print(\"OPENROUTER_API_KEY is set.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Prompt version: 2.0.0\n",
      "  Model (memories): anthropic/claude-haiku-4.5\n",
      "  Model (experiment): anthropic/claude-sonnet-4.5\n",
      "  Raw data dir: data/review_data\n",
      "  Memories dir: data/phase1/memories\n",
      "  DB path: data/phase1/memories/memories.db\n",
      "  Test cases dir: data/phase1/test_cases\n",
      "  Results dir: data/phase1/results\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Configuration\n",
    "PROMPT_VERSION = \"2.0.0\"                        # semantic versioning\n",
    "MODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"   # LLM for memory extraction\n",
    "MODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\" # LLM for query generation\n",
    "\n",
    "RAW_DATA_DIR = \"data/review_data\"\n",
    "MEMORIES_DIR = \"data/phase1/memories\"\n",
    "DB_PATH = \"data/phase1/memories/memories.db\"\n",
    "TEST_CASES_DIR = \"data/phase1/test_cases\"\n",
    "RESULTS_DIR = \"data/phase1/results\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"  Model (memories): {MODEL_MEMORIES}\")\n",
    "print(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\n",
    "print(f\"  Raw data dir: {RAW_DATA_DIR}\")\n",
    "print(f\"  Memories dir: {MEMORIES_DIR}\")\n",
    "print(f\"  DB path: {DB_PATH}\")\n",
    "print(f\"  Test cases dir: {TEST_CASES_DIR}\")\n",
    "print(f\"  Results dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words describing the code pattern/issue) and an **actionable lesson** (imperative guidance, max 160 chars).\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "build-memories-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 raw data files:\n",
      "No raw data files found.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Build Memories: Single File\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} raw data files:\")\n",
    "for i, f in enumerate(raw_files):\n",
    "    print(f\"  [{i}] {f.name}\")\n",
    "\n",
    "if raw_files:\n",
    "    # Process the first file (change index to pick a different one)\n",
    "    target_file = raw_files[0]\n",
    "    print(f\"\\nProcessing: {target_file.name}\")\n",
    "    output_path = build_memories(\n",
    "        raw_path=str(target_file),\n",
    "        out_dir=MEMORIES_DIR,\n",
    "        model=MODEL_MEMORIES,\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "    )\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No raw data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "build-memories-all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all 11 raw data files...\n",
      "\n",
      "Processing: review_1.json\n",
      "Memories written: 3 -> data/phase1/memories/memories_review_1_20260202_180212.jsonl\n",
      "Rejected: 1 -> data/phase1/memories/rejected_review_1_20260202_180212.jsonl\n",
      "Processing: review_10.json\n",
      "Memories written: 5 -> data/phase1/memories/memories_review_10_20260202_180227.jsonl\n",
      "Rejected: 0 -> data/phase1/memories/rejected_review_10_20260202_180227.jsonl\n",
      "Processing: review_11.json\n",
      "Memories written: 4 -> data/phase1/memories/memories_review_11_20260202_180249.jsonl\n",
      "Rejected: 0 -> data/phase1/memories/rejected_review_11_20260202_180249.jsonl\n",
      "Processing: review_2.json\n",
      "Memories written: 2 -> data/phase1/memories/memories_review_2_20260202_180309.jsonl\n",
      "Rejected: 3 -> data/phase1/memories/rejected_review_2_20260202_180309.jsonl\n",
      "Processing: review_3.json\n",
      "Memories written: 2 -> data/phase1/memories/memories_review_3_20260202_180319.jsonl\n",
      "Rejected: 4 -> data/phase1/memories/rejected_review_3_20260202_180319.jsonl\n",
      "Processing: review_4.json\n",
      "Memories written: 3 -> data/phase1/memories/memories_review_4_20260202_180329.jsonl\n",
      "Rejected: 2 -> data/phase1/memories/rejected_review_4_20260202_180329.jsonl\n",
      "Processing: review_5.json\n",
      "Memories written: 5 -> data/phase1/memories/memories_review_5_20260202_180344.jsonl\n",
      "Rejected: 1 -> data/phase1/memories/rejected_review_5_20260202_180344.jsonl\n",
      "Processing: review_6.json\n",
      "Memories written: 4 -> data/phase1/memories/memories_review_6_20260202_180407.jsonl\n",
      "Rejected: 0 -> data/phase1/memories/rejected_review_6_20260202_180407.jsonl\n",
      "Processing: review_7.json\n",
      "Memories written: 4 -> data/phase1/memories/memories_review_7_20260202_180427.jsonl\n",
      "Rejected: 0 -> data/phase1/memories/rejected_review_7_20260202_180427.jsonl\n",
      "Processing: review_8.json\n",
      "Memories written: 5 -> data/phase1/memories/memories_review_8_20260202_180446.jsonl\n",
      "Rejected: 0 -> data/phase1/memories/rejected_review_8_20260202_180446.jsonl\n",
      "Processing: review_9.json\n",
      "Memories written: 4 -> data/phase1/memories/memories_review_9_20260202_180511.jsonl\n",
      "Rejected: 0 -> data/phase1/memories/rejected_review_9_20260202_180511.jsonl\n",
      "\n",
      "Summary: 11/11 files processed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Build Memories: All Files\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing all {len(raw_files)} raw data files...\\n\")\n",
    "\n",
    "results = []\n",
    "for f in raw_files:\n",
    "    print(f\"Processing: {f.name}\")\n",
    "    try:\n",
    "        output_path = build_memories(\n",
    "            raw_path=str(f),\n",
    "            out_dir=MEMORIES_DIR,\n",
    "            model=MODEL_MEMORIES,\n",
    "            prompt_version=PROMPT_VERSION,\n",
    "        )\n",
    "        results.append({\"file\": f.name, \"output\": output_path, \"status\": \"ok\"})\n",
    "    except Exception as e:\n",
    "        results.append({\"file\": f.name, \"output\": None, \"status\": str(e)})\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nSummary: {sum(1 for r in results if r['status'] == 'ok')}/{len(results)} files processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rebuild-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding database...\n",
      "Creating database at data/phase1/memories/memories.db...\n",
      "Loading memories from data/phase1/memories...\n",
      "Found 41 memories\n",
      "Inserting memories (generating embeddings via Ollama)...\n",
      "  [1/41] Embedding mem_47f0c280ca8f...\n",
      "  [2/41] Embedding mem_52c300d9d6dc...\n",
      "  [3/41] Embedding mem_f29050903e92...\n",
      "  [4/41] Embedding mem_56746712ed52...\n",
      "  [5/41] Embedding mem_b4ad4ce6e957...\n",
      "  [6/41] Embedding mem_d14ca46cce40...\n",
      "  [7/41] Embedding mem_8df897f58579...\n",
      "  [8/41] Embedding mem_819630bee3db...\n",
      "  [9/41] Embedding mem_b09708770b0e...\n",
      "  [10/41] Embedding mem_638ce08683ad...\n",
      "  [11/41] Embedding mem_2d01622ee7e8...\n",
      "  [12/41] Embedding mem_ddb0fa4f5e2c...\n",
      "  [13/41] Embedding mem_bd7e14fb86d7...\n",
      "  [14/41] Embedding mem_13c005e3e7f9...\n",
      "  [15/41] Embedding mem_7847dca3ec16...\n",
      "  [16/41] Embedding mem_5158e1038395...\n",
      "  [17/41] Embedding mem_6f362f1d6808...\n",
      "  [18/41] Embedding mem_eac383ab5d1a...\n",
      "  [19/41] Embedding mem_716ae57343ef...\n",
      "  [20/41] Embedding mem_c2acc67695b5...\n",
      "  [21/41] Embedding mem_9c08b6b6a581...\n",
      "  [22/41] Embedding mem_788f1dcfbb8c...\n",
      "  [23/41] Embedding mem_c29a37674a8d...\n",
      "  [24/41] Embedding mem_a5cf71c32e60...\n",
      "  [25/41] Embedding mem_3ee4e60637bc...\n",
      "  [26/41] Embedding mem_eb24d7ff7c4d...\n",
      "  [27/41] Embedding mem_5310e451f75f...\n",
      "  [28/41] Embedding mem_271942fa5cc8...\n",
      "  [29/41] Embedding mem_d99cc0989017...\n",
      "  [30/41] Embedding mem_337f95b5d490...\n",
      "  [31/41] Embedding mem_80962cfd2f56...\n",
      "  [32/41] Embedding mem_45341f89a411...\n",
      "  [33/41] Embedding mem_3fe166a3b523...\n",
      "  [34/41] Embedding mem_bb5267a2e339...\n",
      "  [35/41] Embedding mem_dec07d36f54c...\n",
      "  [36/41] Embedding mem_d595320d70d3...\n",
      "  [37/41] Embedding mem_cc83c544db09...\n",
      "  [38/41] Embedding mem_77b5f52ca49d...\n",
      "  [39/41] Embedding mem_a87067e30c33...\n",
      "  [40/41] Embedding mem_0171a029889c...\n",
      "  [41/41] Embedding mem_4c83492fe063...\n",
      "Inserted 41 memories into database\n",
      "Database rebuilt. Total memories indexed: 41\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Rebuild Database\n",
    "print(\"Rebuilding database...\")\n",
    "rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n",
    "\n",
    "count = get_memory_count(DB_PATH)\n",
    "print(f\"Database rebuilt. Total memories indexed: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample search: \"error handling in async functions\"\n",
      "\n",
      "--- Result 1 (distance: 0.7539) ---\n",
      "  ID: mem_d595320d70d3\n",
      "  Situation: Form submission handler with try-catch error handling. Error state persists across retries; missing `setError(null)` at operation start, causing stale error messages to display after successful subsequent attempts.\n",
      "  Lesson: Clear error state at the start of async operations to prevent stale errors from persisting after successful retries.\n",
      "\n",
      "--- Result 2 (distance: 0.8491) ---\n",
      "  ID: mem_3ee4e60637bc\n",
      "  Situation: Service method executing async database insert within loop. Missing `await` on `db.execute()` call for revoked token insertion; synchronous execution blocks async flow while commit awaits properly.\n",
      "  Lesson: Always await async function calls in loops; missing await silently creates unawaited coroutines that never execute.\n",
      "\n",
      "--- Result 3 (distance: 0.8565) ---\n",
      "  ID: mem_9c08b6b6a581\n",
      "  Situation: Exception filter catching specific exception type with optional chaining on context object. Accesses context.data property without verifying it matches expected structure; conditional check for truthiness insufficient for safe property access in error mapping.\n",
      "  Lesson: Verify exception context properties match expected types before accessing nested attributes to prevent runtime errors in error handlers.\n",
      "\n",
      "--- Result 4 (distance: 0.8737) ---\n",
      "  ID: mem_3fe166a3b523\n",
      "  Situation: useEffect hook fetching data without AbortController. Missing cleanup function to cancel in-flight requests on component unmount, allowing state updates on unmounted component when async response resolves after unmount.\n",
      "  Lesson: Always cancel in-flight requests in useEffect cleanup to prevent state updates on unmounted components.\n",
      "\n",
      "--- Result 5 (distance: 0.8794) ---\n",
      "  ID: mem_47f0c280ca8f\n",
      "  Situation: Command-line tool function opening file with unwrap() on Result. Error handling bypassed for file I/O operation; panics instead of propagating error through Result type, preventing graceful error reporting to user.\n",
      "  Lesson: Replace unwrap() with the ? operator on Result types to propagate errors for graceful handling instead of panicking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Verify Database: Sample Search\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f\"Sample search: \\\"{sample_query}\\\"\\n\")\n",
    "\n",
    "results = search_memories(db_path=DB_PATH, query=sample_query, limit=5)\n",
    "\n",
    "if results:\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"--- Result {i + 1} (distance: {r.get('distance', 'N/A'):.4f}) ---\")\n",
    "        print(f\"  ID: {r.get('id', 'N/A')}\")\n",
    "        print(f\"  Situation: {r.get('situation_description', 'N/A')}\")\n",
    "        print(f\"  Lesson: {r.get('lesson', 'N/A')}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found. Check that the database is populated and Ollama is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "build-test-cases",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building test cases...\n",
      "\n",
      "Loading all memories...\n",
      "Loaded 41 memories\n",
      "[1/11] Processing review_1.json...\n",
      "  Created test case with 3 ground truth memories\n",
      "[2/11] Processing review_10.json...\n",
      "  Created test case with 5 ground truth memories\n",
      "[3/11] Processing review_11.json...\n",
      "  Created test case with 4 ground truth memories\n",
      "[4/11] Processing review_2.json...\n",
      "  Created test case with 2 ground truth memories\n",
      "[5/11] Processing review_3.json...\n",
      "  Created test case with 2 ground truth memories\n",
      "[6/11] Processing review_4.json...\n",
      "  Created test case with 3 ground truth memories\n",
      "[7/11] Processing review_5.json...\n",
      "  Created test case with 5 ground truth memories\n",
      "[8/11] Processing review_6.json...\n",
      "  Created test case with 4 ground truth memories\n",
      "[9/11] Processing review_7.json...\n",
      "  Created test case with 4 ground truth memories\n",
      "[10/11] Processing review_8.json...\n",
      "  Created test case with 5 ground truth memories\n",
      "[11/11] Processing review_9.json...\n",
      "  Created test case with 4 ground truth memories\n",
      "\n",
      "============================================================\n",
      "TEST CASE GENERATION SUMMARY\n",
      "============================================================\n",
      "Test cases created: 11\n",
      "PRs skipped (no ground truth): 0\n",
      "Output directory: data/phase1/test_cases\n",
      "============================================================\n",
      "\n",
      "Generated 11 test cases:\n",
      "  review_1.json — 3 ground truth memories\n",
      "  review_10.json — 5 ground truth memories\n",
      "  review_11.json — 4 ground truth memories\n",
      "  review_2.json — 2 ground truth memories\n",
      "  review_3.json — 2 ground truth memories\n",
      "  review_4.json — 3 ground truth memories\n",
      "  review_5.json — 5 ground truth memories\n",
      "  review_6.json — 4 ground truth memories\n",
      "  review_7.json — 4 ground truth memories\n",
      "  review_8.json — 5 ground truth memories\n",
      "  review_9.json — 4 ground truth memories\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — Build Test Cases\n",
    "print(\"Building test cases...\\n\")\n",
    "build_test_cases(\n",
    "    raw_dir=RAW_DATA_DIR,\n",
    "    memories_dir=MEMORIES_DIR,\n",
    "    output_dir=TEST_CASES_DIR,\n",
    ")\n",
    "\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "print(f\"\\nGenerated {len(test_case_files)} test cases:\")\n",
    "for f in test_case_files:\n",
    "    tc = load_json(str(f))\n",
    "    gt_count = tc.get(\"ground_truth_count\", len(tc.get(\"ground_truth_memory_ids\", [])))\n",
    "    print(f\"  {f.name} — {gt_count} ground truth memories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 — Run Experiments\n",
    "\n",
    "For each test case, the experiment:\n",
    "1. Generates search queries from the PR context and diff via LLM\n",
    "2. Runs vector similarity search against the database\n",
    "3. Computes **recall**, **precision**, and **F1** against the ground truth\n",
    "\n",
    "Requires both `OPENROUTER_API_KEY` and Ollama with `mxbai-embed-large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-single-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Run Single Experiment\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "\n",
    "if test_case_files:\n",
    "    target = test_case_files[0]\n",
    "    print(f\"Running experiment on: {target.name}\\n\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        test_case_path=str(target),\n",
    "        db_path=DB_PATH,\n",
    "        model=MODEL_EXPERIMENT,\n",
    "        results_dir=RESULTS_DIR,\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "    )\n",
    "\n",
    "    metrics = result.get(\"metrics\", {})\n",
    "    gt = result.get(\"ground_truth\", {})\n",
    "    print(f\"Recall:    {metrics.get('recall', 'N/A')}\")\n",
    "    print(f\"Precision: {metrics.get('precision', 'N/A')}\")\n",
    "    print(f\"F1:        {metrics.get('f1', 'N/A')}\")\n",
    "    print(f\"\\nQueries generated: {metrics.get('total_queries', 'N/A')}\")\n",
    "    print(f\"Unique memories retrieved: {metrics.get('total_unique_retrieved', 'N/A')}\")\n",
    "    print(f\"Within threshold: {metrics.get('total_within_threshold', 'N/A')}\")\n",
    "    print(f\"Ground truth count: {gt.get('count', 'N/A')}\")\n",
    "    print(f\"Ground truth retrieved: {metrics.get('ground_truth_retrieved', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No test cases found. Run Step 3 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all experiments...\n",
      "\n",
      "\n",
      "[1/11] Processing review_1.json\n",
      "Test case: tc_review_1\n",
      "Diff stats: 18569 -> 18569 chars\n",
      "Ground truth memories: 3\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 9 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_1\n",
      "============================================================\n",
      "Source: review_1.json\n",
      "PR: bugfix/VAPI-766 -> release/v1.32.1\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 9\n",
      "Avg query length: 32.0 words\n",
      "Ground truth memories: 3\n",
      "Retrieved (unique, all): 36\n",
      "Retrieved (within threshold 1.1): 36\n",
      "Ground truth retrieved: 3\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 8.3%\n",
      "  F1 Score:  0.154\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_1_exp_20260202_180652.json\n",
      "\n",
      "[2/11] Processing review_10.json\n",
      "Test case: tc_review_10\n",
      "Diff stats: 5475 -> 5475 chars\n",
      "Ground truth memories: 5\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 8 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_10\n",
      "============================================================\n",
      "Source: review_10.json\n",
      "PR: feature/analyze-subcommand -> main\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 8\n",
      "Avg query length: 31.6 words\n",
      "Ground truth memories: 5\n",
      "Retrieved (unique, all): 35\n",
      "Retrieved (within threshold 1.1): 35\n",
      "Ground truth retrieved: 5\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 14.3%\n",
      "  F1 Score:  0.250\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_10_exp_20260202_180709.json\n",
      "\n",
      "[3/11] Processing review_11.json\n",
      "Test case: tc_review_11\n",
      "Diff stats: 5369 -> 5369 chars\n",
      "Ground truth memories: 4\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 9 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_11\n",
      "============================================================\n",
      "Source: review_11.json\n",
      "PR: fix/timezone-handling-etl -> main\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 9\n",
      "Avg query length: 26.1 words\n",
      "Ground truth memories: 4\n",
      "Retrieved (unique, all): 37\n",
      "Retrieved (within threshold 1.1): 37\n",
      "Ground truth retrieved: 4\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 10.8%\n",
      "  F1 Score:  0.195\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_11_exp_20260202_180723.json\n",
      "\n",
      "[4/11] Processing review_2.json\n",
      "Test case: tc_review_2\n",
      "Diff stats: 31476 -> 31476 chars\n",
      "Ground truth memories: 2\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 8 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_2\n",
      "============================================================\n",
      "Source: review_2.json\n",
      "PR: feature/VAPI-561-be-datasource-mock-planningscope -> release/v1.33.0\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 8\n",
      "Avg query length: 33.1 words\n",
      "Ground truth memories: 2\n",
      "Retrieved (unique, all): 35\n",
      "Retrieved (within threshold 1.1): 35\n",
      "Ground truth retrieved: 2\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 5.7%\n",
      "  F1 Score:  0.108\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_2_exp_20260202_180739.json\n",
      "\n",
      "[5/11] Processing review_3.json\n",
      "Test case: tc_review_3\n",
      "Diff stats: 12020 -> 12020 chars\n",
      "Ground truth memories: 2\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 8 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_3\n",
      "============================================================\n",
      "Source: review_3.json\n",
      "PR: feature/VAPI-652 -> release/v1.33.0\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 8\n",
      "Avg query length: 33.6 words\n",
      "Ground truth memories: 2\n",
      "Retrieved (unique, all): 37\n",
      "Retrieved (within threshold 1.1): 37\n",
      "Ground truth retrieved: 2\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 5.4%\n",
      "  F1 Score:  0.103\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_3_exp_20260202_180755.json\n",
      "\n",
      "[6/11] Processing review_4.json\n",
      "Test case: tc_review_4\n",
      "Diff stats: 73404 -> 73404 chars\n",
      "Ground truth memories: 3\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 10 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_4\n",
      "============================================================\n",
      "Source: review_4.json\n",
      "PR: feature/VAPI-724 -> release/v1.33.0\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 10\n",
      "Avg query length: 27.7 words\n",
      "Ground truth memories: 3\n",
      "Retrieved (unique, all): 33\n",
      "Retrieved (within threshold 1.1): 33\n",
      "Ground truth retrieved: 3\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 9.1%\n",
      "  F1 Score:  0.167\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_4_exp_20260202_180811.json\n",
      "\n",
      "[7/11] Processing review_5.json\n",
      "Test case: tc_review_5\n",
      "Diff stats: 195936 -> 195936 chars\n",
      "Ground truth memories: 5\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 10 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_5\n",
      "============================================================\n",
      "Source: review_5.json\n",
      "PR: feature/VAPI-732-be-multiple-auth-strategies-on-endpoint-8h -> release/v1.33.0\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 10\n",
      "Avg query length: 31.0 words\n",
      "Ground truth memories: 5\n",
      "Retrieved (unique, all): 39\n",
      "Retrieved (within threshold 1.1): 39\n",
      "Ground truth retrieved: 5\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 12.8%\n",
      "  F1 Score:  0.227\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_5_exp_20260202_180830.json\n",
      "\n",
      "[8/11] Processing review_6.json\n",
      "Test case: tc_review_6\n",
      "Diff stats: 6474 -> 6474 chars\n",
      "Ground truth memories: 4\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 8 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_6\n",
      "============================================================\n",
      "Source: review_6.json\n",
      "PR: fix/jwt-refresh-token-invalidation -> main\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 8\n",
      "Avg query length: 27.4 words\n",
      "Ground truth memories: 4\n",
      "Retrieved (unique, all): 36\n",
      "Retrieved (within threshold 1.1): 36\n",
      "Ground truth retrieved: 4\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 11.1%\n",
      "  F1 Score:  0.200\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_6_exp_20260202_180843.json\n",
      "\n",
      "[9/11] Processing review_7.json\n",
      "Test case: tc_review_7\n",
      "Diff stats: 3955 -> 3955 chars\n",
      "Ground truth memories: 4\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 8 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_7\n",
      "============================================================\n",
      "Source: review_7.json\n",
      "PR: fix/cache-race-condition -> main\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 8\n",
      "Avg query length: 30.9 words\n",
      "Ground truth memories: 4\n",
      "Retrieved (unique, all): 36\n",
      "Retrieved (within threshold 1.1): 36\n",
      "Ground truth retrieved: 4\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 11.1%\n",
      "  F1 Score:  0.200\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_7_exp_20260202_180858.json\n",
      "\n",
      "[10/11] Processing review_8.json\n",
      "Test case: tc_review_8\n",
      "Diff stats: 5291 -> 5291 chars\n",
      "Ground truth memories: 5\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 9 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_8\n",
      "============================================================\n",
      "Source: review_8.json\n",
      "PR: feature/user-settings-form -> develop\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 9\n",
      "Avg query length: 30.1 words\n",
      "Ground truth memories: 5\n",
      "Retrieved (unique, all): 36\n",
      "Retrieved (within threshold 1.1): 36\n",
      "Ground truth retrieved: 5\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 13.9%\n",
      "  F1 Score:  0.244\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_8_exp_20260202_180915.json\n",
      "\n",
      "[11/11] Processing review_9.json\n",
      "Test case: tc_review_9\n",
      "Diff stats: 6819 -> 6819 chars\n",
      "Ground truth memories: 4\n",
      "Using prompt memory-query/v2.0.0 with 5 sample memories\n",
      "Generating queries via anthropic/claude-sonnet-4.5...\n",
      "Generated 10 queries\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT RESULTS: tc_review_9\n",
      "============================================================\n",
      "Source: review_9.json\n",
      "PR: feature/redis-product-caching -> develop\n",
      "Model: anthropic/claude-sonnet-4.5\n",
      "Prompt version: memory-query/v2.0.0\n",
      "Queries generated: 10\n",
      "Avg query length: 33.8 words\n",
      "Ground truth memories: 4\n",
      "Retrieved (unique, all): 36\n",
      "Retrieved (within threshold 1.1): 36\n",
      "Ground truth retrieved: 4\n",
      "\n",
      "METRICS (threshold=1.1):\n",
      "  Recall:    100.0%\n",
      "  Precision: 11.1%\n",
      "  F1 Score:  0.200\n",
      "  Query hit rate: 100.0%\n",
      "============================================================\n",
      "\n",
      "Results saved to: data/phase1/results/results_tc_review_9_exp_20260202_180936.json\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "Prompt version: 2.0.0\n",
      "Distance threshold: 1.1\n",
      "Experiments run: 11\n",
      "Total ground truth memories: 41\n",
      "Total retrieved: 41\n",
      "\n",
      "AGGREGATE METRICS (threshold=1.1):\n",
      "  Average recall:    100.0%\n",
      "  Average precision: 10.3%\n",
      "  Average F1:        0.186\n",
      "\n",
      "Completed 11 experiments.\n",
      "\n",
      "Test Case                                  Recall  Precision       F1\n",
      "----------------------------------------------------------------------\n",
      "tc_review_1                                 1.000      0.083    0.154\n",
      "tc_review_10                                1.000      0.143    0.250\n",
      "tc_review_11                                1.000      0.108    0.195\n",
      "tc_review_2                                 1.000      0.057    0.108\n",
      "tc_review_3                                 1.000      0.054    0.103\n",
      "tc_review_4                                 1.000      0.091    0.167\n",
      "tc_review_5                                 1.000      0.128    0.227\n",
      "tc_review_6                                 1.000      0.111    0.200\n",
      "tc_review_7                                 1.000      0.111    0.200\n",
      "tc_review_8                                 1.000      0.139    0.244\n",
      "tc_review_9                                 1.000      0.111    0.200\n",
      "----------------------------------------------------------------------\n",
      "AVERAGE                                     1.000      0.103    0.186\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 — Run All Experiments\n",
    "print(\"Running all experiments...\\n\")\n",
    "\n",
    "all_results = run_all_experiments(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    db_path=DB_PATH,\n",
    "    model=MODEL_EXPERIMENT,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    prompt_version=PROMPT_VERSION,\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} experiments.\\n\")\n",
    "print(f\"{'Test Case':<40} {'Recall':>8} {'Precision':>10} {'F1':>8}\")\n",
    "print(\"-\" * 70)\n",
    "for r in all_results:\n",
    "    name = r.get(\"test_case_id\", \"?\")[:40]\n",
    "    m = r.get(\"metrics\", {})\n",
    "    print(f\"{name:<40} {m.get('recall', 0):>8.3f} {m.get('precision', 0):>10.3f} {m.get('f1', 0):>8.3f}\")\n",
    "\n",
    "if all_results:\n",
    "    metrics_list = [r.get(\"metrics\", {}) for r in all_results]\n",
    "    avg_recall = sum(m.get(\"recall\", 0) for m in metrics_list) / len(metrics_list)\n",
    "    avg_precision = sum(m.get(\"precision\", 0) for m in metrics_list) / len(metrics_list)\n",
    "    avg_f1 = sum(m.get(\"f1\", 0) for m in metrics_list) / len(metrics_list)\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'AVERAGE':<40} {avg_recall:>8.3f} {avg_precision:>10.3f} {avg_f1:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-summary",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 14 — Results Analysis: Top@K & Distance Threshold Sweep (with MRR)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# === Configuration ===\nDISTANCE_THRESHOLDS = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.10]\nMAX_K = 20\n\nresults_path = Path(RESULTS_DIR)\nresult_files = sorted(results_path.glob(\"*.json\"))\n\nif not result_files:\n    print(\"No result files found. Run experiments first.\")\nelse:\n    all_data = [load_json(str(f)) for f in result_files]\n    n_cases = len(all_data)\n\n    # --- Helper: deduplicate results per test case (best distance per memory) ---\n    def get_best_distances(data):\n        best = {}\n        for qr in data.get(\"queries\", []):\n            for r in qr.get(\"results\", []):\n                mid = r[\"id\"]\n                d = r[\"distance\"]\n                if mid not in best or d < best[mid]:\n                    best[mid] = d\n        return sorted(best.items(), key=lambda x: x[1])\n\n    # --- Helper: compute reciprocal rank for a ranked list ---\n    def reciprocal_rank(sorted_ids, gt_ids):\n        \"\"\"Return 1/rank of the first GT hit, or 0 if none found.\"\"\"\n        for rank, mid in enumerate(sorted_ids, 1):\n            if mid in gt_ids:\n                return 1.0 / rank\n        return 0.0\n\n    # =========================================================================\n    # Top@K Analysis (per-test-case: pool all queries, deduplicate, take top k)\n    # =========================================================================\n    k_values = list(range(1, MAX_K + 1))\n    all_precisions = {k: [] for k in k_values}\n    all_recalls = {k: [] for k in k_values}\n    all_f1s = {k: [] for k in k_values}\n    all_rrs = {k: [] for k in k_values}  # reciprocal ranks for MRR@k\n\n    for data in all_data:\n        gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n        gt_count = len(gt_ids)\n        sorted_memories = get_best_distances(data)\n\n        for k in k_values:\n            top_k = sorted_memories[:k]\n            top_k_ids = {mid for mid, _ in top_k}\n            gt_in_top_k = len(top_k_ids & gt_ids)\n            actual_k = min(k, len(sorted_memories))\n\n            p = gt_in_top_k / actual_k if actual_k > 0 else 0.0\n            r = gt_in_top_k / gt_count if gt_count > 0 else 0.0\n            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n            rr = reciprocal_rank([mid for mid, _ in top_k], gt_ids)\n\n            all_precisions[k].append(p)\n            all_recalls[k].append(r)\n            all_f1s[k].append(f1)\n            all_rrs[k].append(rr)\n\n    avg_p = [np.mean(all_precisions[k]) for k in k_values]\n    avg_r = [np.mean(all_recalls[k]) for k in k_values]\n    avg_f1 = [np.mean(all_f1s[k]) for k in k_values]\n    avg_mrr = [np.mean(all_rrs[k]) for k in k_values]\n\n    # Top@K table\n    print(f\"Top@K Results (averaged over {n_cases} test cases)\\n\")\n    print(f\"{'K':>4} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8}\")\n    print(\"-\" * 42)\n    for k in [1, 2, 3, 5, 10, 15, 20]:\n        if k <= MAX_K:\n            i = k - 1\n            print(f\"{k:>4} {avg_p[i]:>10.3f} {avg_r[i]:>8.3f} {avg_f1[i]:>8.3f} {avg_mrr[i]:>8.3f}\")\n\n    # Top@K plot — single axis\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(k_values, avg_p, label=\"Precision\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=4)\n    ax.plot(k_values, avg_r, label=\"Recall\", color=\"#2ecc71\", linewidth=2, marker=\"s\", markersize=4)\n    ax.plot(k_values, avg_f1, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=4)\n    ax.plot(k_values, avg_mrr, label=\"MRR@K\", color=\"#e67e22\", linewidth=2, marker=\"D\", markersize=5)\n    ax.set_xlabel(\"K (top results kept per test case)\")\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"Precision / Recall / F1 / MRR vs Top@K (avg over {n_cases} test cases)\")\n    ax.set_xticks(k_values)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    for k in [1, 5, 10]:\n        i = k - 1\n        ax.annotate(f\"MRR={avg_mrr[i]:.3f}\", (k, avg_mrr[i]),\n                     textcoords=\"offset points\", xytext=(8, 8), fontsize=9,\n                     arrowprops=dict(arrowstyle=\"->\", color=\"#e67e22\", lw=0.8))\n    plt.tight_layout()\n    plt.show()\n\n    # =========================================================================\n    # Distance Threshold Sweep (no top@k limit, uses all deduplicated results)\n    # =========================================================================\n    print(f\"\\nDistance Threshold Results (averaged over {n_cases} test cases)\\n\")\n    print(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8}\")\n    print(\"-\" * 48)\n\n    sweep_p, sweep_r, sweep_f1, sweep_mrr = [], [], [], []\n    for threshold in DISTANCE_THRESHOLDS:\n        t_p, t_r, t_f1, t_rr = [], [], [], []\n        for data in all_data:\n            gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n            gt_count = len(gt_ids)\n            sorted_memories = get_best_distances(data)\n\n            accepted = [(mid, d) for mid, d in sorted_memories if d <= threshold]\n            accepted_ids = {mid for mid, _ in accepted}\n            gt_accepted = len(accepted_ids & gt_ids)\n            n_accepted = len(accepted_ids)\n\n            p = gt_accepted / n_accepted if n_accepted > 0 else 0.0\n            r = gt_accepted / gt_count if gt_count > 0 else 0.0\n            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n            rr = reciprocal_rank([mid for mid, _ in accepted], gt_ids)\n\n            t_p.append(p)\n            t_r.append(r)\n            t_f1.append(f1)\n            t_rr.append(rr)\n\n        sweep_p.append(np.mean(t_p))\n        sweep_r.append(np.mean(t_r))\n        sweep_f1.append(np.mean(t_f1))\n        sweep_mrr.append(np.mean(t_rr))\n        print(f\"{threshold:>10.2f} {sweep_p[-1]:>10.3f} {sweep_r[-1]:>8.3f} {sweep_f1[-1]:>8.3f} {sweep_mrr[-1]:>8.3f}\")\n\n    # Threshold sweep plot — single axis\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(DISTANCE_THRESHOLDS, sweep_p, label=\"Precision\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=5)\n    ax.plot(DISTANCE_THRESHOLDS, sweep_r, label=\"Recall\", color=\"#2ecc71\", linewidth=2, marker=\"s\", markersize=5)\n    ax.plot(DISTANCE_THRESHOLDS, sweep_f1, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=5)\n    ax.plot(DISTANCE_THRESHOLDS, sweep_mrr, label=\"MRR\", color=\"#e67e22\", linewidth=2, marker=\"D\", markersize=5)\n    ax.set_xlabel(\"Cosine Distance Threshold\")\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"Precision / Recall / F1 / MRR vs Distance Threshold (avg over {n_cases} test cases)\")\n    ax.set_xticks(DISTANCE_THRESHOLDS)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}