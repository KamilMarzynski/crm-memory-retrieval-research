{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Phase 1: Distance Threshold Analysis\n\nAnalyzes experiment results to determine an optimal cosine distance cutoff for filtering\nretrieved memories. The goal is to increase **precision** (reduce false positives) while\nmaintaining acceptable **recall** (not losing ground truth memories).\n\n**Two analysis levels:**\n1. **Per-result level** — every `(distance, is_ground_truth)` data point from query results\n2. **Per-memory level** — deduplicated by memory ID, using the *best* (minimum) distance across all queries. This reflects how the system actually works: a memory is \"found\" if *any* query retrieves it.\n\n**Approach:**\n1. Collect distance data at both levels\n2. Visualize distributions (ground truth vs non-ground-truth)\n3. Sweep thresholds and compute precision/recall/F1\n4. Identify the optimal cutoff and its tradeoffs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "RESULTS_DIR = Path(\"../data/phase1/results\")\n",
    "\n",
    "# Load all experiment result files\n",
    "result_files = sorted(RESULTS_DIR.glob(\"results_*.json\"))\n",
    "print(f\"Found {len(result_files)} result files:\")\n",
    "for f in result_files:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all (distance, is_ground_truth) pairs from every query result\n",
    "gt_distances = []       # distances where result IS ground truth\n",
    "non_gt_distances = []   # distances where result is NOT ground truth\n",
    "all_points = []         # (distance, is_gt, test_case_id, query_index)\n",
    "\n",
    "experiments = []\n",
    "for f in result_files:\n",
    "    with open(f) as fh:\n",
    "        data = json.load(fh)\n",
    "    experiments.append(data)\n",
    "    tc_id = data.get(\"test_case_id\", f.stem)\n",
    "\n",
    "    for qi, qr in enumerate(data.get(\"queries\", [])):\n",
    "        for r in qr.get(\"results\", []):\n",
    "            d = r[\"distance\"]\n",
    "            is_gt = r[\"is_ground_truth\"]\n",
    "            all_points.append((d, is_gt, tc_id, qi))\n",
    "            if is_gt:\n",
    "                gt_distances.append(d)\n",
    "            else:\n",
    "                non_gt_distances.append(d)\n",
    "\n",
    "gt_distances = np.array(gt_distances)\n",
    "non_gt_distances = np.array(non_gt_distances)\n",
    "\n",
    "print(f\"Total data points: {len(all_points)}\")\n",
    "print(f\"  Ground truth hits: {len(gt_distances)}\")\n",
    "print(f\"  Non-ground-truth:  {len(non_gt_distances)}\")\n",
    "print(f\"\\nGround truth distance range: [{gt_distances.min():.4f}, {gt_distances.max():.4f}]\")\n",
    "print(f\"Non-GT distance range:       [{non_gt_distances.min():.4f}, {non_gt_distances.max():.4f}]\")\n",
    "print(f\"\\nGround truth mean: {gt_distances.mean():.4f}, median: {np.median(gt_distances):.4f}\")\n",
    "print(f\"Non-GT mean:       {non_gt_distances.mean():.4f}, median: {np.median(non_gt_distances):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "histogram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overlaid histograms of distance distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: overlaid histograms\n",
    "bins = np.arange(0.5, 1.3, 0.025)\n",
    "axes[0].hist(gt_distances, bins=bins, alpha=0.6, label=f\"Ground Truth (n={len(gt_distances)})\",\n",
    "             color=\"#2ecc71\", edgecolor=\"white\", linewidth=0.5)\n",
    "axes[0].hist(non_gt_distances, bins=bins, alpha=0.6, label=f\"Non-GT (n={len(non_gt_distances)})\",\n",
    "             color=\"#e74c3c\", edgecolor=\"white\", linewidth=0.5)\n",
    "axes[0].axvline(np.median(gt_distances), color=\"#27ae60\", linestyle=\"--\", linewidth=1.5,\n",
    "                label=f\"GT median: {np.median(gt_distances):.3f}\")\n",
    "axes[0].axvline(np.median(non_gt_distances), color=\"#c0392b\", linestyle=\"--\", linewidth=1.5,\n",
    "                label=f\"Non-GT median: {np.median(non_gt_distances):.3f}\")\n",
    "axes[0].set_xlabel(\"Cosine Distance\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distance Distribution: Ground Truth vs Non-GT\")\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Right: KDE-style comparison using normalized histograms\n",
    "axes[1].hist(gt_distances, bins=bins, alpha=0.6, density=True,\n",
    "             label=\"Ground Truth\", color=\"#2ecc71\", edgecolor=\"white\", linewidth=0.5)\n",
    "axes[1].hist(non_gt_distances, bins=bins, alpha=0.6, density=True,\n",
    "             label=\"Non-GT\", color=\"#e74c3c\", edgecolor=\"white\", linewidth=0.5)\n",
    "axes[1].set_xlabel(\"Cosine Distance\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].set_title(\"Normalized Distribution Comparison\")\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep distance thresholds and compute precision/recall/F1\n",
    "# A result is \"accepted\" if distance <= threshold\n",
    "# Precision = GT accepted / total accepted\n",
    "# Recall = GT accepted / total GT\n",
    "\n",
    "thresholds = np.arange(0.50, 1.25, 0.01)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "n_accepted = []\n",
    "\n",
    "total_gt = len(gt_distances)\n",
    "\n",
    "for t in thresholds:\n",
    "    gt_accepted = np.sum(gt_distances <= t)\n",
    "    non_gt_accepted = np.sum(non_gt_distances <= t)\n",
    "    total_accepted = gt_accepted + non_gt_accepted\n",
    "\n",
    "    p = gt_accepted / total_accepted if total_accepted > 0 else 1.0\n",
    "    r = gt_accepted / total_gt if total_gt > 0 else 0.0\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1_scores.append(f1)\n",
    "    n_accepted.append(total_accepted)\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "f1_scores = np.array(f1_scores)\n",
    "n_accepted = np.array(n_accepted)\n",
    "\n",
    "# Find optimal F1 threshold\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_f1_idx]\n",
    "print(f\"Optimal F1 threshold: {best_threshold:.2f}\")\n",
    "print(f\"  F1:        {f1_scores[best_f1_idx]:.3f}\")\n",
    "print(f\"  Precision: {precisions[best_f1_idx]:.3f}\")\n",
    "print(f\"  Recall:    {recalls[best_f1_idx]:.3f}\")\n",
    "print(f\"  Accepted:  {n_accepted[best_f1_idx]} / {len(all_points)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pr-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision/Recall/F1 vs threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: P/R/F1 vs threshold\n",
    "axes[0].plot(thresholds, precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\n",
    "axes[0].plot(thresholds, recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\n",
    "axes[0].plot(thresholds, f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\n",
    "axes[0].axvline(best_threshold, color=\"#e74c3c\", linestyle=\"--\", linewidth=1.5,\n",
    "                label=f\"Best F1 @ {best_threshold:.2f}\")\n",
    "axes[0].set_xlabel(\"Distance Threshold\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_title(\"Precision / Recall / F1 vs Distance Threshold\")\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Precision-Recall curve\n",
    "axes[1].plot(recalls, precisions, color=\"#8e44ad\", linewidth=2)\n",
    "axes[1].scatter([recalls[best_f1_idx]], [precisions[best_f1_idx]],\n",
    "               color=\"#e74c3c\", s=100, zorder=5,\n",
    "               label=f\"Best F1: P={precisions[best_f1_idx]:.2f}, R={recalls[best_f1_idx]:.2f}\")\n",
    "# Annotate a few threshold values along the curve\n",
    "for t_val in [0.70, 0.80, 0.90, 1.00]:\n",
    "    idx = np.argmin(np.abs(thresholds - t_val))\n",
    "    axes[1].annotate(f\"t={t_val:.2f}\",\n",
    "                     xy=(recalls[idx], precisions[idx]),\n",
    "                     fontsize=8, ha=\"left\",\n",
    "                     xytext=(5, 5), textcoords=\"offset points\")\n",
    "    axes[1].scatter([recalls[idx]], [precisions[idx]], color=\"gray\", s=30, zorder=4)\n",
    "\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Precision-Recall Curve (by distance threshold)\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1.05)\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed table of metrics at key thresholds\n",
    "key_thresholds = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.10, 1.20]\n",
    "\n",
    "print(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'Accepted':>10} {'GT Kept':>8} {'GT Lost':>8}\")\n",
    "print(\"-\" * 74)\n",
    "\n",
    "for t in key_thresholds:\n",
    "    gt_accepted = int(np.sum(gt_distances <= t))\n",
    "    non_gt_accepted = int(np.sum(non_gt_distances <= t))\n",
    "    total_accepted = gt_accepted + non_gt_accepted\n",
    "\n",
    "    p = gt_accepted / total_accepted if total_accepted > 0 else 1.0\n",
    "    r = gt_accepted / total_gt if total_gt > 0 else 0.0\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "    gt_lost = total_gt - gt_accepted\n",
    "    marker = \" <-- best F1\" if abs(t - best_threshold) < 0.005 else \"\"\n",
    "    print(f\"{t:>10.2f} {p:>10.3f} {r:>8.3f} {f1:>8.3f} {total_accepted:>10} {gt_accepted:>8} {gt_lost:>8}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rmpj4i4a00f",
   "source": "## Experiment-Level Analysis (Best Distance Per Unique Memory)\n\nThe per-result view above counts each query→result pair independently. But in practice, a memory is\n\"found\" if **any** query retrieves it. This section deduplicates by memory ID and uses the minimum\ndistance across all queries — this is the metric that determines whether a memory passes the threshold.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8r49ituha2d",
   "source": "# Build per-unique-memory data: best (minimum) distance for each memory across all queries\ngt_best_distances = []     # best distance per GT memory (across all experiments)\nnon_gt_best_distances = [] # best distance per non-GT memory\ngt_memory_details = []     # (memory_id, best_distance, experiment_id)\n\nfor exp in experiments:\n    tc_id = exp.get(\"test_case_id\", \"unknown\")\n    gt_ids = set(exp.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n\n    best_distances = {}\n    for qr in exp.get(\"queries\", []):\n        for r in qr.get(\"results\", []):\n            mid = r[\"id\"]\n            d = r[\"distance\"]\n            if mid not in best_distances or d < best_distances[mid]:\n                best_distances[mid] = d\n\n    for mid, d in best_distances.items():\n        if mid in gt_ids:\n            gt_best_distances.append(d)\n            gt_memory_details.append((mid, d, tc_id))\n        else:\n            non_gt_best_distances.append(d)\n\ngt_best = np.array(gt_best_distances)\nnon_gt_best = np.array(non_gt_best_distances)\n\n# Count GT memories NOT retrieved at all\ntotal_gt_memories = sum(len(exp.get(\"ground_truth\", {}).get(\"memory_ids\", [])) for exp in experiments)\nnot_retrieved = total_gt_memories - len(gt_best)\n\nprint(f\"Unique memories retrieved across all experiments:\")\nprint(f\"  GT memories retrieved:     {len(gt_best)} / {total_gt_memories}\")\nprint(f\"  GT memories NOT retrieved: {not_retrieved}\")\nprint(f\"  Non-GT memories:           {len(non_gt_best)}\")\nprint(f\"\\nGT best-distance range:     [{gt_best.min():.4f}, {gt_best.max():.4f}]\")\nprint(f\"Non-GT best-distance range: [{non_gt_best.min():.4f}, {non_gt_best.max():.4f}]\")\nprint(f\"\\nGT mean: {gt_best.mean():.4f}, median: {np.median(gt_best):.4f}\")\nprint(f\"Non-GT mean: {non_gt_best.mean():.4f}, median: {np.median(non_gt_best):.4f}\")\n\nprint(f\"\\nPer GT memory details (sorted by distance):\")\nfor mid, d, tc in sorted(gt_memory_details, key=lambda x: x[1]):\n    print(f\"  {d:.4f}  {mid}  ({tc})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h0r2ga7nyzl",
   "source": "# Experiment-level histograms and threshold sweep\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nbins = np.arange(0.4, 1.15, 0.025)\n\n# Left: overlaid histograms (unique memories, best distance)\naxes[0].hist(gt_best, bins=bins, alpha=0.6, label=f\"GT (n={len(gt_best)})\",\n             color=\"#2ecc71\", edgecolor=\"white\", linewidth=0.5)\naxes[0].hist(non_gt_best, bins=bins, alpha=0.6, label=f\"Non-GT (n={len(non_gt_best)})\",\n             color=\"#e74c3c\", edgecolor=\"white\", linewidth=0.5)\naxes[0].axvline(np.median(gt_best), color=\"#27ae60\", linestyle=\"--\", linewidth=1.5,\n                label=f\"GT median: {np.median(gt_best):.3f}\")\naxes[0].axvline(np.median(non_gt_best), color=\"#c0392b\", linestyle=\"--\", linewidth=1.5,\n                label=f\"Non-GT median: {np.median(non_gt_best):.3f}\")\naxes[0].set_xlabel(\"Best Cosine Distance (per memory)\")\naxes[0].set_ylabel(\"Count\")\naxes[0].set_title(\"Unique Memory Distribution (best distance)\")\naxes[0].legend(fontsize=9)\n\n# Right: P/R/F1 vs threshold (experiment level)\nexp_thresholds = np.arange(0.40, 1.15, 0.01)\nexp_precisions = []\nexp_recalls = []\nexp_f1_scores = []\nexp_total_gt = len(gt_best)\n\nfor t in exp_thresholds:\n    gt_acc = np.sum(gt_best <= t)\n    non_gt_acc = np.sum(non_gt_best <= t)\n    total_acc = gt_acc + non_gt_acc\n    p = gt_acc / total_acc if total_acc > 0 else 1.0\n    r = gt_acc / exp_total_gt if exp_total_gt > 0 else 0.0\n    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n    exp_precisions.append(p)\n    exp_recalls.append(r)\n    exp_f1_scores.append(f1)\n\nexp_precisions = np.array(exp_precisions)\nexp_recalls = np.array(exp_recalls)\nexp_f1_scores = np.array(exp_f1_scores)\n\nexp_best_f1_idx = np.argmax(exp_f1_scores)\nexp_best_threshold = exp_thresholds[exp_best_f1_idx]\n\naxes[1].plot(exp_thresholds, exp_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\naxes[1].plot(exp_thresholds, exp_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\naxes[1].plot(exp_thresholds, exp_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\naxes[1].axvline(exp_best_threshold, color=\"#e74c3c\", linestyle=\"--\", linewidth=1.5,\n                label=f\"Best F1 @ {exp_best_threshold:.2f}\")\naxes[1].set_xlabel(\"Distance Threshold\")\naxes[1].set_ylabel(\"Score\")\naxes[1].set_title(\"Experiment-Level P/R/F1 (unique memories)\")\naxes[1].legend()\naxes[1].set_ylim(0, 1.05)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nExperiment-level optimal F1 threshold: {exp_best_threshold:.2f}\")\nprint(f\"  F1:        {exp_f1_scores[exp_best_f1_idx]:.3f}\")\nprint(f\"  Precision: {exp_precisions[exp_best_f1_idx]:.3f}\")\nprint(f\"  Recall:    {exp_recalls[exp_best_f1_idx]:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ttksiuwzcbl",
   "source": "# Experiment-level threshold table (unique memories)\nprint(f\"Experiment-level threshold table (unique memories, best distance per memory):\\n\")\nprint(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'Accepted':>10} {'GT Kept':>8} {'GT Lost':>8}\")\nprint(\"-\" * 74)\n\nfor t in [0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.05]:\n    gt_acc = int(np.sum(gt_best <= t))\n    non_gt_acc = int(np.sum(non_gt_best <= t))\n    total_acc = gt_acc + non_gt_acc\n    p = gt_acc / total_acc if total_acc > 0 else 1.0\n    r = gt_acc / exp_total_gt if exp_total_gt > 0 else 0.0\n    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n    gt_lost = exp_total_gt - gt_acc\n    marker = \" <--\" if abs(t - exp_best_threshold) < 0.015 else \"\"\n    print(f\"{t:>10.2f} {p:>10.3f} {r:>8.3f} {f1:>8.3f} {total_acc:>10} {gt_acc:>8} {gt_lost:>8}{marker}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-experiment",
   "metadata": {},
   "outputs": [],
   "source": "# Per-experiment impact of applying the best threshold\nprint(f\"Impact of threshold={best_threshold:.2f} per experiment:\\n\")\nprint(f\"{'Test Case':<55} {'Recall':>7} {'Prec':>7} {'F1':>7} {'GT':>4} {'Kept':>5} {'Lost':>5}\")\nprint(\"-\" * 95)\n\nfor exp in experiments:\n    tc_id = exp.get(\"test_case_id\", \"unknown\")\n    gt_ids = set(exp.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n\n    # Collect unique retrieved IDs with their best (minimum) distance\n    best_distances = {}  # memory_id -> min distance across all queries\n    for qr in exp.get(\"queries\", []):\n        for r in qr.get(\"results\", []):\n            mid = r[\"id\"]\n            d = r[\"distance\"]\n            if mid not in best_distances or d < best_distances[mid]:\n                best_distances[mid] = d\n\n    # Apply threshold\n    accepted_ids = {mid for mid, d in best_distances.items() if d <= best_threshold}\n    gt_accepted = accepted_ids & gt_ids\n    gt_lost = gt_ids - accepted_ids\n\n    r = len(gt_accepted) / len(gt_ids) if gt_ids else 0\n    p = len(gt_accepted) / len(accepted_ids) if accepted_ids else 0\n    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n\n    print(f\"{tc_id:<55} {r:>7.1%} {p:>7.1%} {f1:>7.3f} {len(gt_ids):>4} {len(gt_accepted):>5} {len(gt_lost):>5}\")\n\n    if gt_lost:\n        for mid in sorted(gt_lost):\n            d = best_distances.get(mid)\n            d_str = f\"{d:.4f}\" if d is not None else \"NOT RETRIEVED\"\n            print(f\"  {'':55} Lost: {mid} (best d={d_str})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: every result colored by ground truth status\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Group by experiment for x-axis positioning\n",
    "exp_names = [e.get(\"test_case_id\", \"?\").replace(\"tc_\", \"\")[:30] for e in experiments]\n",
    "x_positions = []\n",
    "colors = []\n",
    "distances_plot = []\n",
    "\n",
    "for exp_idx, exp in enumerate(experiments):\n",
    "    for qr in exp.get(\"queries\", []):\n",
    "        for r in qr.get(\"results\", []):\n",
    "            # Add jitter for readability\n",
    "            x_positions.append(exp_idx + np.random.uniform(-0.3, 0.3))\n",
    "            distances_plot.append(r[\"distance\"])\n",
    "            colors.append(\"#2ecc71\" if r[\"is_ground_truth\"] else \"#e74c3c\")\n",
    "\n",
    "ax.scatter(x_positions, distances_plot, c=colors, alpha=0.5, s=25, edgecolors=\"white\", linewidth=0.3)\n",
    "ax.axhline(best_threshold, color=\"#3498db\", linestyle=\"--\", linewidth=2,\n",
    "           label=f\"Optimal threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#2ecc71', markersize=8, label='Ground Truth'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#e74c3c', markersize=8, label='Non-GT'),\n",
    "    Line2D([0], [0], color='#3498db', linestyle='--', linewidth=2, label=f'Threshold: {best_threshold:.2f}'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"upper left\")\n",
    "\n",
    "ax.set_xticks(range(len(exp_names)))\n",
    "ax.set_xticklabels(exp_names, rotation=30, ha=\"right\", fontsize=8)\n",
    "ax.set_ylabel(\"Cosine Distance\")\n",
    "ax.set_title(\"All Results by Experiment (below threshold = accepted)\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence-buckets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the current confidence buckets from db.py vs data-driven buckets\n",
    "current_buckets = [\n",
    "    (\"high\",     0.0,  0.5),\n",
    "    (\"medium\",   0.5,  0.8),\n",
    "    (\"low\",      0.8,  1.2),\n",
    "    (\"very_low\", 1.2,  2.0),\n",
    "]\n",
    "\n",
    "print(\"Current confidence buckets (db.py) vs actual data:\\n\")\n",
    "print(f\"{'Bucket':<10} {'Range':<15} {'GT Count':>10} {'Non-GT':>10} {'GT %':>8} {'Precision':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, lo, hi in current_buckets:\n",
    "    gt_in = int(np.sum((gt_distances >= lo) & (gt_distances < hi)))\n",
    "    non_gt_in = int(np.sum((non_gt_distances >= lo) & (non_gt_distances < hi)))\n",
    "    total_in = gt_in + non_gt_in\n",
    "    gt_pct = gt_in / len(gt_distances) * 100 if len(gt_distances) > 0 else 0\n",
    "    prec = gt_in / total_in if total_in > 0 else 0\n",
    "    print(f\"{name:<10} [{lo:.1f}, {hi:.1f}){'':<5} {gt_in:>10} {non_gt_in:>10} {gt_pct:>7.1f}% {prec:>10.1%}\")\n",
    "\n",
    "print(f\"\\nRecommendation: Update confidence buckets based on data distribution.\")\n",
    "print(f\"The optimal F1 threshold ({best_threshold:.2f}) should be the cutoff between 'accepted' and 'rejected'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "# Final summary and recommendation\nprint(\"=\" * 70)\nprint(\"THRESHOLD ANALYSIS SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\n--- Per-Result Level (raw query results) ---\")\nprint(f\"Data points: {len(all_points)} ({len(gt_distances)} GT, {len(non_gt_distances)} non-GT)\")\nprint(f\"Optimal F1 threshold: {best_threshold:.2f}\")\nprint(f\"  P={precisions[best_f1_idx]:.1%}, R={recalls[best_f1_idx]:.1%}, F1={f1_scores[best_f1_idx]:.3f}\")\n\nprint(f\"\\n--- Experiment Level (unique memories, best distance) ---\")\nprint(f\"Unique memories: {len(gt_best) + len(non_gt_best)} ({len(gt_best)} GT, {len(non_gt_best)} non-GT)\")\nprint(f\"GT not retrieved at all: {not_retrieved}\")\nprint(f\"Optimal F1 threshold: {exp_best_threshold:.2f}\")\nprint(f\"  P={exp_precisions[exp_best_f1_idx]:.1%}, R={exp_recalls[exp_best_f1_idx]:.1%}, F1={exp_f1_scores[exp_best_f1_idx]:.3f}\")\n\nprint(f\"\\n--- Key Tradeoff Points (experiment level) ---\")\nfor t, label in [(0.80, \"aggressive\"), (exp_best_threshold, \"optimal F1\"), (0.95, \"conservative\")]:\n    idx = np.argmin(np.abs(exp_thresholds - t))\n    gt_lost = exp_total_gt - int(np.sum(gt_best <= t))\n    print(f\"  t={t:.2f} ({label:>12}): P={exp_precisions[idx]:.1%}, R={exp_recalls[idx]:.1%}, \"\n          f\"F1={exp_f1_scores[idx]:.3f}, GT lost={gt_lost}\")\n\nprint(f\"\\n--- Recommendation ---\")\nprint(f\"The GT and non-GT distributions overlap significantly (GT median={np.median(gt_best):.3f}, \"\n      f\"non-GT median={np.median(non_gt_best):.3f}).\")\nprint(f\"A single distance threshold cannot cleanly separate them.\")\nprint(f\"\")\nprint(f\"Practical options:\")\nprint(f\"  1. Use threshold={exp_best_threshold:.2f} as MAX_DISTANCE_THRESHOLD for best F1\")\nprint(f\"  2. Use threshold=0.95 to keep ~all GT while removing the worst noise\")\nprint(f\"  3. Combine threshold with additional signals (RRF scoring, re-ranking)\")\nprint(f\"     to improve separation beyond what distance alone provides\")\nprint(\"=\" * 70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}