{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Cross-Run Comparison\n",
    "\n",
    "Compare experiment runs across configs to:\n",
    "1. **Quantify reproducibility** — how much does F1 vary across runs with identical config?\n",
    "2. **Compare configurations** — when changing prompt/reranker/model, is the improvement real?\n",
    "3. **Track trends** — are results improving over time?\n",
    "\n",
    "Works with Phase 1 and Phase 2 runs. Backfills fingerprints for existing runs automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 — Setup, Imports & Backfill\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from memory_retrieval.experiments.comparison import (\n",
    "    compare_configs,\n",
    "    compute_variance_report,\n",
    "    fingerprint_diff,\n",
    "    group_runs_by_fingerprint,\n",
    "    load_run_summaries,\n",
    "    reconstruct_fingerprint_from_run,\n",
    ")\n",
    "from memory_retrieval.experiments.metrics_adapter import extract_metric_from_nested\n",
    "from memory_retrieval.infra.figures import create_figure_session, save_figure\n",
    "from memory_retrieval.infra.io import load_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    PHASE1,\n",
    "    PHASE2,\n",
    "    list_runs,\n",
    "    update_config_fingerprint,\n",
    ")\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "# --- Helpers for extracting metrics from run_summary.json ---\n",
    "def extract_f1_from_summary(macro, metric_path, strategy=None):\n",
    "    \"\"\"Extract F1 from macro_averaged dict via shared adapter.\"\"\"\n",
    "    return extract_metric_from_nested(macro, metric_path, strategy, metric_key=\"f1\")\n",
    "\n",
    "\n",
    "def extract_f1_from_per_test_case(per_tc, metric_path, strategy=None):\n",
    "    \"\"\"Extract F1 from per_test_case entry via shared adapter.\"\"\"\n",
    "    return extract_metric_from_nested(per_tc, metric_path, strategy, metric_key=\"f1\")\n",
    "\n",
    "\n",
    "# --- Backfill fingerprints for existing runs ---\n",
    "backfilled_count = 0\n",
    "for phase in [PHASE1, PHASE2]:\n",
    "    for run in list_runs(phase):\n",
    "        run_dir = run[\"run_dir\"]\n",
    "        run_metadata = load_json(run_dir / \"run.json\")\n",
    "        if \"config_fingerprint\" not in run_metadata:\n",
    "            results_dir = run_dir / \"results\"\n",
    "            if results_dir.exists() and list(results_dir.glob(\"*.json\")):\n",
    "                fingerprint = reconstruct_fingerprint_from_run(run_dir)\n",
    "                update_config_fingerprint(run_dir, fingerprint)\n",
    "                backfilled_count += 1\n",
    "                print(f\"  Backfilled: {run['run_id']} -> {fingerprint['fingerprint_hash']}\")\n",
    "\n",
    "if backfilled_count > 0:\n",
    "    print(f\"\\nBackfilled {backfilled_count} runs.\")\n",
    "else:\n",
    "    print(\"All runs already have fingerprints.\")\n",
    "\n",
    "print(\"\\nSetup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Configuration\n",
    "\n",
    "# Which phase to analyze\n",
    "PHASE = PHASE2\n",
    "\n",
    "# Optional: restrict to specific run IDs (None = all runs)\n",
    "SELECTED_RUN_IDS = None  # e.g., [\"run_20260212_181821\", \"run_20260213_173243\"]\n",
    "\n",
    "# Rerank strategy to focus on (for reranking runs) \"situation_and_lesson\" or \"situation_only\" - for \"post_rerank\" only\n",
    "STRATEGY = \"situation_only\"\n",
    "\n",
    "# Metric path: \"post_rerank\" for reranking runs, \"pre_rerank\" for vector-only baseline\n",
    "METRIC_PATH = \"post_rerank\"\n",
    "\n",
    "# For A/B comparison: select two fingerprint hashes to compare\n",
    "# Set after viewing Cell 2 output; leave as None for auto-selection\n",
    "COMPARE_HASH_A = None\n",
    "COMPARE_HASH_B = None\n",
    "\n",
    "# Derived label for titles/logs — avoids showing a strategy name when using pre_rerank\n",
    "METRIC_LABEL = STRATEGY if METRIC_PATH == \"post_rerank\" else \"pre-rerank (distance)\"\n",
    "\n",
    "print(f\"Phase: {PHASE}\")\n",
    "print(f\"Strategy: {STRATEGY}\")\n",
    "print(f\"Metric path: {METRIC_PATH}\")\n",
    "print(f\"Metric label: {METRIC_LABEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load Runs & Overview Table\n",
    "\n",
    "summaries = load_run_summaries(PHASE, run_ids=SELECTED_RUN_IDS)\n",
    "print(f\"Loaded {len(summaries)} run summaries for {PHASE}\\n\")\n",
    "\n",
    "groups = group_runs_by_fingerprint(summaries)\n",
    "print(f\"Found {len(groups)} config groups:\\n\")\n",
    "\n",
    "# Overview table\n",
    "header = f\"{'Group':>8} {'Hash':>10} {'N Runs':>7} {'Avg F1':>8} {'Std':>8} {'Key Config Differences':>40}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "group_labels = {}  # hash -> label for later cells\n",
    "sorted_hashes = sorted(groups.keys(), key=lambda h: groups[h][0].get(\"run_id\", \"\"))\n",
    "\n",
    "for group_index, fingerprint_hash in enumerate(sorted_hashes):\n",
    "    group_summaries = groups[fingerprint_hash]\n",
    "    label = chr(ord(\"A\") + group_index)\n",
    "    group_labels[fingerprint_hash] = label\n",
    "\n",
    "    # Extract F1 per run using backward-compatible helper\n",
    "    f1_values = []\n",
    "    for summary in group_summaries:\n",
    "        macro = summary.get(\"macro_averaged\", {})\n",
    "        f1 = extract_f1_from_summary(macro, METRIC_PATH, STRATEGY)\n",
    "        if f1 is not None:\n",
    "            f1_values.append(f1)\n",
    "\n",
    "    avg_f1 = np.mean(f1_values) if f1_values else 0.0\n",
    "    std_f1 = np.std(f1_values, ddof=1) if len(f1_values) > 1 else 0.0\n",
    "\n",
    "    # Key config from fingerprint\n",
    "    fingerprint = group_summaries[0].get(\"config_fingerprint\", {})\n",
    "    config_desc = f\"ext={fingerprint.get('extraction_prompt_version', '?')}, q={fingerprint.get('query_prompt_version', '?')}\"\n",
    "\n",
    "    print(\n",
    "        f\"{label:>8} {fingerprint_hash:>10} {len(group_summaries):>7} \"\n",
    "        f\"{avg_f1:>8.3f} {std_f1:>8.4f} {config_desc:>40}\"\n",
    "    )\n",
    "\n",
    "    # Print individual runs\n",
    "    for summary in group_summaries:\n",
    "        macro = summary.get(\"macro_averaged\", {})\n",
    "        run_f1 = extract_f1_from_summary(macro, METRIC_PATH, STRATEGY) or 0.0\n",
    "        print(f\"{'':>8} {'':>10} {'':>7} {run_f1:>8.3f} {'':>8} {summary['run_id']}\")\n",
    "\n",
    "print(f\"\\nGroup labels for A/B comparison: {group_labels}\")\n",
    "\n",
    "loaded_run_ids = sorted(\n",
    "    [summary.get(\"run_id\", \"unknown\") for summary in summaries if summary.get(\"run_id\")]\n",
    ")\n",
    "if SELECTED_RUN_IDS:\n",
    "    if loaded_run_ids:\n",
    "        selected_part = f\"selected-{loaded_run_ids[0]}-{loaded_run_ids[-1]}-n{len(loaded_run_ids)}\"\n",
    "    else:\n",
    "        selected_part = \"selected-none-none-n0\"\n",
    "else:\n",
    "    selected_part = f\"selected-all-runs-n{len(loaded_run_ids)}\"\n",
    "\n",
    "if sorted_hashes:\n",
    "    groups_part = f\"groups-{sorted_hashes[0][:8]}-{sorted_hashes[-1][:8]}-n{len(sorted_hashes)}\"\n",
    "else:\n",
    "    groups_part = \"groups-none-none-n0\"\n",
    "\n",
    "comparison_key = f\"{selected_part}__{groups_part}__metric-{METRIC_PATH}__label-{METRIC_LABEL}\"\n",
    "FIGURE_SESSION = create_figure_session(\n",
    "    root_dir=PROJECT_ROOT / \"data\" / \"comparisons\" / \"exports\",\n",
    "    notebook_slug=f\"cross_run_comparison/{PHASE}\",\n",
    "    context_key=comparison_key,\n",
    "    context={\n",
    "        \"phase\": PHASE,\n",
    "        \"strategy\": STRATEGY,\n",
    "        \"metric_path\": METRIC_PATH,\n",
    "        \"metric_label\": METRIC_LABEL,\n",
    "        \"selected_run_ids\": SELECTED_RUN_IDS,\n",
    "        \"loaded_run_ids\": loaded_run_ids,\n",
    "        \"group_hashes\": sorted_hashes,\n",
    "    },\n",
    ")\n",
    "print(f\"Figure export session: {FIGURE_SESSION.session_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Run Timeline\n",
    "# Each run gets its own x position (sorted by time), so batch runs never overlap.\n",
    "# X tick labels show the actual timestamp. Dashed lines connect consecutive runs\n",
    "# within the same config group to make trends visible across sequential experiments.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Color map shared with Cell 6\n",
    "colors_map = plt.cm.Set2(np.linspace(0, 1, max(len(groups), 3)))\n",
    "hash_to_color = {\n",
    "    fingerprint_hash: colors_map[i] for i, fingerprint_hash in enumerate(sorted_hashes)\n",
    "}\n",
    "\n",
    "# Collect all runs with timestamps and F1 values\n",
    "run_records = []\n",
    "for fingerprint_hash, group_summaries in groups.items():\n",
    "    for summary in group_summaries:\n",
    "        run_id = summary.get(\"run_id\", \"\")\n",
    "        try:\n",
    "            # Strip \"run_\" prefix and take only date+time parts (first two underscore-separated\n",
    "            # segments), so both old format (YYYYMMDD_HHMMSS) and new format\n",
    "            # (YYYYMMDD_HHMMSS_ffffff) parse correctly.\n",
    "            run_id_parts = run_id.replace(\"run_\", \"\").split(\"_\")\n",
    "            timestamp = datetime.strptime(\"_\".join(run_id_parts[:2]), \"%Y%m%d_%H%M%S\")\n",
    "        except (ValueError, AttributeError):\n",
    "            continue\n",
    "        macro = summary.get(\"macro_averaged\", {})\n",
    "        f1 = extract_f1_from_summary(macro, METRIC_PATH, STRATEGY)\n",
    "        if f1 is not None:\n",
    "            run_records.append(\n",
    "                {\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"f1\": f1,\n",
    "                    \"fingerprint_hash\": fingerprint_hash,\n",
    "                }\n",
    "            )\n",
    "\n",
    "run_records.sort(key=lambda record: record[\"timestamp\"])\n",
    "\n",
    "if not run_records:\n",
    "    print(\"No runs with valid timestamps found.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    # One dot per run at its sorted index — no overlap regardless of how close timestamps are\n",
    "    for run_index, record in enumerate(run_records):\n",
    "        color = hash_to_color[record[\"fingerprint_hash\"]]\n",
    "        ax.scatter(\n",
    "            run_index, record[\"f1\"], color=color, s=80, zorder=5, edgecolors=\"white\", linewidth=0.5\n",
    "        )\n",
    "\n",
    "    # Dashed trend line connecting runs of the same config group in time order\n",
    "    for fingerprint_hash in sorted_hashes:\n",
    "        group_indices = [\n",
    "            (i, r[\"f1\"])\n",
    "            for i, r in enumerate(run_records)\n",
    "            if r[\"fingerprint_hash\"] == fingerprint_hash\n",
    "        ]\n",
    "        if len(group_indices) > 1:\n",
    "            xs, ys = zip(*group_indices)\n",
    "            ax.plot(\n",
    "                xs,\n",
    "                ys,\n",
    "                color=hash_to_color[fingerprint_hash],\n",
    "                linewidth=1,\n",
    "                alpha=0.35,\n",
    "                linestyle=\"--\",\n",
    "            )\n",
    "\n",
    "    # X tick labels: actual timestamps so temporal context is not lost\n",
    "    ax.set_xticks(range(len(run_records)))\n",
    "    ax.set_xticklabels(\n",
    "        [r[\"timestamp\"].strftime(\"%m-%d %H:%M\") for r in run_records],\n",
    "        rotation=45,\n",
    "        ha=\"right\",\n",
    "        fontsize=7,\n",
    "    )\n",
    "\n",
    "    # Legend\n",
    "    for fingerprint_hash in sorted_hashes:\n",
    "        label = group_labels[fingerprint_hash]\n",
    "        fingerprint = groups[fingerprint_hash][0].get(\"config_fingerprint\", {})\n",
    "        desc = (\n",
    "            f\"Group {label}: ext={fingerprint.get('extraction_prompt_version', '?')}, \"\n",
    "            f\"q={fingerprint.get('query_prompt_version', '?')}\"\n",
    "        )\n",
    "        ax.scatter([], [], color=hash_to_color[fingerprint_hash], s=80, label=desc)\n",
    "\n",
    "    ax.set_xlabel(\"Run (sorted by time)\")\n",
    "    ax.set_ylabel(f\"Macro F1 ({METRIC_LABEL})\")\n",
    "    ax.set_title(f\"F1 Over Time — {PHASE}\")\n",
    "    ax.legend(fontsize=8, loc=\"best\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"f1_over_time\",\n",
    "        title=f\"F1 Over Time — {PHASE}\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Reproducibility (Same-Config Variance)\n",
    "\n",
    "print(\"VARIANCE ANALYSIS (same-config runs)\\n\")\n",
    "\n",
    "# Find groups with multiple runs\n",
    "multi_run_groups = {h: g for h, g in groups.items() if len(g) >= 2}\n",
    "\n",
    "if not multi_run_groups:\n",
    "    print(\"No config groups with 2+ runs found. Run more experiments with the same config.\")\n",
    "else:\n",
    "    # Box plots of per-run macro F1 per config group\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: Box plot of run-level F1\n",
    "    ax = axes[0]\n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    for fingerprint_hash, group_summaries in multi_run_groups.items():\n",
    "        label = group_labels.get(fingerprint_hash, fingerprint_hash[:8])\n",
    "        f1_values = []\n",
    "        for summary in group_summaries:\n",
    "            macro = summary.get(\"macro_averaged\", {})\n",
    "            f1 = extract_f1_from_summary(macro, METRIC_PATH, STRATEGY)\n",
    "            if f1 is not None:\n",
    "                f1_values.append(f1)\n",
    "        if f1_values:\n",
    "            box_data.append(f1_values)\n",
    "            box_labels.append(f\"Group {label}\\n(n={len(f1_values)})\")\n",
    "\n",
    "    if box_data:\n",
    "        bp = ax.boxplot(box_data, tick_labels=box_labels, patch_artist=True)\n",
    "        for patch in bp[\"boxes\"]:\n",
    "            patch.set_facecolor(\"#3498db\")\n",
    "            patch.set_alpha(0.4)\n",
    "        # Overlay individual points\n",
    "        for group_index, values in enumerate(box_data):\n",
    "            x_positions = np.random.normal(group_index + 1, 0.04, len(values))\n",
    "            ax.scatter(x_positions, values, color=\"#2c3e50\", s=40, zorder=5, alpha=0.7)\n",
    "        ax.set_ylabel(\"Macro F1\")\n",
    "        ax.set_title(\"Run-Level F1 Variance (Same Config)\")\n",
    "        ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Right: Per-test-case variance table as text\n",
    "    ax = axes[1]\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    variance_text = \"\"\n",
    "    for fingerprint_hash, group_summaries in multi_run_groups.items():\n",
    "        label = group_labels.get(fingerprint_hash, fingerprint_hash[:8])\n",
    "        report = compute_variance_report(\n",
    "            group_summaries, strategy=STRATEGY, metric_path=METRIC_PATH\n",
    "        )\n",
    "\n",
    "        run_level = report.get(\"run_level\", {})\n",
    "        variance_text += f\"Group {label} ({report['num_runs']} runs):\\n\"\n",
    "        variance_text += f\"  F1: {run_level.get('mean', 0):.3f} +/- {run_level.get('std', 0):.4f}\\n\"\n",
    "        variance_text += f\"  95% CI: [{run_level.get('ci_95_lower', 0):.3f}, {run_level.get('ci_95_upper', 0):.3f}]\\n\"\n",
    "        variance_text += f\"  CV: {run_level.get('cv', 0):.3f}\\n\\n\"\n",
    "\n",
    "        # Per-test-case with highest variance\n",
    "        per_tc = report.get(\"per_test_case\", {})\n",
    "        if per_tc:\n",
    "            sorted_by_cv = sorted(\n",
    "                per_tc.items(), key=lambda item: item[1].get(\"cv\", 0), reverse=True\n",
    "            )\n",
    "            variance_text += \"  Most variable test cases:\\n\"\n",
    "            for test_case_id, stats in sorted_by_cv[:5]:\n",
    "                variance_text += f\"    {test_case_id}: F1={stats['mean']:.3f} +/- {stats['std']:.4f} (CV={stats['cv']:.3f})\\n\"\n",
    "            variance_text += \"\\n\"\n",
    "\n",
    "    ax.text(\n",
    "        0,\n",
    "        1,\n",
    "        variance_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=8,\n",
    "        verticalalignment=\"top\",\n",
    "        fontfamily=\"monospace\",\n",
    "    )\n",
    "    ax.set_title(\"Variance Details\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"same_config_variance\",\n",
    "        title=\"Run-Level F1 Variance (Same Config)\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4b — Pre-Rerank vs Post-Rerank Effect Analysis (Within Config Group)\n",
    "#\n",
    "# For each config group that has reranking data, compares pre-rerank (vector-only)\n",
    "# F1 against post-rerank F1 using per-run paired analysis via\n",
    "# retrieval_metrics.compute_effect_size.\n",
    "#\n",
    "# Each run contributes one pre-rerank F1 and one post-rerank F1 — a natural\n",
    "# paired design since both metrics come from the same run.\n",
    "#\n",
    "# Note: These are Level 1 F1 values (per-test-case optimal threshold).\n",
    "# Use for: \"does reranking help?\" — NOT for estimating production F1.\n",
    "\n",
    "from retrieval_metrics import compute_effect_size\n",
    "\n",
    "if not multi_run_groups:\n",
    "    print(\"No config groups with 2+ runs found.\")\n",
    "else:\n",
    "    print(\"=\" * 92)\n",
    "    print(f\"PRE-RERANK vs POST-RERANK EFFECT ANALYSIS (per-run paired, strategy={STRATEGY})\")\n",
    "    print(\"=\" * 92)\n",
    "    print()\n",
    "    print(\"Note: F1 values are Level 1 (per-test-case optimal threshold, most optimistic).\")\n",
    "    print(\"Use for comparing pre vs post rerank, NOT for estimating production F1.\")\n",
    "\n",
    "    for fingerprint_hash, group_summaries in multi_run_groups.items():\n",
    "        label = group_labels.get(fingerprint_hash, fingerprint_hash[:8])\n",
    "\n",
    "        # Check if this group has reranking data\n",
    "        has_rerank = any(\n",
    "            extract_f1_from_summary(\n",
    "                summary.get(\"macro_averaged\", {}), \"post_rerank\", STRATEGY\n",
    "            ) is not None\n",
    "            for summary in group_summaries\n",
    "        )\n",
    "\n",
    "        if not has_rerank:\n",
    "            print(f\"\\nGroup {label} ({fingerprint_hash}): No post-rerank data found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Collect per-run paired differences: post_rerank - pre_rerank\n",
    "        pre_rerank_f1_values = []\n",
    "        post_rerank_f1_values = []\n",
    "        paired_differences = []\n",
    "\n",
    "        for summary in group_summaries:\n",
    "            macro = summary.get(\"macro_averaged\", {})\n",
    "            pre_f1 = extract_f1_from_summary(macro, \"pre_rerank\", None)\n",
    "            post_f1 = extract_f1_from_summary(macro, \"post_rerank\", STRATEGY)\n",
    "\n",
    "            if pre_f1 is not None and post_f1 is not None:\n",
    "                pre_rerank_f1_values.append(pre_f1)\n",
    "                post_rerank_f1_values.append(post_f1)\n",
    "                paired_differences.append(post_f1 - pre_f1)\n",
    "\n",
    "        num_pairs = len(paired_differences)\n",
    "        if num_pairs < 2:\n",
    "            print(\n",
    "                f\"\\nGroup {label}: Too few runs with both pre/post data ({num_pairs}). \"\n",
    "                f\"Need at least 2.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        mean_pre = sum(pre_rerank_f1_values) / num_pairs\n",
    "        mean_post = sum(post_rerank_f1_values) / num_pairs\n",
    "\n",
    "        # Use the library for all statistical calculations\n",
    "        effect = compute_effect_size(paired_differences)\n",
    "\n",
    "        print(f\"\\n{'─' * 92}\")\n",
    "        print(f\"  Group {label} ({fingerprint_hash}) — {num_pairs} runs\")\n",
    "        print(f\"  Pre-rerank (vector only) vs Post-rerank ({STRATEGY})\")\n",
    "        print(f\"{'─' * 92}\")\n",
    "        print(f\"  Pre-rerank mean F1:     {mean_pre:.4f}\")\n",
    "        print(f\"  Post-rerank mean F1:    {mean_post:.4f}\")\n",
    "        print(f\"  Mean diff (post-pre):   {effect.mean_difference:+.4f}\")\n",
    "        print(f\"  Std of paired diffs:    {effect.std_of_differences:.4f}\")\n",
    "        print(f\"  Standard error:         {effect.standard_error:.4f}\")\n",
    "        print(f\"  t-statistic:            {effect.t_statistic:.3f}\")\n",
    "        print(f\"  95% CI:                 [{effect.ci_lower:+.4f}, {effect.ci_upper:+.4f}]\")\n",
    "        print(f\"  CI excludes zero:       {effect.ci_excludes_zero}\")\n",
    "        print()\n",
    "        print(f\"  Cohen's d:              {effect.cohens_d:+.3f}  ({effect.effect_category})\")\n",
    "        print()\n",
    "        print(f\"  Runs post > pre:        {effect.count_positive}/{effect.n}\")\n",
    "        print(f\"  Runs post < pre:        {effect.count_negative}/{effect.n}\")\n",
    "        print(f\"  Runs tied:              {effect.count_zero}/{effect.n}\")\n",
    "        print()\n",
    "        print(f\"  MDE at n={effect.n}:           {effect.minimum_detectable_effect:.4f}\")\n",
    "        print(f\"  Observed |effect|:      {abs(effect.mean_difference):.4f}\")\n",
    "\n",
    "        if effect.power_sufficient:\n",
    "            print(\"  -> Effect > MDE: DETECTABLE at 80% power\")\n",
    "        else:\n",
    "            print(\"  -> Effect < MDE: BELOW detection threshold (power < 80%)\")\n",
    "\n",
    "        if effect.required_n is not None:\n",
    "            print(f\"  Required n for 80% power: {effect.required_n}\")\n",
    "\n",
    "        if effect.ci_excludes_zero:\n",
    "            direction = \"IMPROVEMENT\" if effect.mean_difference > 0 else \"DEGRADATION\"\n",
    "            print(f\"\\n  VERDICT: Statistically significant {direction}\")\n",
    "        else:\n",
    "            print(f\"\\n  VERDICT: NOT significant (CI includes zero)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — A/B Comparison (Select Two Groups)\n",
    "\n",
    "# Auto-select: first two groups, or use COMPARE_HASH_A / COMPARE_HASH_B\n",
    "if COMPARE_HASH_A and COMPARE_HASH_B:\n",
    "    hash_a, hash_b = COMPARE_HASH_A, COMPARE_HASH_B\n",
    "elif len(sorted_hashes) >= 2:\n",
    "    hash_a, hash_b = sorted_hashes[0], sorted_hashes[-1]\n",
    "else:\n",
    "    print(\"Need at least 2 config groups for A/B comparison.\")\n",
    "    hash_a, hash_b = None, None\n",
    "\n",
    "if hash_a and hash_b:\n",
    "    label_a = group_labels.get(hash_a, hash_a[:8])\n",
    "    label_b = group_labels.get(hash_b, hash_b[:8])\n",
    "    summaries_a = groups[hash_a]\n",
    "    summaries_b = groups[hash_b]\n",
    "\n",
    "    print(f\"Comparing Group {label_a} ({hash_a}) vs Group {label_b} ({hash_b})\")\n",
    "\n",
    "    # Show config diff\n",
    "    fp_a = summaries_a[0].get(\"config_fingerprint\", {})\n",
    "    fp_b = summaries_b[0].get(\"config_fingerprint\", {})\n",
    "    diff = fingerprint_diff(fp_a, fp_b)\n",
    "    if diff:\n",
    "        print(\"\\nConfig differences:\")\n",
    "        for field, values in diff.items():\n",
    "            print(f\"  {field}: {values['a']} -> {values['b']}\")\n",
    "\n",
    "    # Run comparison\n",
    "    comparison = compare_configs(\n",
    "        summaries_a,\n",
    "        summaries_b,\n",
    "        strategy=STRATEGY,\n",
    "        metric_path=METRIC_PATH,\n",
    "    )\n",
    "\n",
    "    if \"error\" in comparison:\n",
    "        print(f\"\\nError: {comparison['error']}\")\n",
    "    else:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"STATISTICAL COMPARISON\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"  Mean F1 (A): {comparison['mean_f1_a']:.3f}\")\n",
    "        print(f\"  Mean F1 (B): {comparison['mean_f1_b']:.3f}\")\n",
    "        print(f\"  Mean diff (B-A): {comparison['mean_diff']:+.4f}\")\n",
    "        print(f\"  Direction: {comparison['direction']}\")\n",
    "        print(\n",
    "            f\"  Bootstrap 95% CI: [{comparison['bootstrap_ci']['lower']:.4f}, {comparison['bootstrap_ci']['upper']:.4f}]\"\n",
    "        )\n",
    "        print(f\"  Excludes zero: {comparison['bootstrap_ci']['excludes_zero']}\")\n",
    "        print(f\"  Wilcoxon p-value: {comparison['wilcoxon']['p_value']:.4f}\")\n",
    "        print(f\"  Significance: {comparison['significance']}\")\n",
    "\n",
    "        # Paired dot plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        paired = comparison[\"paired_differences\"]\n",
    "        test_case_ids = [entry[\"test_case_id\"] for entry in paired]\n",
    "        y_positions = list(range(len(test_case_ids)))\n",
    "\n",
    "        for y_pos, entry in enumerate(paired):\n",
    "            color = (\n",
    "                \"#2ecc71\"\n",
    "                if entry[\"diff\"] > 0.001\n",
    "                else \"#e74c3c\"\n",
    "                if entry[\"diff\"] < -0.001\n",
    "                else \"#95a5a6\"\n",
    "            )\n",
    "            ax.plot(\n",
    "                [entry[\"f1_a\"], entry[\"f1_b\"]],\n",
    "                [y_pos, y_pos],\n",
    "                color=color,\n",
    "                linewidth=2,\n",
    "                alpha=0.6,\n",
    "            )\n",
    "            ax.scatter(entry[\"f1_a\"], y_pos, color=\"#3498db\", s=60, zorder=5, edgecolors=\"white\")\n",
    "            ax.scatter(entry[\"f1_b\"], y_pos, color=\"#e67e22\", s=60, zorder=5, edgecolors=\"white\")\n",
    "\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(test_case_ids, fontsize=8)\n",
    "        ax.set_xlabel(\"F1 Score\")\n",
    "        ax.set_title(f\"Paired Comparison: Group {label_a} (blue) vs Group {label_b} (orange)\")\n",
    "        ax.scatter([], [], color=\"#3498db\", s=60, label=f\"Group {label_a}\")\n",
    "        ax.scatter([], [], color=\"#e67e22\", s=60, label=f\"Group {label_b}\")\n",
    "        ax.plot([], [], color=\"#2ecc71\", linewidth=2, label=\"B improved\")\n",
    "        ax.plot([], [], color=\"#e74c3c\", linewidth=2, label=\"B degraded\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "        plt.tight_layout()\n",
    "        saved_paths = save_figure(\n",
    "            fig,\n",
    "            FIGURE_SESSION,\n",
    "            \"ab_paired_comparison\",\n",
    "            title=f\"Paired Comparison: Group {label_a} vs Group {label_b}\",\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Threshold Sweep Overlay\n",
    "# Supports both metric paths:\n",
    "# - post_rerank: rerank score threshold sweep from rerank_strategies[STRATEGY].threshold_sweep\n",
    "# - pre_rerank: distance threshold sweep from baseline.distance_threshold_sweep\n",
    "\n",
    "if not sorted_hashes:\n",
    "    print(\"Cannot plot: no config groups loaded.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    line_styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "    is_pre_rerank_mode = METRIC_PATH == \"pre_rerank\"\n",
    "    threshold_label = (\n",
    "        \"Distance Threshold (cosine distance)\" if is_pre_rerank_mode else \"Rerank Score Threshold\"\n",
    "    )\n",
    "    annotation_prefix = \"d\" if is_pre_rerank_mode else \"t\"\n",
    "    plot_mode_label = \"Distance\" if is_pre_rerank_mode else \"Rerank Score\"\n",
    "\n",
    "    plotted_groups = 0\n",
    "\n",
    "    for group_index, fingerprint_hash in enumerate(sorted_hashes):\n",
    "        group_summaries = groups[fingerprint_hash]\n",
    "        label = group_labels.get(fingerprint_hash, fingerprint_hash[:8])\n",
    "        color = hash_to_color[fingerprint_hash]\n",
    "\n",
    "        # Collect threshold sweeps from all runs in this group.\n",
    "        group_sweeps = []  # list of {threshold: f1} dicts\n",
    "        for summary in group_summaries:\n",
    "            if is_pre_rerank_mode:\n",
    "                sweep_data = summary.get(\"baseline\", {}).get(\"distance_threshold_sweep\", {})\n",
    "            else:\n",
    "                sweep_data = (\n",
    "                    summary.get(\"rerank_strategies\", {})\n",
    "                    .get(STRATEGY, {})\n",
    "                    .get(\"threshold_sweep\", {})\n",
    "                )\n",
    "\n",
    "            full_sweep = sweep_data.get(\"full_sweep\", [])\n",
    "            if full_sweep:\n",
    "                sweep_dict = {entry[\"threshold\"]: entry[\"f1\"] for entry in full_sweep}\n",
    "                group_sweeps.append(sweep_dict)\n",
    "\n",
    "        if not group_sweeps:\n",
    "            continue\n",
    "\n",
    "        # Get common thresholds (all runs share the same fixed grid).\n",
    "        all_thresholds = sorted(set().union(*[set(sweep.keys()) for sweep in group_sweeps]))\n",
    "\n",
    "        # Compute mean and min/max bands as numpy arrays.\n",
    "        mean_f1_values = np.zeros(len(all_thresholds))\n",
    "        min_f1_values = np.zeros(len(all_thresholds))\n",
    "        max_f1_values = np.zeros(len(all_thresholds))\n",
    "        for threshold_index, threshold in enumerate(all_thresholds):\n",
    "            f1_at_threshold = [sweep[threshold] for sweep in group_sweeps if threshold in sweep]\n",
    "            if f1_at_threshold:\n",
    "                mean_f1_values[threshold_index] = np.mean(f1_at_threshold)\n",
    "                min_f1_values[threshold_index] = min(f1_at_threshold)\n",
    "                max_f1_values[threshold_index] = max(f1_at_threshold)\n",
    "\n",
    "        ax.plot(\n",
    "            all_thresholds,\n",
    "            mean_f1_values,\n",
    "            label=f\"Group {label} (n={len(group_sweeps)})\",\n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "            linestyle=line_styles[group_index % len(line_styles)],\n",
    "        )\n",
    "        plotted_groups += 1\n",
    "\n",
    "        # Shaded band for min-max (if multiple runs).\n",
    "        if len(group_sweeps) > 1:\n",
    "            ax.fill_between(all_thresholds, min_f1_values, max_f1_values, color=color, alpha=0.15)\n",
    "\n",
    "        # Vertical line + annotation at optimal threshold (mean curve peak).\n",
    "        optimal_index = np.argmax(mean_f1_values)\n",
    "        optimal_threshold = all_thresholds[optimal_index]\n",
    "        optimal_f1 = mean_f1_values[optimal_index]\n",
    "        ax.axvline(optimal_threshold, color=color, linestyle=\":\", alpha=0.5, linewidth=1)\n",
    "        ax.annotate(\n",
    "            f\"{annotation_prefix}={optimal_threshold:.3f}\\nF1={optimal_f1:.3f}\",\n",
    "            xy=(optimal_threshold, optimal_f1),\n",
    "            xytext=(10, 10),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=7,\n",
    "            color=color,\n",
    "            ha=\"left\",\n",
    "        )\n",
    "\n",
    "    if plotted_groups == 0:\n",
    "        if is_pre_rerank_mode:\n",
    "            print(\n",
    "                \"No distance threshold sweep data found for METRIC_PATH='pre_rerank'. \"\n",
    "                \"Expected baseline.distance_threshold_sweep.full_sweep in run summaries.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"No rerank threshold sweep data found for METRIC_PATH='post_rerank' \"\n",
    "                f\"and STRATEGY='{STRATEGY}'.\"\n",
    "            )\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        ax.set_xlabel(threshold_label)\n",
    "        ax.set_ylabel(\"Macro F1\")\n",
    "        ax.set_title(f\"{plot_mode_label} Threshold Sweep Overlay — {PHASE} ({METRIC_LABEL})\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        saved_paths = save_figure(\n",
    "            fig,\n",
    "            FIGURE_SESSION,\n",
    "            \"threshold_sweep_overlay\",\n",
    "            title=f\"{plot_mode_label} Threshold Sweep Overlay — {PHASE} ({METRIC_LABEL})\",\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Per-Test-Case Heatmap\n",
    "\n",
    "# Collect F1 per test case per group (averaged across runs in each group)\n",
    "all_test_case_ids = set()\n",
    "for group_summaries in groups.values():\n",
    "    for summary in group_summaries:\n",
    "        all_test_case_ids.update(summary.get(\"per_test_case\", {}).keys())\n",
    "all_test_case_ids = sorted(all_test_case_ids)\n",
    "\n",
    "heatmap_data = np.full((len(all_test_case_ids), len(sorted_hashes)), np.nan)\n",
    "column_labels = []\n",
    "\n",
    "for col_index, fingerprint_hash in enumerate(sorted_hashes):\n",
    "    label = group_labels.get(fingerprint_hash, fingerprint_hash[:8])\n",
    "    column_labels.append(f\"Group {label}\")\n",
    "    group_summaries = groups[fingerprint_hash]\n",
    "\n",
    "    for row_index, test_case_id in enumerate(all_test_case_ids):\n",
    "        f1_values = []\n",
    "        for summary in group_summaries:\n",
    "            per_tc = summary.get(\"per_test_case\", {}).get(test_case_id, {})\n",
    "            f1 = extract_f1_from_per_test_case(per_tc, METRIC_PATH, STRATEGY)\n",
    "            if f1 is not None:\n",
    "                f1_values.append(f1)\n",
    "        if f1_values:\n",
    "            heatmap_data[row_index, col_index] = np.mean(f1_values)\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(max(8, len(sorted_hashes) * 2), max(6, len(all_test_case_ids) * 0.5))\n",
    ")\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=column_labels,\n",
    "    yticklabels=all_test_case_ids,\n",
    "    cmap=\"RdYlGn\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(f\"Per-Test-Case F1 by Config Group — {PHASE} ({METRIC_LABEL})\")\n",
    "ax.set_ylabel(\"Test Case\")\n",
    "ax.set_xlabel(\"Config Group\")\n",
    "plt.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig,\n",
    "    FIGURE_SESSION,\n",
    "    \"per_test_case_heatmap\",\n",
    "    title=f\"Per-Test-Case F1 by Config Group — {PHASE} ({METRIC_LABEL})\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "# Hardest test cases (lowest average F1 across groups)\n",
    "avg_f1_per_tc = np.nanmean(heatmap_data, axis=1)\n",
    "sorted_indices = np.argsort(avg_f1_per_tc)\n",
    "print(\"\\nHardest test cases (lowest avg F1 across all groups):\")\n",
    "for rank, index in enumerate(sorted_indices[:5]):\n",
    "    print(f\"  {rank + 1}. {all_test_case_ids[index]}: avg F1 = {avg_f1_per_tc[index]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Summary Dashboard\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY DASHBOARD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best config group\n",
    "best_hash = None\n",
    "best_f1 = -1.0\n",
    "for fingerprint_hash, group_summaries in groups.items():\n",
    "    f1_values = []\n",
    "    for summary in group_summaries:\n",
    "        macro = summary.get(\"macro_averaged\", {})\n",
    "        f1 = extract_f1_from_summary(macro, METRIC_PATH, STRATEGY)\n",
    "        if f1 is not None:\n",
    "            f1_values.append(f1)\n",
    "    avg_f1 = np.mean(f1_values) if f1_values else 0.0\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        best_hash = fingerprint_hash\n",
    "\n",
    "if best_hash:\n",
    "    best_label = group_labels.get(best_hash, best_hash[:8])\n",
    "    best_fp = groups[best_hash][0].get(\"config_fingerprint\", {})\n",
    "    print(f\"\\nBest config: Group {best_label} (hash: {best_hash})\")\n",
    "    print(f\"  Avg F1: {best_f1:.3f}\")\n",
    "    print(f\"  Extraction prompt: v{best_fp.get('extraction_prompt_version', '?')}\")\n",
    "    print(f\"  Query prompt: v{best_fp.get('query_prompt_version', '?')}\")\n",
    "    print(f\"  Query model: {best_fp.get('query_model', '?')}\")\n",
    "    print(f\"  Reranker: {best_fp.get('reranker_model', 'None')}\")\n",
    "    print(f\"  N runs: {len(groups[best_hash])}\")\n",
    "\n",
    "# Hardest test cases\n",
    "print(\"\\nHardest test cases (avg F1 across all groups):\")\n",
    "for rank, index in enumerate(sorted_indices[:3]):\n",
    "    print(f\"  {rank + 1}. {all_test_case_ids[index]}: {avg_f1_per_tc[index]:.3f}\")\n",
    "\n",
    "# Recommended next experiments\n",
    "print(\"\\nRecommended next steps:\")\n",
    "multi_run_count = sum(1 for g in groups.values() if len(g) >= 2)\n",
    "if multi_run_count == 0:\n",
    "    print(\"  - Run same config again to measure LLM variance (need 2+ runs per config)\")\n",
    "if len(groups) < 3:\n",
    "    print(\"  - Try a new prompt version to expand comparison space\")\n",
    "print(\"  - Investigate hardest test cases to understand failure modes\")\n",
    "print(f\"  - Total runs analyzed: {len(summaries)} across {len(groups)} config groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
