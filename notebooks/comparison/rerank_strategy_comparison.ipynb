{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Rerank Strategy Comparison\n",
    "\n",
    "Compare pre-rerank vector search baseline vs post-rerank results for all strategies.\n",
    "\n",
    "**What this notebook answers:**\n",
    "- Does reranking actually improve over vector search alone?\n",
    "- Which reranking strategy performs best?\n",
    "- For which test cases does reranking help — or hurt?\n",
    "- What are the optimal threshold and top-N settings per strategy?\n",
    "\n",
    "When multiple runs are loaded, metrics are averaged to reduce LLM query generation variance.  \n",
    "Requires runs with reranking data (`phase2.ipynb` or `phase1_reranking_comparison.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 — Setup, Imports & Helpers\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from memory_retrieval.experiments.comparison import (\n",
    "    group_runs_by_fingerprint,\n",
    "    load_run_summaries,\n",
    "    load_subrun_summaries,\n",
    "    reconstruct_fingerprint_from_run,\n",
    ")\n",
    "from memory_retrieval.experiments.metrics_adapter import extract_metric_from_nested\n",
    "from memory_retrieval.infra.io import load_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    PHASE1,\n",
    "    PHASE2,\n",
    "    list_runs,\n",
    "    update_config_fingerprint,\n",
    ")\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "def is_reranking_summary(summary):\n",
    "    \"\"\"Check if a summary contains reranking data.\"\"\"\n",
    "    return \"baseline\" in summary or \"rerank_strategies\" in summary\n",
    "\n",
    "\n",
    "def get_all_strategies(summaries):\n",
    "    \"\"\"Auto-detect all rerank strategy names present across summaries.\"\"\"\n",
    "    strategy_names = set()\n",
    "    for summary in summaries:\n",
    "        for strategy_name in summary.get(\"rerank_strategies\", {}).keys():\n",
    "            strategy_names.add(strategy_name)\n",
    "        for strategy_name in summary.get(\"macro_averaged\", {}).get(\"post_rerank\", {}).keys():\n",
    "            strategy_names.add(strategy_name)\n",
    "    return sorted(strategy_names)\n",
    "\n",
    "\n",
    "def extract_macro_stage_metrics(summary, stage_key):\n",
    "    \"\"\"Extract macro-averaged P/R/F1/MRR for a given stage key.\n",
    "\n",
    "    Stage keys:\n",
    "    - pre_rerank_distance: pre-rerank at optimal distance threshold\n",
    "    - pre_rerank_top_n: pre-rerank at optimal top-N\n",
    "    - post_rerank_threshold:<strategy>: post-rerank at optimal rerank threshold\n",
    "    - post_rerank_top_n:<strategy>: post-rerank at optimal top-N\n",
    "    \"\"\"\n",
    "    macro = summary.get(\"macro_averaged\", {})\n",
    "    if stage_key == \"pre_rerank_distance\":\n",
    "        return macro.get(\"pre_rerank\", {}).get(\"at_optimal_distance_threshold\", {})\n",
    "    elif stage_key == \"pre_rerank_top_n\":\n",
    "        return macro.get(\"pre_rerank\", {}).get(\"at_optimal_top_n\", {})\n",
    "    elif stage_key.startswith(\"post_rerank_threshold:\"):\n",
    "        strategy_name = stage_key.split(\":\", 1)[1]\n",
    "        return macro.get(\"post_rerank\", {}).get(strategy_name, {}).get(\"at_optimal_threshold\", {})\n",
    "    elif stage_key.startswith(\"post_rerank_top_n:\"):\n",
    "        strategy_name = stage_key.split(\":\", 1)[1]\n",
    "        return macro.get(\"post_rerank\", {}).get(strategy_name, {}).get(\"at_optimal_top_n\", {})\n",
    "    return {}\n",
    "\n",
    "\n",
    "def average_stage_metrics_across_runs(summaries, stage_key):\n",
    "    \"\"\"Average P/R/F1/MRR across multiple runs for a given stage key.\"\"\"\n",
    "    all_metrics = [extract_macro_stage_metrics(summary, stage_key) for summary in summaries]\n",
    "    valid_metrics = [metrics for metrics in all_metrics if metrics and \"f1\" in metrics]\n",
    "    if not valid_metrics:\n",
    "        return {}\n",
    "    return {\n",
    "        key: round(float(np.mean([metrics[key] for metrics in valid_metrics if key in metrics])), 4)\n",
    "        for key in [\"precision\", \"recall\", \"f1\", \"mrr\"]\n",
    "        if any(key in metrics for metrics in valid_metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_per_test_case_stage_f1(summary, stage_key):\n",
    "    \"\"\"Extract per-test-case F1 for a given stage key.\"\"\"\n",
    "    f1_by_test_case = {}\n",
    "    for test_case_id, per_tc in summary.get(\"per_test_case\", {}).items():\n",
    "        if stage_key == \"pre_rerank_distance\":\n",
    "            f1 = extract_metric_from_nested(per_tc, \"pre_rerank\", None, metric_key=\"f1\")\n",
    "        elif stage_key == \"pre_rerank_top_n\":\n",
    "            f1 = per_tc.get(\"pre_rerank\", {}).get(\"top_n\", {}).get(\"at_optimal\", {}).get(\"f1\")\n",
    "        elif stage_key.startswith(\"post_rerank_threshold:\"):\n",
    "            strategy_name = stage_key.split(\":\", 1)[1]\n",
    "            f1 = extract_metric_from_nested(per_tc, \"post_rerank\", strategy_name, metric_key=\"f1\")\n",
    "        elif stage_key.startswith(\"post_rerank_top_n:\"):\n",
    "            strategy_name = stage_key.split(\":\", 1)[1]\n",
    "            f1 = (\n",
    "                per_tc.get(\"post_rerank\", {})\n",
    "                .get(strategy_name, {})\n",
    "                .get(\"top_n\", {})\n",
    "                .get(\"at_optimal\", {})\n",
    "                .get(\"f1\")\n",
    "            )\n",
    "        else:\n",
    "            f1 = None\n",
    "        if f1 is not None:\n",
    "            f1_by_test_case[test_case_id] = f1\n",
    "    return f1_by_test_case\n",
    "\n",
    "\n",
    "def average_per_test_case_stage_f1_across_runs(summaries, stage_key):\n",
    "    \"\"\"Average per-test-case F1 across multiple runs for a given stage key.\"\"\"\n",
    "    all_test_case_ids = set()\n",
    "    for summary in summaries:\n",
    "        all_test_case_ids.update(summary.get(\"per_test_case\", {}).keys())\n",
    "    averaged_f1 = {}\n",
    "    for test_case_id in sorted(all_test_case_ids):\n",
    "        f1_values = []\n",
    "        for summary in summaries:\n",
    "            per_tc_f1_map = extract_per_test_case_stage_f1(summary, stage_key)\n",
    "            if test_case_id in per_tc_f1_map:\n",
    "                f1_values.append(per_tc_f1_map[test_case_id])\n",
    "        if f1_values:\n",
    "            averaged_f1[test_case_id] = round(float(np.mean(f1_values)), 4)\n",
    "    return averaged_f1\n",
    "\n",
    "\n",
    "def get_averaged_sweep_curve(summaries, sweep_type):\n",
    "    \"\"\"Compute averaged F1 curve across runs for a given sweep type.\n",
    "\n",
    "    sweep_type options:\n",
    "    - distance_threshold: pre-rerank distance threshold sweep\n",
    "    - pre_top_n: pre-rerank top-N sweep\n",
    "    - rerank_threshold:<strategy>: post-rerank rerank score threshold sweep\n",
    "    - post_top_n:<strategy>: post-rerank top-N sweep\n",
    "\n",
    "    Returns (x_values, mean_f1, min_f1, max_f1, mean_optimal_x).\n",
    "    \"\"\"\n",
    "    all_sweep_dicts = []\n",
    "    optimal_x_values = []\n",
    "\n",
    "    for summary in summaries:\n",
    "        if sweep_type == \"distance_threshold\":\n",
    "            sweep_section = summary.get(\"baseline\", {}).get(\"distance_threshold_sweep\", {})\n",
    "            full_sweep = sweep_section.get(\"full_sweep\", [])\n",
    "            x_key = \"threshold\"\n",
    "            optimal_x_values.append(sweep_section.get(\"optimal_threshold\"))\n",
    "        elif sweep_type == \"pre_top_n\":\n",
    "            sweep_section = summary.get(\"baseline\", {}).get(\"top_n_sweep\", {})\n",
    "            full_sweep = sweep_section.get(\"full_sweep\", [])\n",
    "            x_key = \"top_n\"\n",
    "            optimal_x_values.append(sweep_section.get(\"optimal_n\"))\n",
    "        elif sweep_type.startswith(\"rerank_threshold:\"):\n",
    "            strategy_name = sweep_type.split(\":\", 1)[1]\n",
    "            sweep_section = (\n",
    "                summary.get(\"rerank_strategies\", {})\n",
    "                .get(strategy_name, {})\n",
    "                .get(\"threshold_sweep\", {})\n",
    "            )\n",
    "            full_sweep = sweep_section.get(\"full_sweep\", [])\n",
    "            x_key = \"threshold\"\n",
    "            optimal_x_values.append(sweep_section.get(\"optimal_threshold\"))\n",
    "        elif sweep_type.startswith(\"post_top_n:\"):\n",
    "            strategy_name = sweep_type.split(\":\", 1)[1]\n",
    "            sweep_section = (\n",
    "                summary.get(\"rerank_strategies\", {}).get(strategy_name, {}).get(\"top_n_sweep\", {})\n",
    "            )\n",
    "            full_sweep = sweep_section.get(\"full_sweep\", [])\n",
    "            x_key = \"top_n\"\n",
    "            optimal_x_values.append(sweep_section.get(\"optimal_n\"))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if full_sweep:\n",
    "            sweep_dict = {round(entry[x_key], 4): entry[\"f1\"] for entry in full_sweep}\n",
    "            all_sweep_dicts.append(sweep_dict)\n",
    "\n",
    "    if not all_sweep_dicts:\n",
    "        return [], np.array([]), np.array([]), np.array([]), None\n",
    "\n",
    "    all_x_values = sorted(set().union(*[set(sweep.keys()) for sweep in all_sweep_dicts]))\n",
    "    mean_f1_values = np.zeros(len(all_x_values))\n",
    "    min_f1_values = np.zeros(len(all_x_values))\n",
    "    max_f1_values = np.zeros(len(all_x_values))\n",
    "\n",
    "    for x_index, x_value in enumerate(all_x_values):\n",
    "        f1_at_x = [sweep[x_value] for sweep in all_sweep_dicts if x_value in sweep]\n",
    "        if f1_at_x:\n",
    "            mean_f1_values[x_index] = np.mean(f1_at_x)\n",
    "            min_f1_values[x_index] = min(f1_at_x)\n",
    "            max_f1_values[x_index] = max(f1_at_x)\n",
    "\n",
    "    valid_optimal_x = [x for x in optimal_x_values if x is not None]\n",
    "    mean_optimal_x = float(np.mean(valid_optimal_x)) if valid_optimal_x else None\n",
    "\n",
    "    return all_x_values, mean_f1_values, min_f1_values, max_f1_values, mean_optimal_x\n",
    "\n",
    "\n",
    "def load_selected_run_summaries(\n",
    "    phase,\n",
    "    run_source,\n",
    "    selected_parent_run_ids=None,\n",
    "    selected_subrun_ids=None,\n",
    "):\n",
    "    \"\"\"Load parent/subrun summaries based on RUN_SOURCE selection.\"\"\"\n",
    "    valid_sources = {\"parent\", \"subrun\", \"both\"}\n",
    "    if run_source not in valid_sources:\n",
    "        allowed = \", \".join(sorted(valid_sources))\n",
    "        raise ValueError(f\"Invalid RUN_SOURCE={run_source!r}. Expected one of: {allowed}\")\n",
    "\n",
    "    parent_summaries = []\n",
    "    subrun_summaries = []\n",
    "\n",
    "    if run_source in {\"parent\", \"both\"}:\n",
    "        parent_summaries = [\n",
    "            {**summary, \"source_kind\": \"parent\"}\n",
    "            for summary in load_run_summaries(phase, run_ids=selected_parent_run_ids)\n",
    "        ]\n",
    "\n",
    "    if run_source in {\"subrun\", \"both\"}:\n",
    "        subrun_summaries = [\n",
    "            {**summary, \"source_kind\": \"subrun\"}\n",
    "            for summary in load_subrun_summaries(\n",
    "                phase,\n",
    "                parent_run_ids=selected_parent_run_ids,\n",
    "                subrun_ids=selected_subrun_ids,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return parent_summaries + subrun_summaries\n",
    "\n",
    "\n",
    "# --- Backfill fingerprints for existing runs (same as cross_run_comparison) ---\n",
    "backfilled_count = 0\n",
    "for phase in [PHASE1, PHASE2]:\n",
    "    for run in list_runs(phase):\n",
    "        run_dir = run[\"run_dir\"]\n",
    "        run_metadata = load_json(run_dir / \"run.json\")\n",
    "        if \"config_fingerprint\" not in run_metadata:\n",
    "            results_dir = run_dir / \"results\"\n",
    "            if results_dir.exists() and list(results_dir.glob(\"*.json\")):\n",
    "                fingerprint = reconstruct_fingerprint_from_run(run_dir)\n",
    "                update_config_fingerprint(run_dir, fingerprint)\n",
    "                backfilled_count += 1\n",
    "                run_id = run[\"run_id\"]\n",
    "                fingerprint_hash = fingerprint[\"fingerprint_hash\"]\n",
    "                print(f\"  Backfilled: {run_id} -> {fingerprint_hash}\")\n",
    "\n",
    "if backfilled_count > 0:\n",
    "    print(f\"\\nBackfilled {backfilled_count} runs.\")\n",
    "else:\n",
    "    print(\"All runs already have fingerprints.\")\n",
    "\n",
    "print(\"\\nSetup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Configuration\n",
    "\n",
    "# Which phase to analyze\n",
    "PHASE = PHASE2\n",
    "\n",
    "# Which run source to analyze:\n",
    "# - \"parent\": only parent runs (default, backward compatible)\n",
    "# - \"subrun\": only subruns\n",
    "# - \"both\": parents + subruns together\n",
    "RUN_SOURCE = \"parent\"\n",
    "\n",
    "# Optional: restrict to specific parent run IDs (None = all parent runs in phase).\n",
    "# Used when RUN_SOURCE includes \"parent\", and as a parent constraint for subrun loading.\n",
    "SELECTED_PARENT_RUN_IDS = None  # e.g., ['run_20260218_171450', 'run_20260218_172142']\n",
    "\n",
    "# Optional: restrict to specific subrun IDs (None = all subruns under selected parents).\n",
    "# Used only when RUN_SOURCE includes \"subrun\".\n",
    "SELECTED_SUBRUN_IDS = None  # e.g., ['run_20260219_131817']\n",
    "\n",
    "# Backward compatibility: map legacy SELECTED_RUN_IDS -> SELECTED_PARENT_RUN_IDS once.\n",
    "if (\n",
    "    \"SELECTED_RUN_IDS\" in globals()\n",
    "    and SELECTED_RUN_IDS is not None\n",
    "    and SELECTED_PARENT_RUN_IDS is None\n",
    "):\n",
    "    SELECTED_PARENT_RUN_IDS = SELECTED_RUN_IDS\n",
    "    print(\"Deprecated: SELECTED_RUN_IDS is now SELECTED_PARENT_RUN_IDS.\")\n",
    "\n",
    "# Optional: restrict to a specific config fingerprint hash (None = all loaded runs).\n",
    "# Useful when multiple configs exist and you want to isolate one.\n",
    "# Set this after inspecting Cell 2's output, then re-run from Cell 2 onward.\n",
    "# SELECTED_FINGERPRINT_HASH = '9c3ca8dc'  # Qwen 0.6B reranker\n",
    "SELECTED_FINGERPRINT_HASH = None\n",
    "\n",
    "print(f\"Phase: {PHASE}\")\n",
    "print(f\"Run source: {RUN_SOURCE}\")\n",
    "print(f\"Parent run filter: {SELECTED_PARENT_RUN_IDS or 'all parents'}\")\n",
    "print(f\"Subrun filter: {SELECTED_SUBRUN_IDS or 'all subruns (within parent filter)'}\")\n",
    "print(f\"Fingerprint filter: {SELECTED_FINGERPRINT_HASH or 'none (all configs)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load Data & Overview Table\n",
    "\n",
    "all_summaries = load_selected_run_summaries(\n",
    "    PHASE,\n",
    "    RUN_SOURCE,\n",
    "    selected_parent_run_ids=SELECTED_PARENT_RUN_IDS,\n",
    "    selected_subrun_ids=SELECTED_SUBRUN_IDS,\n",
    ")\n",
    "\n",
    "parent_count = sum(1 for summary in all_summaries if summary.get(\"source_kind\") == \"parent\")\n",
    "subrun_count = sum(1 for summary in all_summaries if summary.get(\"source_kind\") == \"subrun\")\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(all_summaries)} run summaries for {PHASE} \"\n",
    "    f\"(parents={parent_count}, subruns={subrun_count}, source={RUN_SOURCE})\"\n",
    ")\n",
    "\n",
    "if all_summaries:\n",
    "    fingerprint_counts = {}\n",
    "    for summary in all_summaries:\n",
    "        fingerprint = summary.get(\"config_fingerprint\", {})\n",
    "        fingerprint_hash = fingerprint.get(\"fingerprint_hash\", \"unknown\")\n",
    "        fingerprint_counts[fingerprint_hash] = fingerprint_counts.get(fingerprint_hash, 0) + 1\n",
    "\n",
    "    print(\"Fingerprint coverage:\")\n",
    "    for fingerprint_hash, count in sorted(fingerprint_counts.items()):\n",
    "        print(f\"  {fingerprint_hash}: {count} run(s)\")\n",
    "else:\n",
    "    print(\"Fingerprint coverage: no runs loaded\")\n",
    "\n",
    "reranking_summaries = [summary for summary in all_summaries if is_reranking_summary(summary)]\n",
    "non_reranking_count = len(all_summaries) - len(reranking_summaries)\n",
    "print(\n",
    "    f\"Reranking summaries: {len(reranking_summaries)} (skipped {non_reranking_count} non-reranking)\"\n",
    ")\n",
    "\n",
    "if not reranking_summaries:\n",
    "    raise RuntimeError(\n",
    "        \"No reranking summaries found. This notebook requires runs with reranking data.\\n\"\n",
    "        \"Run phase2.ipynb or phase1_reranking_comparison.ipynb first.\"\n",
    "    )\n",
    "\n",
    "# Optionally filter to a specific fingerprint\n",
    "if SELECTED_FINGERPRINT_HASH:\n",
    "    config_groups = group_runs_by_fingerprint(reranking_summaries)\n",
    "    if SELECTED_FINGERPRINT_HASH not in config_groups:\n",
    "        available_hashes = list(config_groups.keys())\n",
    "        print(f\"Fingerprint not found. Available: {available_hashes}\")\n",
    "        raise RuntimeError(f\"Fingerprint {SELECTED_FINGERPRINT_HASH} not found\")\n",
    "    analysis_summaries = config_groups[SELECTED_FINGERPRINT_HASH]\n",
    "    print(\n",
    "        f\"Filtered to {len(analysis_summaries)} runs with fingerprint {SELECTED_FINGERPRINT_HASH}\"\n",
    "    )\n",
    "else:\n",
    "    config_groups = group_runs_by_fingerprint(reranking_summaries)\n",
    "    if len(config_groups) > 1:\n",
    "        print(\n",
    "            f\"\\nWarning: {len(config_groups)} different configs found — averaging across ALL configs.\"\n",
    "        )\n",
    "        print(\"Set SELECTED_FINGERPRINT_HASH to focus on one config. Available hashes:\")\n",
    "        for fingerprint_hash, group_summaries in config_groups.items():\n",
    "            fingerprint = group_summaries[0].get(\"config_fingerprint\", {})\n",
    "            extraction_version = fingerprint.get(\"extraction_prompt_version\", \"?\")\n",
    "            query_version = fingerprint.get(\"query_prompt_version\", \"?\")\n",
    "            print(\n",
    "                f\"  {fingerprint_hash}: {len(group_summaries)} runs, ext={extraction_version}, q={query_version}\"\n",
    "            )\n",
    "    analysis_summaries = reranking_summaries\n",
    "\n",
    "# Show config info\n",
    "if analysis_summaries:\n",
    "    config_fingerprint = analysis_summaries[0].get(\"config_fingerprint\", {})\n",
    "    if config_fingerprint:\n",
    "        print(f\"\\nConfig (first run):\")\n",
    "        print(f\"  Extraction prompt: v{config_fingerprint.get('extraction_prompt_version', '?')}\")\n",
    "        print(f\"  Query prompt:      v{config_fingerprint.get('query_prompt_version', '?')}\")\n",
    "        print(f\"  Query model:       {config_fingerprint.get('query_model', '?')}\")\n",
    "        print(f\"  Reranker model:    {config_fingerprint.get('reranker_model', '?')}\")\n",
    "\n",
    "all_strategies = get_all_strategies(analysis_summaries)\n",
    "print(f\"\\nRerank strategies: {all_strategies}\")\n",
    "print(f\"Runs being averaged: {len(analysis_summaries)}\")\n",
    "\n",
    "# --- Build stage keys for all comparisons ---\n",
    "# These variables are used in all subsequent cells.\n",
    "comparison_stage_keys = [\"pre_rerank_distance\", \"pre_rerank_top_n\"]\n",
    "for strategy_name in all_strategies:\n",
    "    comparison_stage_keys.append(f\"post_rerank_threshold:{strategy_name}\")\n",
    "    comparison_stage_keys.append(f\"post_rerank_top_n:{strategy_name}\")\n",
    "\n",
    "stage_display_names = {\n",
    "    \"pre_rerank_distance\": \"Pre-Rerank (Distance Threshold)\",\n",
    "    \"pre_rerank_top_n\": \"Pre-Rerank (Top-N)\",\n",
    "}\n",
    "for strategy_name in all_strategies:\n",
    "    stage_display_names[f\"post_rerank_threshold:{strategy_name}\"] = (\n",
    "        f\"Post-Rerank [{strategy_name}] (Threshold)\"\n",
    "    )\n",
    "    stage_display_names[f\"post_rerank_top_n:{strategy_name}\"] = (\n",
    "        f\"Post-Rerank [{strategy_name}] (Top-N)\"\n",
    "    )\n",
    "\n",
    "# Compute averaged metrics for every stage\n",
    "stage_averaged_metrics = {\n",
    "    stage_key: average_stage_metrics_across_runs(analysis_summaries, stage_key)\n",
    "    for stage_key in comparison_stage_keys\n",
    "}\n",
    "\n",
    "# --- Print overview table ---\n",
    "pre_rerank_baseline_f1 = stage_averaged_metrics.get(\"pre_rerank_distance\", {}).get(\"f1\", 0.0)\n",
    "\n",
    "print(f\"\\n{'Stage':<50} {'Precision':>10} {'Recall':>8} {'F1':>8} {'Delta':>9} {'MRR':>8}\")\n",
    "print(\"-\" * 98)\n",
    "\n",
    "for stage_key in comparison_stage_keys:\n",
    "    metrics = stage_averaged_metrics[stage_key]\n",
    "    if not metrics:\n",
    "        continue\n",
    "    display_name = stage_display_names[stage_key]\n",
    "    f1 = metrics.get(\"f1\", 0.0)\n",
    "    delta_str = f\"{f1 - pre_rerank_baseline_f1:+.3f}\" if stage_key != \"pre_rerank_distance\" else \"—\"\n",
    "    print(\n",
    "        f\"{display_name:<50} \"\n",
    "        f\"{metrics.get('precision', 0.0):>10.4f} \"\n",
    "        f\"{metrics.get('recall', 0.0):>8.4f} \"\n",
    "        f\"{f1:>8.4f} \"\n",
    "        f\"{delta_str:>9} \"\n",
    "        f\"{metrics.get('mrr', 0.0):>8.4f}\"\n",
    "    )\n",
    "    if stage_key == \"pre_rerank_top_n\":\n",
    "        print()  # blank line between pre-rerank and post-rerank sections\n",
    "\n",
    "print(f\"\\nBaseline F1 (pre-rerank at optimal distance threshold): {pre_rerank_baseline_f1:.4f}\")\n",
    "print(\"Delta is relative to this baseline.\")\n",
    "print()\n",
    "print(\"How to read these F1 values — there are 3 levels of optimism:\")\n",
    "print(\"  [most optimistic]  subrun_comparison Cell 4: each test case picks its own threshold\")\n",
    "print(\"  [this table]       each RUN picks one threshold best for all its test cases combined,\")\n",
    "print(\"                     then we average the 30 best results — not deployable, because in\")\n",
    "print(\"                     production you pick a threshold before seeing any queries\")\n",
    "print(\"  [most realistic]   Cell 4 sweep below: one fixed threshold for all runs — this is\")\n",
    "print(\"                     what you actually set in config and the F1 you can expect to see\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Metrics Bar Chart: Pre-Rerank vs All Strategies\n",
    "# Shows F1, Precision, Recall, MRR side by side for each stage.\n",
    "# Top-N versions are included to show the full picture.\n",
    "\n",
    "# Stages to show in bars (threshold-optimized versions = the recommended metrics)\n",
    "bar_stage_keys = [\"pre_rerank_distance\"]\n",
    "for strategy_name in all_strategies:\n",
    "    bar_stage_keys.append(f\"post_rerank_threshold:{strategy_name}\")\n",
    "\n",
    "bar_short_labels = [\"Pre-Rerank\\n(Dist. Thresh.)\"]\n",
    "for strategy_name in all_strategies:\n",
    "    bar_short_labels.append(f\"{strategy_name}\\n(Rerank Thresh.)\")\n",
    "\n",
    "# Color scheme: pre-rerank = gray, each strategy = distinct color\n",
    "strategy_palette = plt.cm.Set2(np.linspace(0, 0.8, max(len(all_strategies), 3)))\n",
    "bar_colors = [\"#95a5a6\"]  # gray for pre-rerank\n",
    "for strategy_index in range(len(all_strategies)):\n",
    "    bar_colors.append(strategy_palette[strategy_index])\n",
    "\n",
    "metric_names = [\"f1\", \"precision\", \"recall\", \"mrr\"]\n",
    "metric_display_names = {\"f1\": \"F1\", \"precision\": \"Precision\", \"recall\": \"Recall\", \"mrr\": \"MRR\"}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "for metric_index, metric_name in enumerate(metric_names):\n",
    "    ax = axes[metric_index]\n",
    "\n",
    "    bar_values = [\n",
    "        stage_averaged_metrics.get(stage_key, {}).get(metric_name, 0.0)\n",
    "        for stage_key in bar_stage_keys\n",
    "    ]\n",
    "\n",
    "    bars = ax.bar(\n",
    "        range(len(bar_stage_keys)),\n",
    "        bar_values,\n",
    "        color=bar_colors[: len(bar_stage_keys)],\n",
    "        alpha=0.85,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "\n",
    "    # Baseline reference line at pre-rerank level\n",
    "    baseline_value = stage_averaged_metrics.get(\"pre_rerank_distance\", {}).get(metric_name, 0.0)\n",
    "    if baseline_value > 0:\n",
    "        ax.axhline(baseline_value, color=\"#7f8c8d\", linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "    # Value annotations on bars\n",
    "    for bar in bars:\n",
    "        bar_height = bar.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{bar_height:.3f}\",\n",
    "            xy=(bar.get_x() + bar.get_width() / 2, bar_height),\n",
    "            xytext=(0, 4),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(range(len(bar_stage_keys)))\n",
    "    ax.set_xticklabels(bar_short_labels, fontsize=7.5)\n",
    "    ax.set_title(metric_display_names[metric_name], fontsize=11)\n",
    "    ax.set_ylim(0, 1.12)\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    ax.set_ylabel(metric_display_names[metric_name] if metric_index == 0 else \"\")\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Pre-Rerank vs Rerank Strategies — {PHASE}\\n\"\n",
    "    f\"(averaged across {len(analysis_summaries)} run(s), dashed line = pre-rerank baseline)\",\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Threshold Sweep Comparison\n",
    "# Left: pre-rerank distance threshold sweep (how F1 changes with distance cutoff)\n",
    "# Right: per-strategy rerank score threshold sweep\n",
    "# The axes are intentionally separate — distance and rerank score are different scales.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "strategy_palette = plt.cm.Set2(np.linspace(0, 0.8, max(len(all_strategies), 3)))\n",
    "\n",
    "# --- Left: Pre-rerank distance threshold sweep ---\n",
    "ax_left = axes[0]\n",
    "\n",
    "distance_x_values, distance_mean_f1, distance_min_f1, distance_max_f1, optimal_distance = (\n",
    "    get_averaged_sweep_curve(analysis_summaries, \"distance_threshold\")\n",
    ")\n",
    "\n",
    "if distance_x_values:\n",
    "    num_runs_with_data = len(analysis_summaries)\n",
    "    ax_left.plot(\n",
    "        distance_x_values,\n",
    "        distance_mean_f1,\n",
    "        color=\"#2c3e50\",\n",
    "        linewidth=2.5,\n",
    "        label=f\"Pre-Rerank (n={num_runs_with_data})\",\n",
    "    )\n",
    "    if len(analysis_summaries) > 1:\n",
    "        ax_left.fill_between(\n",
    "            distance_x_values,\n",
    "            distance_min_f1,\n",
    "            distance_max_f1,\n",
    "            color=\"#2c3e50\",\n",
    "            alpha=0.12,\n",
    "            label=\"Min-max range\",\n",
    "        )\n",
    "    if optimal_distance is not None:\n",
    "        optimal_index = int(np.argmin(np.abs(np.array(distance_x_values) - optimal_distance)))\n",
    "        optimal_f1_value = distance_mean_f1[optimal_index]\n",
    "        ax_left.axvline(optimal_distance, color=\"#2c3e50\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "        ax_left.annotate(\n",
    "            f\"d={optimal_distance:.3f}\\nF1={optimal_f1_value:.3f}\",\n",
    "            xy=(optimal_distance, optimal_f1_value),\n",
    "            xytext=(8, -30),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "            color=\"#2c3e50\",\n",
    "        )\n",
    "else:\n",
    "    ax_left.text(0.5, 0.5, \"No distance sweep data\", transform=ax_left.transAxes, ha=\"center\")\n",
    "\n",
    "ax_left.set_xlabel(\"Distance Threshold (lower = stricter filter, fewer results)\")\n",
    "ax_left.set_ylabel(\"Macro F1\")\n",
    "ax_left.set_title(\"Pre-Rerank: Distance Threshold Sweep\")\n",
    "ax_left.legend(fontsize=8)\n",
    "ax_left.set_ylim(0, 1.05)\n",
    "ax_left.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right: Per-strategy rerank score threshold sweep ---\n",
    "ax_right = axes[1]\n",
    "line_styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "\n",
    "for strategy_index, strategy_name in enumerate(all_strategies):\n",
    "    rerank_x_values, rerank_mean_f1, rerank_min_f1, rerank_max_f1, optimal_rerank = (\n",
    "        get_averaged_sweep_curve(analysis_summaries, f\"rerank_threshold:{strategy_name}\")\n",
    "    )\n",
    "    if not rerank_x_values:\n",
    "        continue\n",
    "\n",
    "    color = strategy_palette[strategy_index]\n",
    "    line_style = line_styles[strategy_index % len(line_styles)]\n",
    "\n",
    "    ax_right.plot(\n",
    "        rerank_x_values,\n",
    "        rerank_mean_f1,\n",
    "        color=color,\n",
    "        linewidth=2.5,\n",
    "        linestyle=line_style,\n",
    "        label=strategy_name,\n",
    "    )\n",
    "    if len(analysis_summaries) > 1:\n",
    "        ax_right.fill_between(\n",
    "            rerank_x_values, rerank_min_f1, rerank_max_f1, color=color, alpha=0.12\n",
    "        )\n",
    "    if optimal_rerank is not None:\n",
    "        optimal_index = int(np.argmin(np.abs(np.array(rerank_x_values) - optimal_rerank)))\n",
    "        optimal_f1_value = rerank_mean_f1[optimal_index]\n",
    "        ax_right.axvline(optimal_rerank, color=color, linestyle=\":\", alpha=0.6, linewidth=1.5)\n",
    "        offset_y = -30 - strategy_index * 28\n",
    "        ax_right.annotate(\n",
    "            f\"t={optimal_rerank:.3f}\\nF1={optimal_f1_value:.3f}\",\n",
    "            xy=(optimal_rerank, optimal_f1_value),\n",
    "            xytext=(8, offset_y),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "if not all_strategies:\n",
    "    ax_right.text(0.5, 0.5, \"No rerank strategy data\", transform=ax_right.transAxes, ha=\"center\")\n",
    "\n",
    "ax_right.set_xlabel(\"Rerank Score Threshold (higher = stricter filter)\")\n",
    "ax_right.set_ylabel(\"Macro F1\")\n",
    "ax_right.set_title(\"Post-Rerank: Rerank Score Threshold Sweep\")\n",
    "if all_strategies:\n",
    "    ax_right.legend(fontsize=8)\n",
    "ax_right.set_ylim(0, 1.05)\n",
    "ax_right.grid(True, alpha=0.3)\n",
    "\n",
    "num_runs = len(analysis_summaries)\n",
    "fig.suptitle(\n",
    "    f\"Threshold Sweep Comparison — {PHASE} (averaged across {num_runs} run(s))\",\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Annotations (d=.../t=..., F1=...) = the most realistic deployment estimate [level 3/3].\")\n",
    "print(\"One fixed threshold is chosen (average of per-run optima), F1 is read off the averaged\")\n",
    "print(\"curve at that point. Lower than the table above because every run is forced to share the\")\n",
    "print(\"same threshold — nobody gets to pick their own. What you see here is what you will get.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Top-N Sweep Comparison\n",
    "# All stages on the same chart (same x-axis: number of returned results).\n",
    "# Shows how F1 evolves as you return more results, for each stage.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "strategy_palette = plt.cm.Set2(np.linspace(0, 0.8, max(len(all_strategies), 3)))\n",
    "line_styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "\n",
    "# Pre-rerank top-N sweep\n",
    "pre_top_n_x, pre_top_n_mean_f1, pre_top_n_min_f1, pre_top_n_max_f1, optimal_pre_top_n = (\n",
    "    get_averaged_sweep_curve(analysis_summaries, \"pre_top_n\")\n",
    ")\n",
    "\n",
    "if pre_top_n_x:\n",
    "    ax.plot(\n",
    "        pre_top_n_x,\n",
    "        pre_top_n_mean_f1,\n",
    "        color=\"#2c3e50\",\n",
    "        linewidth=2.5,\n",
    "        linestyle=\"-\",\n",
    "        label=\"Pre-Rerank (distance-sorted)\",\n",
    "        zorder=10,\n",
    "    )\n",
    "    if len(analysis_summaries) > 1:\n",
    "        ax.fill_between(pre_top_n_x, pre_top_n_min_f1, pre_top_n_max_f1, color=\"#2c3e50\", alpha=0.1)\n",
    "    if optimal_pre_top_n is not None:\n",
    "        optimal_index = int(np.argmin(np.abs(np.array(pre_top_n_x) - optimal_pre_top_n)))\n",
    "        ax.axvline(optimal_pre_top_n, color=\"#2c3e50\", linestyle=\":\", alpha=0.5, linewidth=1)\n",
    "        ax.annotate(\n",
    "            f\"N={int(round(optimal_pre_top_n))}\",\n",
    "            xy=(optimal_pre_top_n, pre_top_n_mean_f1[optimal_index]),\n",
    "            xytext=(5, 6),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "            color=\"#2c3e50\",\n",
    "        )\n",
    "\n",
    "# Per-strategy top-N sweeps\n",
    "for strategy_index, strategy_name in enumerate(all_strategies):\n",
    "    post_top_n_x, post_top_n_mean_f1, post_top_n_min_f1, post_top_n_max_f1, optimal_post_top_n = (\n",
    "        get_averaged_sweep_curve(analysis_summaries, f\"post_top_n:{strategy_name}\")\n",
    "    )\n",
    "    if not post_top_n_x:\n",
    "        continue\n",
    "\n",
    "    color = strategy_palette[strategy_index]\n",
    "    line_style = line_styles[strategy_index % len(line_styles)]\n",
    "\n",
    "    ax.plot(\n",
    "        post_top_n_x,\n",
    "        post_top_n_mean_f1,\n",
    "        color=color,\n",
    "        linewidth=2.5,\n",
    "        linestyle=line_style,\n",
    "        label=strategy_name,\n",
    "    )\n",
    "    if len(analysis_summaries) > 1:\n",
    "        ax.fill_between(post_top_n_x, post_top_n_min_f1, post_top_n_max_f1, color=color, alpha=0.1)\n",
    "    if optimal_post_top_n is not None:\n",
    "        optimal_index = int(np.argmin(np.abs(np.array(post_top_n_x) - optimal_post_top_n)))\n",
    "        ax.axvline(optimal_post_top_n, color=color, linestyle=\":\", alpha=0.5, linewidth=1)\n",
    "        ax.annotate(\n",
    "            f\"N={int(round(optimal_post_top_n))}\",\n",
    "            xy=(optimal_post_top_n, post_top_n_mean_f1[optimal_index]),\n",
    "            xytext=(5, 6 + strategy_index * 14),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Top-N (number of results returned)\")\n",
    "ax.set_ylabel(\"Macro F1\")\n",
    "num_runs = len(analysis_summaries)\n",
    "ax.set_title(\n",
    "    f\"Top-N Sweep: Pre-Rerank vs All Strategies — {PHASE}\\n\"\n",
    "    f\"(averaged across {num_runs} run(s), dotted vertical = optimal N per stage)\"\n",
    ")\n",
    "ax.legend(fontsize=9, loc=\"best\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Per-Test-Case Heatmap\n",
    "# Rows: test cases | Columns: pre-rerank + each strategy (at optimal threshold)\n",
    "# Color: red (low F1) → green (high F1). Gray = no data for that test case.\n",
    "\n",
    "# Collect all test case IDs across all summaries\n",
    "all_test_case_ids = set()\n",
    "for summary in analysis_summaries:\n",
    "    all_test_case_ids.update(summary.get(\"per_test_case\", {}).keys())\n",
    "all_test_case_ids = sorted(all_test_case_ids)\n",
    "\n",
    "# Stage columns for heatmap: pre-rerank baseline + each strategy at optimal threshold\n",
    "heatmap_stage_keys = [\"pre_rerank_distance\"]\n",
    "for strategy_name in all_strategies:\n",
    "    heatmap_stage_keys.append(f\"post_rerank_threshold:{strategy_name}\")\n",
    "\n",
    "heatmap_column_labels = [\"Pre-Rerank\\n(Dist. Thresh.)\"]\n",
    "for strategy_name in all_strategies:\n",
    "    heatmap_column_labels.append(f\"{strategy_name}\\n(Rerank Thresh.)\")\n",
    "\n",
    "# Compute averaged per-test-case F1 for each stage\n",
    "per_stage_per_tc_f1 = {}  # stage_key -> {test_case_id: averaged_f1}\n",
    "for stage_key in heatmap_stage_keys:\n",
    "    per_stage_per_tc_f1[stage_key] = average_per_test_case_stage_f1_across_runs(\n",
    "        analysis_summaries, stage_key\n",
    "    )\n",
    "\n",
    "# Build heatmap matrix\n",
    "heatmap_values = np.full((len(all_test_case_ids), len(heatmap_stage_keys)), np.nan)\n",
    "for column_index, stage_key in enumerate(heatmap_stage_keys):\n",
    "    averaged_f1_map = per_stage_per_tc_f1[stage_key]\n",
    "    for row_index, test_case_id in enumerate(all_test_case_ids):\n",
    "        if test_case_id in averaged_f1_map:\n",
    "            heatmap_values[row_index, column_index] = averaged_f1_map[test_case_id]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(\n",
    "        max(9, len(heatmap_stage_keys) * 3),\n",
    "        max(6, len(all_test_case_ids) * 0.65),\n",
    "    )\n",
    ")\n",
    "sns.heatmap(\n",
    "    heatmap_values,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=heatmap_column_labels,\n",
    "    yticklabels=all_test_case_ids,\n",
    "    cmap=\"RdYlGn\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "num_runs = len(analysis_summaries)\n",
    "ax.set_title(\n",
    "    f\"Per-Test-Case F1: Pre-Rerank vs Strategies — {PHASE}\\n(averaged across {num_runs} run(s))\"\n",
    ")\n",
    "ax.set_ylabel(\"Test Case\")\n",
    "ax.set_xlabel(\"Stage\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print macro column averages\n",
    "print(\"Column averages (macro F1):\")\n",
    "for column_index, (stage_key, column_label) in enumerate(\n",
    "    zip(heatmap_stage_keys, heatmap_column_labels)\n",
    "):\n",
    "    column_avg = np.nanmean(heatmap_values[:, column_index])\n",
    "    label_oneliner = column_label.replace(\"\\n\", \" \")\n",
    "    print(f\"  {label_oneliner:<45} F1 = {column_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Improvement Analysis: Per-Test-Case Delta vs Baseline\n",
    "# Horizontal bar chart showing the F1 delta (post-rerank minus pre-rerank baseline).\n",
    "# Green = improved, red = degraded, gray = equivalent (within ±0.01).\n",
    "\n",
    "baseline_per_tc_f1 = per_stage_per_tc_f1.get(\"pre_rerank_distance\", {})\n",
    "\n",
    "bar_height_per_strategy = 0.75 / max(len(all_strategies), 1)\n",
    "test_case_y_positions = np.arange(len(all_test_case_ids))\n",
    "strategy_palette = plt.cm.Set2(np.linspace(0, 0.8, max(len(all_strategies), 3)))\n",
    "\n",
    "fig_height = max(6, len(all_test_case_ids) * 0.65 * max(len(all_strategies), 1))\n",
    "fig, ax = plt.subplots(figsize=(13, fig_height))\n",
    "\n",
    "for strategy_index, strategy_name in enumerate(all_strategies):\n",
    "    post_rerank_stage_key = f\"post_rerank_threshold:{strategy_name}\"\n",
    "    post_rerank_per_tc_f1 = per_stage_per_tc_f1.get(post_rerank_stage_key, {})\n",
    "\n",
    "    delta_f1_values = []\n",
    "    bar_colors = []\n",
    "    for test_case_id in all_test_case_ids:\n",
    "        baseline_f1 = baseline_per_tc_f1.get(test_case_id, 0.0)\n",
    "        post_rerank_f1 = post_rerank_per_tc_f1.get(test_case_id, 0.0)\n",
    "        delta = post_rerank_f1 - baseline_f1\n",
    "        delta_f1_values.append(delta)\n",
    "        if delta > 0.01:\n",
    "            bar_colors.append(\"#2ecc71\")\n",
    "        elif delta < -0.01:\n",
    "            bar_colors.append(\"#e74c3c\")\n",
    "        else:\n",
    "            bar_colors.append(\"#95a5a6\")\n",
    "\n",
    "    y_offset = (strategy_index - len(all_strategies) / 2 + 0.5) * bar_height_per_strategy\n",
    "    ax.barh(\n",
    "        test_case_y_positions + y_offset,\n",
    "        delta_f1_values,\n",
    "        height=bar_height_per_strategy * 0.85,\n",
    "        color=bar_colors,\n",
    "        alpha=0.8,\n",
    "        label=strategy_name,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "ax.axvline(0, color=\"black\", linewidth=1, zorder=5)\n",
    "ax.set_yticks(test_case_y_positions)\n",
    "ax.set_yticklabels(all_test_case_ids, fontsize=9)\n",
    "ax.set_xlabel(\"ΔF1  (post-rerank minus pre-rerank baseline)\")\n",
    "num_runs = len(analysis_summaries)\n",
    "ax.set_title(\n",
    "    f\"F1 Improvement Over Pre-Rerank Baseline — {PHASE}\\n\"\n",
    "    f\"Green = improved, Red = degraded (n={num_runs} runs averaged)\"\n",
    ")\n",
    "if len(all_strategies) > 1:\n",
    "    ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed delta table\n",
    "strategy_header = \"\".join(f\"  {name:<28}\" for name in all_strategies)\n",
    "print(f\"{'Test Case':<32}{strategy_header}\")\n",
    "print(\"-\" * (32 + 30 * len(all_strategies)))\n",
    "\n",
    "for test_case_id in all_test_case_ids:\n",
    "    baseline_f1 = baseline_per_tc_f1.get(test_case_id, 0.0)\n",
    "    row_parts = [f\"{test_case_id:<32}\"]\n",
    "    for strategy_name in all_strategies:\n",
    "        post_rerank_stage_key = f\"post_rerank_threshold:{strategy_name}\"\n",
    "        post_rerank_f1 = per_stage_per_tc_f1.get(post_rerank_stage_key, {}).get(test_case_id, 0.0)\n",
    "        delta = post_rerank_f1 - baseline_f1\n",
    "        row_parts.append(f\"  {baseline_f1:.3f} -> {post_rerank_f1:.3f} ({delta:+.3f})\")\n",
    "    print(\"\".join(row_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Summary Dashboard\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"SUMMARY DASHBOARD\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Config info\n",
    "config_fingerprint = analysis_summaries[0].get(\"config_fingerprint\", {})\n",
    "if config_fingerprint:\n",
    "    fingerprint_hash = config_fingerprint.get(\"fingerprint_hash\", \"?\")\n",
    "    print(f\"\\nConfig fingerprint: {fingerprint_hash}\")\n",
    "    print(f\"  Extraction prompt:  v{config_fingerprint.get('extraction_prompt_version', '?')}\")\n",
    "    print(f\"  Query prompt:       v{config_fingerprint.get('query_prompt_version', '?')}\")\n",
    "    print(f\"  Query model:        {config_fingerprint.get('query_model', '?')}\")\n",
    "    print(f\"  Reranker model:     {config_fingerprint.get('reranker_model', '?')}\")\n",
    "    print(f\"  Runs averaged:      {len(analysis_summaries)}\")\n",
    "\n",
    "# Pre-rerank baseline\n",
    "baseline_metrics = stage_averaged_metrics.get(\"pre_rerank_distance\", {})\n",
    "baseline_f1 = baseline_metrics.get(\"f1\", 0.0)\n",
    "baseline_precision = baseline_metrics.get(\"precision\", 0.0)\n",
    "baseline_recall = baseline_metrics.get(\"recall\", 0.0)\n",
    "baseline_mrr = baseline_metrics.get(\"mrr\", 0.0)\n",
    "\n",
    "print(f\"\\nBaseline — Pre-Rerank (at optimal distance threshold):\")\n",
    "print(f\"  F1:        {baseline_f1:.4f}\")\n",
    "print(f\"  Precision: {baseline_precision:.4f}\")\n",
    "print(f\"  Recall:    {baseline_recall:.4f}\")\n",
    "print(f\"  MRR:       {baseline_mrr:.4f}\")\n",
    "\n",
    "# Per-strategy results\n",
    "print(f\"\\nPost-Rerank Strategies:\")\n",
    "best_strategy_name = None\n",
    "best_strategy_f1 = -1.0\n",
    "\n",
    "for strategy_name in all_strategies:\n",
    "    threshold_stage_key = f\"post_rerank_threshold:{strategy_name}\"\n",
    "    top_n_stage_key = f\"post_rerank_top_n:{strategy_name}\"\n",
    "\n",
    "    threshold_metrics = stage_averaged_metrics.get(threshold_stage_key, {})\n",
    "    top_n_metrics = stage_averaged_metrics.get(top_n_stage_key, {})\n",
    "\n",
    "    threshold_f1 = threshold_metrics.get(\"f1\", 0.0)\n",
    "    top_n_f1 = top_n_metrics.get(\"f1\", 0.0)\n",
    "    best_f1_for_strategy = max(threshold_f1, top_n_f1)\n",
    "\n",
    "    delta_threshold = threshold_f1 - baseline_f1\n",
    "    delta_top_n = top_n_f1 - baseline_f1\n",
    "\n",
    "    print(f\"\\n  [{strategy_name}]\")\n",
    "    print(f\"    At optimal threshold: F1 = {threshold_f1:.4f}  (delta {delta_threshold:+.4f})\")\n",
    "    print(f\"    At optimal top-N:     F1 = {top_n_f1:.4f}  (delta {delta_top_n:+.4f})\")\n",
    "\n",
    "    if best_f1_for_strategy > best_strategy_f1:\n",
    "        best_strategy_f1 = best_f1_for_strategy\n",
    "        best_strategy_name = strategy_name\n",
    "\n",
    "# Winner\n",
    "print(f\"\\n{'=' * 65}\")\n",
    "if best_strategy_name and best_strategy_f1 > baseline_f1:\n",
    "    overall_improvement = best_strategy_f1 - baseline_f1\n",
    "    relative_improvement = overall_improvement / baseline_f1 * 100 if baseline_f1 > 0 else 0\n",
    "    print(f\"Best strategy: {best_strategy_name}\")\n",
    "    print(f\"  Best F1:        {best_strategy_f1:.4f}\")\n",
    "    print(f\"  Improvement:    {overall_improvement:+.4f}  ({relative_improvement:+.1f}% relative)\")\n",
    "else:\n",
    "    print(\"No rerank strategy outperformed the pre-rerank baseline.\")\n",
    "\n",
    "# Hardest test cases (lowest pre-rerank F1)\n",
    "print(f\"\\nHardest test cases (lowest pre-rerank baseline F1):\")\n",
    "sorted_by_baseline_f1 = sorted(\n",
    "    [\n",
    "        (test_case_id, baseline_per_tc_f1.get(test_case_id, 0.0))\n",
    "        for test_case_id in all_test_case_ids\n",
    "    ],\n",
    "    key=lambda item: item[1],\n",
    ")\n",
    "for rank, (test_case_id, f1_value) in enumerate(sorted_by_baseline_f1[:3]):\n",
    "    print(f\"  {rank + 1}. {test_case_id}: pre-rerank F1 = {f1_value:.3f}\")\n",
    "\n",
    "# Biggest regressions\n",
    "if all_strategies:\n",
    "    print(f\"\\nTest cases with largest degradation after reranking:\")\n",
    "    all_deltas = []\n",
    "    for test_case_id in all_test_case_ids:\n",
    "        baseline_f1_value = baseline_per_tc_f1.get(test_case_id, 0.0)\n",
    "        for strategy_name in all_strategies:\n",
    "            post_rerank_stage_key = f\"post_rerank_threshold:{strategy_name}\"\n",
    "            post_rerank_f1 = per_stage_per_tc_f1.get(post_rerank_stage_key, {}).get(\n",
    "                test_case_id, 0.0\n",
    "            )\n",
    "            delta = post_rerank_f1 - baseline_f1_value\n",
    "            all_deltas.append((test_case_id, strategy_name, delta))\n",
    "    worst_regressions = sorted(all_deltas, key=lambda item: item[2])[:3]\n",
    "    for test_case_id, strategy_name, delta in worst_regressions:\n",
    "        if delta < -0.001:\n",
    "            print(f\"  {test_case_id} [{strategy_name}]: delta = {delta:+.3f}\")\n",
    "\n",
    "print(f\"\\nTotal test cases: {len(all_test_case_ids)}\")\n",
    "print(f\"Runs analyzed: {len(analysis_summaries)} (phase: {PHASE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
