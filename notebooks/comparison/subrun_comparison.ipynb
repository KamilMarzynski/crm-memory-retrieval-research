{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Subrun Comparison\n",
    "\n",
    "Compare parent runs to their subruns — the **deterministic swap** use case:\n",
    "- Parent runs use Config A (e.g., original reranker or embedder)\n",
    "- Subruns reuse the same memories, test cases, and queries, but run experiments with Config B\n",
    "- Because everything except the experiment config is identical, any F1 difference is purely due to the config change\n",
    "\n",
    "**Typical workflow:**\n",
    "1. Run a full pipeline batch → parent runs\n",
    "2. Run a subrun batch pointing at those parent run IDs → subruns\n",
    "3. Open this notebook, paste the parent run IDs, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from memory_retrieval.experiments.comparison import (\n",
    "    compare_configs,\n",
    "    fingerprint_diff,\n",
    "    group_runs_by_fingerprint,\n",
    "    load_run_summaries,\n",
    "    load_subrun_summaries,\n",
    ")\n",
    "from memory_retrieval.infra.figures import create_figure_session, save_figure\n",
    "from memory_retrieval.infra.runs import PHASE1, PHASE2\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "def _extract_f1_from_summary(\n",
    "    macro: dict, metric_path: str, strategy: str | None = None\n",
    ") -> float | None:\n",
    "    \"\"\"Extract the primary optimal F1 from a macro_averaged dict.\"\"\"\n",
    "    if metric_path == \"post_rerank\" and strategy:\n",
    "        return (\n",
    "            macro.get(\"post_rerank\", {}).get(strategy, {}).get(\"at_optimal_threshold\", {}).get(\"f1\")\n",
    "        )\n",
    "    elif metric_path == \"pre_rerank\":\n",
    "        return macro.get(\"pre_rerank\", {}).get(\"at_optimal_distance_threshold\", {}).get(\"f1\")\n",
    "    else:\n",
    "        return macro.get(\"metrics\", {}).get(\"f1\")\n",
    "\n",
    "\n",
    "def _extract_f1_from_per_test_case(\n",
    "    per_tc: dict, metric_path: str, strategy: str | None = None\n",
    ") -> float | None:\n",
    "    \"\"\"Extract F1 from a per_test_case entry.\"\"\"\n",
    "    if metric_path == \"post_rerank\" and strategy:\n",
    "        return (\n",
    "            per_tc.get(\"post_rerank\", {})\n",
    "            .get(strategy, {})\n",
    "            .get(\"rerank_threshold\", {})\n",
    "            .get(\"at_optimal\", {})\n",
    "            .get(\"f1\")\n",
    "        )\n",
    "    elif metric_path == \"pre_rerank\":\n",
    "        return (\n",
    "            per_tc.get(\"pre_rerank\", {})\n",
    "            .get(\"distance_threshold\", {})\n",
    "            .get(\"at_optimal\", {})\n",
    "            .get(\"f1\")\n",
    "        )\n",
    "    else:\n",
    "        return per_tc.get(\"metrics\", {}).get(\"f1\")\n",
    "\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Configuration\n",
    "\n",
    "# Which phase to analyze\n",
    "PHASE = PHASE2\n",
    "\n",
    "# Parent run IDs to compare.\n",
    "# Copy these from run_batch.py output or data/<phase>/batches/<batch_id>/batch_manifest.json.\n",
    "# Set to None to load all parent runs that have subruns.\n",
    "PARENT_RUN_IDS = None  # e.g., [\"run_20260218_143022\", \"run_20260218_145301\"]\n",
    "\n",
    "# Rerank strategy — must match what was used in the subrun's ExperimentConfig.\n",
    "STRATEGY = \"default\"\n",
    "\n",
    "# Metric path: \"post_rerank\" for reranking experiments, \"metrics\" for standard vector-only.\n",
    "METRIC_PATH = \"post_rerank\"\n",
    "\n",
    "print(f\"Phase: {PHASE}\")\n",
    "print(f\"Strategy: {STRATEGY}\")\n",
    "print(f\"Metric path: {METRIC_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load Parent & Subrun Summaries\n",
    "\n",
    "parent_summaries = load_run_summaries(PHASE, run_ids=PARENT_RUN_IDS)\n",
    "subrun_summaries = load_subrun_summaries(PHASE, parent_run_ids=PARENT_RUN_IDS)\n",
    "\n",
    "loaded_parent_run_ids = sorted(\n",
    "    [summary.get(\"run_id\", \"unknown\") for summary in parent_summaries if summary.get(\"run_id\")]\n",
    ")\n",
    "if PARENT_RUN_IDS is None:\n",
    "    parents_part = f\"parents-all-n{len(loaded_parent_run_ids)}\"\n",
    "elif loaded_parent_run_ids:\n",
    "    parents_part = f\"parents-{loaded_parent_run_ids[0]}-{loaded_parent_run_ids[-1]}-n{len(loaded_parent_run_ids)}\"\n",
    "else:\n",
    "    parents_part = \"parents-none-none-n0\"\n",
    "\n",
    "comparison_key = (\n",
    "    f\"{parents_part}__subruns-n{len(subrun_summaries)}__strategy-{STRATEGY}__metric-{METRIC_PATH}\"\n",
    ")\n",
    "FIGURE_SESSION = create_figure_session(\n",
    "    root_dir=PROJECT_ROOT / \"data\" / \"comparisons\" / \"exports\",\n",
    "    notebook_slug=f\"subrun_comparison/{PHASE}\",\n",
    "    context_key=comparison_key,\n",
    "    context={\n",
    "        \"phase\": PHASE,\n",
    "        \"strategy\": STRATEGY,\n",
    "        \"metric_path\": METRIC_PATH,\n",
    "        \"parent_run_ids\": PARENT_RUN_IDS,\n",
    "        \"loaded_parent_run_ids\": loaded_parent_run_ids,\n",
    "        \"subrun_ids\": [summary.get(\"run_id\", \"unknown\") for summary in subrun_summaries],\n",
    "    },\n",
    ")\n",
    "print(f\"Figure export session: {FIGURE_SESSION.session_dir}\")\n",
    "\n",
    "if not subrun_summaries:\n",
    "    print(\"No subruns found. Run a subrun batch first:\")\n",
    "    print()\n",
    "    print(\n",
    "        \"  from memory_retrieval.experiments.batch_runner import BatchSubrunConfig, run_subrun_batch\"\n",
    "    )\n",
    "    print(\"  outcome = run_subrun_batch(BatchSubrunConfig(\")\n",
    "    print(\"      phase=PHASE2,\")\n",
    "    print(\"      parent_run_ids=[...],\")\n",
    "    print(\n",
    "        \"      experiment_config=ExperimentConfig(search_backend=VectorBackend(), reranker=Reranker(...)),\"\n",
    "    )\n",
    "    print(\"  ))\")\n",
    "else:\n",
    "    print(f\"Loaded {len(parent_summaries)} parent runs\")\n",
    "    print(f\"Loaded {len(subrun_summaries)} subruns\")\n",
    "    print()\n",
    "\n",
    "    # Group subruns by parent for the overview\n",
    "    subruns_by_parent: dict[str, list[dict]] = {}\n",
    "    for summary in subrun_summaries:\n",
    "        parent_run_id = summary.get(\"parent_run_id\", \"unknown\")\n",
    "        subruns_by_parent.setdefault(parent_run_id, []).append(summary)\n",
    "\n",
    "    print(f\"{'Parent Run ID':<30} {'Parent F1':>10} {'Subruns':>8}  Subrun IDs & F1\")\n",
    "    print(\"-\" * 90)\n",
    "    for parent_summary in parent_summaries:\n",
    "        parent_run_id = parent_summary.get(\"run_id\", \"?\")\n",
    "        parent_f1 = _extract_f1_from_summary(\n",
    "            parent_summary.get(\"macro_averaged\", {}), METRIC_PATH, STRATEGY\n",
    "        )\n",
    "        f1_display = f\"{parent_f1:.4f}\" if parent_f1 is not None else \"n/a\"\n",
    "        subruns = subruns_by_parent.get(parent_run_id, [])\n",
    "        print(f\"{parent_run_id:<30} {f1_display:>10} {len(subruns):>8}\", end=\"\")\n",
    "\n",
    "        for subrun_summary in subruns:\n",
    "            subrun_f1 = _extract_f1_from_summary(\n",
    "                subrun_summary.get(\"macro_averaged\", {}), METRIC_PATH, STRATEGY\n",
    "            )\n",
    "            subrun_f1_display = f\"{subrun_f1:.4f}\" if subrun_f1 is not None else \"n/a\"\n",
    "            print(f\"  {subrun_summary['run_id']} ({subrun_f1_display})\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Config Diff (What Changed)\n",
    "\n",
    "if parent_summaries and subrun_summaries:\n",
    "    parent_fingerprint = parent_summaries[0].get(\"config_fingerprint\", {})\n",
    "    subrun_fingerprint = subrun_summaries[0].get(\"config_fingerprint\", {})\n",
    "\n",
    "    if not parent_fingerprint:\n",
    "        print(\"Parent runs have no config fingerprint.\")\n",
    "        print(\"Run update_config_fingerprint() on parent runs, or re-run them via batch_runner.\")\n",
    "    elif not subrun_fingerprint:\n",
    "        print(\"Subruns have no config fingerprint.\")\n",
    "        print(\"Subruns created via run_subrun_batch() always store a fingerprint.\")\n",
    "    else:\n",
    "        diff = fingerprint_diff(parent_fingerprint, subrun_fingerprint)\n",
    "\n",
    "        if diff:\n",
    "            print(\"Config changes (parent → subrun):\")\n",
    "            print()\n",
    "            max_field_len = max(len(field) for field in diff)\n",
    "            for field, values in diff.items():\n",
    "                print(f\"  {field:<{max_field_len}}  {str(values['a']):<40} → {values['b']}\")\n",
    "        else:\n",
    "            print(\"No config differences detected.\")\n",
    "            print(\n",
    "                \"Both parent and subrun have the same fingerprint hash:\",\n",
    "                parent_fingerprint.get(\"fingerprint_hash\"),\n",
    "            )\n",
    "            print(\n",
    "                \"This means the subrun ran with identical config — results should be very similar.\"\n",
    "            )\n",
    "\n",
    "        print()\n",
    "        print(f\"Parent fingerprint hash: {parent_fingerprint.get('fingerprint_hash', 'n/a')}\")\n",
    "        print(f\"Subrun fingerprint hash: {subrun_fingerprint.get('fingerprint_hash', 'n/a')}\")\n",
    "else:\n",
    "    print(\"Cannot compute diff: missing parent or subrun summaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Statistical Comparison (Parent vs Subrun)\n",
    "#\n",
    "# Treats parent summaries as Config A and subrun summaries as Config B.\n",
    "# Averages each test case's F1 across all parent runs (A) and across all subruns (B),\n",
    "# then runs a paired Wilcoxon signed-rank test on the per-test-case deltas.\n",
    "\n",
    "if parent_summaries and subrun_summaries:\n",
    "    comparison = compare_configs(\n",
    "        parent_summaries,\n",
    "        subrun_summaries,\n",
    "        strategy=STRATEGY,\n",
    "        metric_path=METRIC_PATH,\n",
    "    )\n",
    "\n",
    "    if \"error\" in comparison:\n",
    "        print(f\"Comparison error: {comparison['error']}\")\n",
    "    else:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"PARENT vs SUBRUN — STATISTICAL COMPARISON\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"  Common test cases:       {comparison['num_common_test_cases']}\")\n",
    "        print(f\"  Parent runs (n):         {comparison['num_runs_a']}\")\n",
    "        print(f\"  Subruns (n):             {comparison['num_runs_b']}\")\n",
    "        print()\n",
    "        print(f\"  Mean F1 — parent:        {comparison['mean_f1_a']:.4f}\")\n",
    "        print(f\"  Mean F1 — subrun:        {comparison['mean_f1_b']:.4f}\")\n",
    "        print(f\"  Mean delta (sub - par):  {comparison['mean_diff']:+.4f}\")\n",
    "        print(f\"  Direction:               {comparison['direction']}\")\n",
    "        print()\n",
    "        ci = comparison[\"bootstrap_ci\"]\n",
    "        print(f\"  Bootstrap 95% CI:        [{ci['lower']:.4f}, {ci['upper']:.4f}]\")\n",
    "        print(f\"  Excludes zero:           {ci['excludes_zero']}\")\n",
    "        print(f\"  Wilcoxon p-value:        {comparison['wilcoxon']['p_value']:.4f}\")\n",
    "        print(f\"  Significance:            {comparison['significance']}\")\n",
    "        print()\n",
    "\n",
    "        # Plain-language interpretation\n",
    "        direction = comparison[\"direction\"]\n",
    "        significance = comparison[\"significance\"]\n",
    "        winner = \"subrun\" if direction == \"b_better\" else \"parent\"\n",
    "\n",
    "        if significance == \"significant\":\n",
    "            print(f\"  ✓ {winner.upper()} is significantly better (|delta| ≥ 0.03, p < 0.05)\")\n",
    "        elif significance == \"marginal\":\n",
    "            print(f\"  ~ {winner.upper()} shows marginal improvement (|delta| ≥ 0.01, p < 0.10)\")\n",
    "        else:\n",
    "            print(\"  ✗ No significant difference — subrun config change has no measurable effect\")\n",
    "else:\n",
    "    print(\"Cannot compare: missing parent or subrun summaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Per-Test-Case Paired Plot\n",
    "#\n",
    "# Left: paired dot plot (parent blue, subrun orange) — shows absolute F1 per test case\n",
    "# Right: delta bar chart — shows improvement or degradation per test case\n",
    "\n",
    "if (\n",
    "    parent_summaries\n",
    "    and subrun_summaries\n",
    "    and \"comparison\" in dir()\n",
    "    and \"paired_differences\" in comparison\n",
    "):\n",
    "    paired = comparison[\"paired_differences\"]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, max(5, len(paired) * 0.55 + 2)))\n",
    "\n",
    "    test_case_ids = [entry[\"test_case_id\"] for entry in paired]\n",
    "    y_positions = list(range(len(test_case_ids)))\n",
    "\n",
    "    # Left: paired dot plot\n",
    "    ax = axes[0]\n",
    "    for y_pos, entry in enumerate(paired):\n",
    "        diff_color = (\n",
    "            \"#2ecc71\"\n",
    "            if entry[\"diff\"] > 0.001\n",
    "            else \"#e74c3c\"\n",
    "            if entry[\"diff\"] < -0.001\n",
    "            else \"#95a5a6\"\n",
    "        )\n",
    "        ax.plot(\n",
    "            [entry[\"f1_a\"], entry[\"f1_b\"]],\n",
    "            [y_pos, y_pos],\n",
    "            color=diff_color,\n",
    "            linewidth=2.5,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        ax.scatter(\n",
    "            entry[\"f1_a\"], y_pos, color=\"#3498db\", s=70, zorder=5, edgecolors=\"white\", linewidth=0.5\n",
    "        )\n",
    "        ax.scatter(\n",
    "            entry[\"f1_b\"], y_pos, color=\"#e67e22\", s=70, zorder=5, edgecolors=\"white\", linewidth=0.5\n",
    "        )\n",
    "\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(test_case_ids, fontsize=8)\n",
    "    ax.set_xlabel(\"F1 Score\")\n",
    "    ax.set_title(\"Per-Test-Case F1: Parent (blue) vs Subrun (orange)\")\n",
    "    ax.scatter([], [], color=\"#3498db\", s=70, label=\"Parent\")\n",
    "    ax.scatter([], [], color=\"#e67e22\", s=70, label=\"Subrun\")\n",
    "    ax.plot([], [], color=\"#2ecc71\", linewidth=2.5, label=\"Improved\")\n",
    "    ax.plot([], [], color=\"#e74c3c\", linewidth=2.5, label=\"Degraded\")\n",
    "    ax.legend(fontsize=8, loc=\"lower right\")\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "    # Right: delta bar chart\n",
    "    ax = axes[1]\n",
    "    diffs = [entry[\"diff\"] for entry in paired]\n",
    "    bar_colors = [\"#2ecc71\" if diff > 0 else \"#e74c3c\" for diff in diffs]\n",
    "    ax.barh(y_positions, diffs, color=bar_colors, alpha=0.75, edgecolor=\"white\", linewidth=0.3)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.8)\n",
    "    ax.axvline(\n",
    "        comparison[\"mean_diff\"],\n",
    "        color=\"#2c3e50\",\n",
    "        linewidth=1.5,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean delta: {comparison['mean_diff']:+.3f}\",\n",
    "    )\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(test_case_ids, fontsize=8)\n",
    "    ax.set_xlabel(\"F1 Delta (Subrun − Parent)\")\n",
    "    ax.set_title(\"Per-Test-Case Delta\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Subrun Comparison — {PHASE} ({STRATEGY})  |  {comparison['significance'].replace('_', ' ')}\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"per_test_case_paired_plot\",\n",
    "        title=f\"Subrun Comparison — {PHASE} ({STRATEGY})\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")\n",
    "else:\n",
    "    print(\"Run Cell 4 first to generate the comparison result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Threshold Sweep Overlay (Parent vs Subrun)\n",
    "#\n",
    "# Shows how F1 varies with rerank score threshold for parent runs vs subruns.\n",
    "# Shaded band = min/max across multiple runs in each group.\n",
    "\n",
    "if parent_summaries and subrun_summaries:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    groups_to_plot = [\n",
    "        (parent_summaries, \"#3498db\", \"Parent\"),\n",
    "        (subrun_summaries, \"#e67e22\", \"Subrun\"),\n",
    "    ]\n",
    "\n",
    "    for group_summaries, color, group_label in groups_to_plot:\n",
    "        # Collect threshold sweeps from all summaries in this group\n",
    "        all_sweep_dicts: list[dict[float, float]] = []\n",
    "        for summary in group_summaries:\n",
    "            sweep_data = (\n",
    "                summary.get(\"rerank_strategies\", {}).get(STRATEGY, {}).get(\"threshold_sweep\", {})\n",
    "            )\n",
    "            full_sweep = sweep_data.get(\"full_sweep\", [])\n",
    "            if full_sweep:\n",
    "                all_sweep_dicts.append({entry[\"threshold\"]: entry[\"f1\"] for entry in full_sweep})\n",
    "\n",
    "        if not all_sweep_dicts:\n",
    "            print(f\"No threshold sweep data found for {group_label}. Is STRATEGY correct?\")\n",
    "            continue\n",
    "\n",
    "        all_thresholds = sorted(set().union(*[set(sweep.keys()) for sweep in all_sweep_dicts]))\n",
    "\n",
    "        mean_f1_values = []\n",
    "        min_f1_values = []\n",
    "        max_f1_values = []\n",
    "        for threshold in all_thresholds:\n",
    "            f1_at_threshold = [sweep[threshold] for sweep in all_sweep_dicts if threshold in sweep]\n",
    "            if f1_at_threshold:\n",
    "                mean_f1_values.append(np.mean(f1_at_threshold))\n",
    "                min_f1_values.append(min(f1_at_threshold))\n",
    "                max_f1_values.append(max(f1_at_threshold))\n",
    "\n",
    "        ax.plot(\n",
    "            all_thresholds,\n",
    "            mean_f1_values,\n",
    "            label=f\"{group_label} (n={len(all_sweep_dicts)})\",\n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        if len(all_sweep_dicts) > 1:\n",
    "            ax.fill_between(all_thresholds, min_f1_values, max_f1_values, color=color, alpha=0.15)\n",
    "\n",
    "        optimal_index = int(np.argmax(mean_f1_values))\n",
    "        if optimal_index < len(all_thresholds):\n",
    "            ax.axvline(\n",
    "                all_thresholds[optimal_index],\n",
    "                color=color,\n",
    "                linestyle=\":\",\n",
    "                alpha=0.6,\n",
    "                linewidth=1.2,\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Rerank Score Threshold\")\n",
    "    ax.set_ylabel(\"Macro F1\")\n",
    "    ax.set_title(f\"Threshold Sweep — {PHASE} ({STRATEGY})\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"threshold_sweep_overlay\",\n",
    "        title=f\"Threshold Sweep — {PHASE} ({STRATEGY})\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")\n",
    "else:\n",
    "    print(\"Cannot plot: missing parent or subrun summaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Per-Test-Case Heatmap (Parent vs Subrun Side-by-Side)\n",
    "\n",
    "if parent_summaries and subrun_summaries:\n",
    "    all_test_case_ids = set()\n",
    "    for summary in parent_summaries + subrun_summaries:\n",
    "        all_test_case_ids.update(summary.get(\"per_test_case\", {}).keys())\n",
    "    all_test_case_ids = sorted(all_test_case_ids)\n",
    "\n",
    "    columns = [\n",
    "        (parent_summaries, \"Parent\\n(avg)\"),\n",
    "        (subrun_summaries, \"Subrun\\n(avg)\"),\n",
    "    ]\n",
    "\n",
    "    heatmap_data = np.full((len(all_test_case_ids), 3), np.nan)  # [parent, subrun, delta]\n",
    "\n",
    "    for col_index, (group_summaries, _) in enumerate(columns):\n",
    "        for row_index, test_case_id in enumerate(all_test_case_ids):\n",
    "            f1_values = []\n",
    "            for summary in group_summaries:\n",
    "                per_tc = summary.get(\"per_test_case\", {}).get(test_case_id, {})\n",
    "                f1 = _extract_f1_from_per_test_case(per_tc, METRIC_PATH, STRATEGY)\n",
    "                if f1 is not None:\n",
    "                    f1_values.append(f1)\n",
    "            if f1_values:\n",
    "                heatmap_data[row_index, col_index] = np.mean(f1_values)\n",
    "\n",
    "    # Delta column\n",
    "    for row_index in range(len(all_test_case_ids)):\n",
    "        parent_f1 = heatmap_data[row_index, 0]\n",
    "        subrun_f1 = heatmap_data[row_index, 1]\n",
    "        if not np.isnan(parent_f1) and not np.isnan(subrun_f1):\n",
    "            heatmap_data[row_index, 2] = subrun_f1 - parent_f1\n",
    "\n",
    "    column_labels = [\"Parent\\n(avg F1)\", \"Subrun\\n(avg F1)\", \"Delta\\n(sub−par)\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(8, 5), max(6, len(all_test_case_ids) * 0.55)))\n",
    "    sns.heatmap(\n",
    "        heatmap_data[:, :2],\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        xticklabels=[\"Parent (avg F1)\", \"Subrun (avg F1)\"],\n",
    "        yticklabels=all_test_case_ids,\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        linewidths=0.5,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(f\"Per-Test-Case F1 — {PHASE} ({STRATEGY})\")\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"per_test_case_f1_heatmap\",\n",
    "        title=f\"Per-Test-Case F1 — {PHASE} ({STRATEGY})\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")\n",
    "\n",
    "    # Separate delta heatmap (diverging colormap centered at 0)\n",
    "    fig, ax = plt.subplots(figsize=(3, max(6, len(all_test_case_ids) * 0.55)))\n",
    "    delta_max = max(np.nanmax(np.abs(heatmap_data[:, 2])), 0.01)\n",
    "    sns.heatmap(\n",
    "        heatmap_data[:, 2:3],\n",
    "        annot=True,\n",
    "        fmt=\"+.2f\",\n",
    "        xticklabels=[\"Delta (sub−par)\"],\n",
    "        yticklabels=all_test_case_ids,\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=-delta_max,\n",
    "        vmax=delta_max,\n",
    "        center=0,\n",
    "        linewidths=0.5,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"F1 Delta (Subrun − Parent)\")\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"per_test_case_delta_heatmap\",\n",
    "        title=\"F1 Delta (Subrun - Parent)\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")\n",
    "\n",
    "    # Summary: which test cases improved most / degraded most\n",
    "    delta_column = heatmap_data[:, 2]\n",
    "    valid_indices = [i for i in range(len(all_test_case_ids)) if not np.isnan(delta_column[i])]\n",
    "    sorted_by_delta = sorted(valid_indices, key=lambda i: delta_column[i], reverse=True)\n",
    "\n",
    "    print(\"\\nTest case breakdown:\")\n",
    "    print(f\"  {'Test Case':<35} {'Parent':>8} {'Subrun':>8} {'Delta':>8}\")\n",
    "    print(f\"  {'-' * 35} {'-' * 8} {'-' * 8} {'-' * 8}\")\n",
    "    for i in sorted_by_delta:\n",
    "        test_case_id = all_test_case_ids[i]\n",
    "        parent_f1 = heatmap_data[i, 0]\n",
    "        subrun_f1 = heatmap_data[i, 1]\n",
    "        delta = delta_column[i]\n",
    "        print(f\"  {test_case_id:<35} {parent_f1:>8.3f} {subrun_f1:>8.3f} {delta:>+8.3f}\")\n",
    "else:\n",
    "    print(\"Cannot plot: missing parent or subrun summaries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
