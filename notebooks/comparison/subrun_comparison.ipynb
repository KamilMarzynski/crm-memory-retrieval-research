{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Subrun Comparison\n",
    "\n",
    "Compare parent runs to their subruns — the **deterministic swap** use case:\n",
    "- Parent runs use Config A (e.g., original reranker or embedder)\n",
    "- Subruns reuse the same memories, test cases, and queries, but run experiments with Config B\n",
    "- Because everything except the experiment config is identical, any F1 difference is purely due to the config change\n",
    "\n",
    "**Typical workflow:**\n",
    "1. Run a full pipeline batch → parent runs\n",
    "2. Run a subrun batch pointing at those parent run IDs → subruns\n",
    "3. Open this notebook, paste the parent run IDs, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 — Setup & Imports\n",
    "import os\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from memory_retrieval.experiments.comparison import (\n",
    "    compare_configs,\n",
    "    fingerprint_diff,\n",
    "    group_runs_by_fingerprint,\n",
    "    load_run_summaries,\n",
    "    load_subrun_summaries,\n",
    ")\n",
    "from memory_retrieval.experiments.metrics_adapter import extract_metric_from_nested\n",
    "from memory_retrieval.infra.figures import create_figure_session, save_figure\n",
    "from memory_retrieval.infra.runs import PHASE1, PHASE2\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "def _extract_f1_from_summary(\n",
    "    macro: dict, metric_path: str, strategy: str | None = None\n",
    ") -> float | None:\n",
    "    \"\"\"Extract the primary optimal F1 from a macro_averaged dict.\"\"\"\n",
    "    return extract_metric_from_nested(macro, metric_path, strategy, metric_key=\"f1\")\n",
    "\n",
    "\n",
    "def _extract_f1_from_per_test_case(\n",
    "    per_tc: dict, metric_path: str, strategy: str | None = None\n",
    ") -> float | None:\n",
    "    \"\"\"Extract F1 from a per_test_case entry.\"\"\"\n",
    "    return extract_metric_from_nested(per_tc, metric_path, strategy, metric_key=\"f1\")\n",
    "\n",
    "\n",
    "def _fingerprint_hash(summary: dict) -> str:\n",
    "    fingerprint = summary.get(\"config_fingerprint\", {})\n",
    "    return fingerprint.get(\"fingerprint_hash\", \"unknown\")\n",
    "\n",
    "\n",
    "def _summary_parent_anchor(summary: dict) -> str | None:\n",
    "    \"\"\"Return the parent-run anchor used to match groups fairly.\"\"\"\n",
    "    if summary.get(\"source_kind\") == \"subrun\":\n",
    "        return summary.get(\"parent_run_id\")\n",
    "    return summary.get(\"run_id\")\n",
    "\n",
    "\n",
    "def _label_for_index(index: int) -> str:\n",
    "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    if index < len(alphabet):\n",
    "        return alphabet[index]\n",
    "    return f\"G{index + 1}\"\n",
    "\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Configuration\n",
    "\n",
    "# Which phase to analyze\n",
    "PHASE = PHASE2\n",
    "\n",
    "# Parent run IDs to compare.\n",
    "# Copy these from run_batch.py output or data/<phase>/batches/<batch_id>/batch_manifest.json.\n",
    "# Set to None to load all parent runs that have subruns.\n",
    "PARENT_RUN_IDS = None  # e.g., [\"run_20260218_143022\", \"run_20260218_145301\"]\n",
    "\n",
    "# Rerank strategy — must match what was used in the subrun's ExperimentConfig.\n",
    "STRATEGY = \"default\"\n",
    "\n",
    "# Metric path: \"post_rerank\" for reranking experiments, \"metrics\" for standard vector-only.\n",
    "METRIC_PATH = \"pre_rerank\"\n",
    "\n",
    "# Optional detailed pair (e.g., \"A_vs_B\"). None = auto-select baseline-vs-first-variant.\n",
    "DETAIL_PAIR = None\n",
    "\n",
    "# Derived label for titles/logs — avoids showing a strategy name when using pre_rerank\n",
    "METRIC_LABEL = STRATEGY if METRIC_PATH == \"post_rerank\" else \"pre-rerank (distance)\"\n",
    "\n",
    "print(f\"Phase: {PHASE}\")\n",
    "print(f\"Strategy: {STRATEGY}\")\n",
    "print(f\"Metric path: {METRIC_PATH}\")\n",
    "print(f\"Metric label: {METRIC_LABEL}\")\n",
    "print(f\"Detail pair: {DETAIL_PAIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load Parent/Subruns, Group by Fingerprint, and Show Coverage\n",
    "\n",
    "parent_summaries = [\n",
    "    {**summary, \"source_kind\": \"parent\"}\n",
    "    for summary in load_run_summaries(PHASE, run_ids=PARENT_RUN_IDS)\n",
    "]\n",
    "subrun_summaries = [\n",
    "    {**summary, \"source_kind\": \"subrun\"}\n",
    "    for summary in load_subrun_summaries(PHASE, parent_run_ids=PARENT_RUN_IDS)\n",
    "]\n",
    "\n",
    "loaded_parent_run_ids = sorted(\n",
    "    [summary.get(\"run_id\", \"unknown\") for summary in parent_summaries if summary.get(\"run_id\")]\n",
    ")\n",
    "all_summaries = parent_summaries + subrun_summaries\n",
    "fingerprint_groups = group_runs_by_fingerprint(all_summaries)\n",
    "\n",
    "parent_hashes = sorted({_fingerprint_hash(summary) for summary in parent_summaries})\n",
    "ordered_hashes = parent_hashes + sorted(\n",
    "    [hash_key for hash_key in fingerprint_groups if hash_key not in set(parent_hashes)]\n",
    ")\n",
    "group_labels = {\n",
    "    hash_key: _label_for_index(index) for index, hash_key in enumerate(ordered_hashes)\n",
    "}\n",
    "\n",
    "parent_ids_by_hash: dict[str, set[str]] = {}\n",
    "for hash_key in ordered_hashes:\n",
    "    parent_ids = {\n",
    "        parent_id\n",
    "        for parent_id in (_summary_parent_anchor(summary) for summary in fingerprint_groups[hash_key])\n",
    "        if parent_id is not None\n",
    "    }\n",
    "    parent_ids_by_hash[hash_key] = parent_ids\n",
    "\n",
    "if PARENT_RUN_IDS is None:\n",
    "    parents_part = f\"parents-all-n{len(loaded_parent_run_ids)}\"\n",
    "elif loaded_parent_run_ids:\n",
    "    parents_part = (\n",
    "        f\"parents-{loaded_parent_run_ids[0]}-{loaded_parent_run_ids[-1]}-n{len(loaded_parent_run_ids)}\"\n",
    "    )\n",
    "else:\n",
    "    parents_part = \"parents-none-none-n0\"\n",
    "\n",
    "if ordered_hashes:\n",
    "    groups_part = f\"groups-{ordered_hashes[0][:8]}-{ordered_hashes[-1][:8]}-n{len(ordered_hashes)}\"\n",
    "else:\n",
    "    groups_part = \"groups-none-none-n0\"\n",
    "\n",
    "comparison_key = (\n",
    "    f\"{parents_part}__subruns-n{len(subrun_summaries)}__{groups_part}\"\n",
    "    f\"__strategy-{STRATEGY}__metric-{METRIC_PATH}\"\n",
    ")\n",
    "FIGURE_SESSION = create_figure_session(\n",
    "    root_dir=PROJECT_ROOT / \"data\" / \"comparisons\" / \"exports\",\n",
    "    notebook_slug=f\"subrun_comparison/{PHASE}\",\n",
    "    context_key=comparison_key,\n",
    "    context={\n",
    "        \"phase\": PHASE,\n",
    "        \"strategy\": STRATEGY,\n",
    "        \"metric_path\": METRIC_PATH,\n",
    "        \"detail_pair\": DETAIL_PAIR,\n",
    "        \"parent_run_ids\": PARENT_RUN_IDS,\n",
    "        \"loaded_parent_run_ids\": loaded_parent_run_ids,\n",
    "        \"subrun_ids\": [summary.get(\"run_id\", \"unknown\") for summary in subrun_summaries],\n",
    "        \"group_hashes\": ordered_hashes,\n",
    "        \"group_labels\": group_labels,\n",
    "    },\n",
    ")\n",
    "print(f\"Figure export session: {FIGURE_SESSION.session_dir}\")\n",
    "\n",
    "if not subrun_summaries:\n",
    "    print(\"No subruns found. Run a subrun batch first:\")\n",
    "    print()\n",
    "    print(\n",
    "        \"  from memory_retrieval.experiments.batch_runner import BatchSubrunConfig, run_subrun_batch\"\n",
    "    )\n",
    "    print(\"  outcome = run_subrun_batch(BatchSubrunConfig(\")\n",
    "    print(\"      phase=PHASE2,\")\n",
    "    print(\"      parent_run_ids=[...],\")\n",
    "    print(\n",
    "        \"      experiment_config=ExperimentConfig(search_backend=VectorBackend(), reranker=Reranker(...)),\"\n",
    "    )\n",
    "    print(\"  ))\")\n",
    "else:\n",
    "    print(f\"Loaded {len(parent_summaries)} parent runs\")\n",
    "    print(f\"Loaded {len(subrun_summaries)} subruns\")\n",
    "    print(f\"Found {len(ordered_hashes)} fingerprint groups\")\n",
    "    print()\n",
    "\n",
    "    header = (\n",
    "        f\"{'Group':>6} {'Hash':>10} {'Parents':>8} {'Subruns':>8} \"\n",
    "        f\"{'Anchors':>8} {'Avg F1':>8} {'Kind':>20}\"\n",
    "    )\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for hash_key in ordered_hashes:\n",
    "        label = group_labels[hash_key]\n",
    "        group_summaries = fingerprint_groups[hash_key]\n",
    "        parent_count = sum(1 for summary in group_summaries if summary.get(\"source_kind\") == \"parent\")\n",
    "        subrun_count = sum(1 for summary in group_summaries if summary.get(\"source_kind\") == \"subrun\")\n",
    "\n",
    "        f1_values = []\n",
    "        for summary in group_summaries:\n",
    "            f1_value = _extract_f1_from_summary(\n",
    "                summary.get(\"macro_averaged\", {}), METRIC_PATH, STRATEGY\n",
    "            )\n",
    "            if f1_value is not None:\n",
    "                f1_values.append(f1_value)\n",
    "        avg_f1 = float(np.mean(f1_values)) if f1_values else float(\"nan\")\n",
    "\n",
    "        kind_parts = []\n",
    "        if parent_count:\n",
    "            kind_parts.append(\"parent\")\n",
    "        if subrun_count:\n",
    "            kind_parts.append(\"subrun\")\n",
    "        kind_text = \"+\".join(kind_parts) if kind_parts else \"unknown\"\n",
    "\n",
    "        avg_text = f\"{avg_f1:.4f}\" if not np.isnan(avg_f1) else \"n/a\"\n",
    "        print(\n",
    "            f\"{label:>6} {hash_key:>10} {parent_count:>8} {subrun_count:>8} \"\n",
    "            f\"{len(parent_ids_by_hash[hash_key]):>8} {avg_text:>8} {kind_text:>20}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nParent-anchor coverage (which groups each parent run has):\")\n",
    "    print(f\"{'Parent Run ID':<30} {'Groups':>12}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    groups_by_parent: dict[str, set[str]] = {}\n",
    "    for summary in all_summaries:\n",
    "        parent_id = _summary_parent_anchor(summary)\n",
    "        if parent_id is None:\n",
    "            continue\n",
    "        hash_key = _fingerprint_hash(summary)\n",
    "        groups_by_parent.setdefault(parent_id, set()).add(group_labels.get(hash_key, \"?\"))\n",
    "\n",
    "    for parent_run_id in sorted(groups_by_parent):\n",
    "        labels = \",\".join(sorted(groups_by_parent[parent_run_id]))\n",
    "        print(f\"{parent_run_id:<30} {labels:>12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Config Diff by Fingerprint Group\n",
    "\n",
    "if not ordered_hashes:\n",
    "    print(\"Cannot compute diffs: no fingerprint groups found.\")\n",
    "elif len(ordered_hashes) == 1:\n",
    "    only_hash = ordered_hashes[0]\n",
    "    print(f\"Only one fingerprint group found: {group_labels[only_hash]} ({only_hash})\")\n",
    "    print(\"Need at least two groups to compare config diffs.\")\n",
    "else:\n",
    "    baseline_hash = ordered_hashes[0]\n",
    "    baseline_label = group_labels[baseline_hash]\n",
    "    baseline_fingerprint = fingerprint_groups[baseline_hash][0].get(\"config_fingerprint\", {})\n",
    "\n",
    "    print(f\"Baseline group: {baseline_label} ({baseline_hash})\")\n",
    "\n",
    "    if not baseline_fingerprint:\n",
    "        print(\"Baseline group has no config fingerprint.\")\n",
    "    else:\n",
    "        for hash_key in ordered_hashes[1:]:\n",
    "            label = group_labels[hash_key]\n",
    "            candidate_fingerprint = fingerprint_groups[hash_key][0].get(\"config_fingerprint\", {})\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 72)\n",
    "            print(f\"Group {baseline_label} ({baseline_hash}) -> Group {label} ({hash_key})\")\n",
    "            print(\"=\" * 72)\n",
    "\n",
    "            if not candidate_fingerprint:\n",
    "                print(\"Target group has no config fingerprint.\")\n",
    "                continue\n",
    "\n",
    "            diff = fingerprint_diff(baseline_fingerprint, candidate_fingerprint)\n",
    "            if not diff:\n",
    "                print(\"No config differences detected.\")\n",
    "                continue\n",
    "\n",
    "            max_field_len = max(len(field) for field in diff)\n",
    "            for field, values in diff.items():\n",
    "                print(f\"  {field:<{max_field_len}}  {str(values['a']):<40} -> {values['b']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Statistical Comparison Matrix (A vs B, A vs C, B vs C, ...)\n",
    "\n",
    "pairwise_comparisons = []\n",
    "selected_pair_result = None\n",
    "comparison = None\n",
    "\n",
    "if len(ordered_hashes) < 2:\n",
    "    print(\"Need at least two fingerprint groups for comparison.\")\n",
    "else:\n",
    "    for hash_a, hash_b in combinations(ordered_hashes, 2):\n",
    "        label_a = group_labels[hash_a]\n",
    "        label_b = group_labels[hash_b]\n",
    "\n",
    "        matched_parent_ids = parent_ids_by_hash[hash_a] & parent_ids_by_hash[hash_b]\n",
    "        if not matched_parent_ids:\n",
    "            continue\n",
    "\n",
    "        group_a = [\n",
    "            summary\n",
    "            for summary in fingerprint_groups[hash_a]\n",
    "            if _summary_parent_anchor(summary) in matched_parent_ids\n",
    "        ]\n",
    "        group_b = [\n",
    "            summary\n",
    "            for summary in fingerprint_groups[hash_b]\n",
    "            if _summary_parent_anchor(summary) in matched_parent_ids\n",
    "        ]\n",
    "\n",
    "        result = compare_configs(\n",
    "            group_a,\n",
    "            group_b,\n",
    "            strategy=STRATEGY,\n",
    "            metric_path=METRIC_PATH,\n",
    "        )\n",
    "\n",
    "        pairwise_comparisons.append(\n",
    "            {\n",
    "                \"pair_id\": f\"{label_a}_vs_{label_b}\",\n",
    "                \"hash_a\": hash_a,\n",
    "                \"hash_b\": hash_b,\n",
    "                \"label_a\": label_a,\n",
    "                \"label_b\": label_b,\n",
    "                \"matched_parent_ids\": sorted(matched_parent_ids),\n",
    "                \"group_a\": group_a,\n",
    "                \"group_b\": group_b,\n",
    "                \"comparison\": result,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not pairwise_comparisons:\n",
    "        print(\"No comparable group pairs found after parent-anchor matching.\")\n",
    "    else:\n",
    "        print(\"=\" * 92)\n",
    "        print(\"PAIRWISE COMPARISONS (parent-anchor matched)\")\n",
    "        print(\"=\" * 92)\n",
    "        print(\n",
    "            f\"{'Pair':<10} {'Hash A':<10} {'Hash B':<10} {'Anchors':>8} {'Runs A':>8} {'Runs B':>8} \"\n",
    "            f\"{'Mean Diff':>10} {'p-value':>10} {'Significance':>14}\"\n",
    "        )\n",
    "        print(\"-\" * 92)\n",
    "\n",
    "        for entry in pairwise_comparisons:\n",
    "            result = entry[\"comparison\"]\n",
    "            if \"error\" in result:\n",
    "                print(\n",
    "                    f\"{entry['pair_id']:<10} {entry['hash_a']:<10} {entry['hash_b']:<10} \"\n",
    "                    f\"{len(entry['matched_parent_ids']):>8} {len(entry['group_a']):>8} {len(entry['group_b']):>8} \"\n",
    "                    f\"{'n/a':>10} {'n/a':>10} {'error':>14}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            mean_diff = result[\"mean_diff\"]\n",
    "            p_value = result[\"wilcoxon\"][\"p_value\"]\n",
    "            significance = result[\"significance\"]\n",
    "            print(\n",
    "                f\"{entry['pair_id']:<10} {entry['hash_a']:<10} {entry['hash_b']:<10} \"\n",
    "                f\"{len(entry['matched_parent_ids']):>8} {result['num_runs_a']:>8} {result['num_runs_b']:>8} \"\n",
    "                f\"{mean_diff:>+10.4f} {p_value:>10.4f} {significance:>14}\"\n",
    "            )\n",
    "\n",
    "        valid_pairs = [entry for entry in pairwise_comparisons if \"error\" not in entry[\"comparison\"]]\n",
    "\n",
    "        if not valid_pairs:\n",
    "            print(\"\\nAll pairwise comparisons returned errors.\")\n",
    "        else:\n",
    "            if DETAIL_PAIR:\n",
    "                normalized = DETAIL_PAIR.strip().upper().replace(\" \", \"\")\n",
    "                for entry in valid_pairs:\n",
    "                    pair_id = entry[\"pair_id\"].upper().replace(\" \", \"\")\n",
    "                    pair_compact = pair_id.replace(\"_VS_\", \"VS\")\n",
    "                    if normalized in {pair_id, pair_compact}:\n",
    "                        selected_pair_result = entry\n",
    "                        break\n",
    "\n",
    "            if selected_pair_result is None:\n",
    "                baseline_hash = ordered_hashes[0]\n",
    "                baseline_candidates = [\n",
    "                    entry for entry in valid_pairs if entry[\"hash_a\"] == baseline_hash\n",
    "                ]\n",
    "                selected_pair_result = baseline_candidates[0] if baseline_candidates else valid_pairs[0]\n",
    "\n",
    "            comparison = selected_pair_result[\"comparison\"]\n",
    "            print(\"\\nSelected detailed pair:\")\n",
    "            print(\n",
    "                f\"  {selected_pair_result['pair_id']} \"\n",
    "                f\"({selected_pair_result['hash_a']} vs {selected_pair_result['hash_b']})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  Mean F1: {selected_pair_result['label_a']}={comparison['mean_f1_a']:.4f}, \"\n",
    "                f\"{selected_pair_result['label_b']}={comparison['mean_f1_b']:.4f}, \"\n",
    "                f\"delta={comparison['mean_diff']:+.4f}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Per-Test-Case Paired Plot (Selected Pair)\n",
    "\n",
    "if selected_pair_result and comparison and \"paired_differences\" in comparison:\n",
    "    paired = comparison[\"paired_differences\"]\n",
    "    label_a = selected_pair_result[\"label_a\"]\n",
    "    label_b = selected_pair_result[\"label_b\"]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, max(5, len(paired) * 0.55 + 2)))\n",
    "\n",
    "    test_case_ids = [entry[\"test_case_id\"] for entry in paired]\n",
    "    y_positions = list(range(len(test_case_ids)))\n",
    "\n",
    "    # Left: paired dot plot\n",
    "    ax = axes[0]\n",
    "    for y_pos, entry in enumerate(paired):\n",
    "        diff_color = (\n",
    "            \"#2ecc71\"\n",
    "            if entry[\"diff\"] > 0.001\n",
    "            else \"#e74c3c\"\n",
    "            if entry[\"diff\"] < -0.001\n",
    "            else \"#95a5a6\"\n",
    "        )\n",
    "        ax.plot(\n",
    "            [entry[\"f1_a\"], entry[\"f1_b\"]],\n",
    "            [y_pos, y_pos],\n",
    "            color=diff_color,\n",
    "            linewidth=2.5,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        ax.scatter(\n",
    "            entry[\"f1_a\"], y_pos, color=\"#3498db\", s=70, zorder=5, edgecolors=\"white\", linewidth=0.5\n",
    "        )\n",
    "        ax.scatter(\n",
    "            entry[\"f1_b\"], y_pos, color=\"#e67e22\", s=70, zorder=5, edgecolors=\"white\", linewidth=0.5\n",
    "        )\n",
    "\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(test_case_ids, fontsize=8)\n",
    "    ax.set_xlabel(\"F1 Score\")\n",
    "    ax.set_title(f\"Per-Test-Case F1: Group {label_a} (blue) vs Group {label_b} (orange)\")\n",
    "    ax.scatter([], [], color=\"#3498db\", s=70, label=f\"Group {label_a}\")\n",
    "    ax.scatter([], [], color=\"#e67e22\", s=70, label=f\"Group {label_b}\")\n",
    "    ax.plot([], [], color=\"#2ecc71\", linewidth=2.5, label=\"Improved\")\n",
    "    ax.plot([], [], color=\"#e74c3c\", linewidth=2.5, label=\"Degraded\")\n",
    "    ax.legend(fontsize=8, loc=\"lower right\")\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "    # Right: delta bar chart\n",
    "    ax = axes[1]\n",
    "    diffs = [entry[\"diff\"] for entry in paired]\n",
    "    bar_colors = [\"#2ecc71\" if diff > 0 else \"#e74c3c\" for diff in diffs]\n",
    "    ax.barh(y_positions, diffs, color=bar_colors, alpha=0.75, edgecolor=\"white\", linewidth=0.3)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.8)\n",
    "    ax.axvline(\n",
    "        comparison[\"mean_diff\"],\n",
    "        color=\"#2c3e50\",\n",
    "        linewidth=1.5,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean delta: {comparison['mean_diff']:+.3f}\",\n",
    "    )\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(test_case_ids, fontsize=8)\n",
    "    ax.set_xlabel(f\"F1 Delta (Group {label_b} - Group {label_a})\")\n",
    "    ax.set_title(\"Per-Test-Case Delta\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        (\n",
    "            f\"Subrun Comparison - {PHASE} ({METRIC_LABEL}) | \"\n",
    "            f\"{selected_pair_result['pair_id']} | \"\n",
    "            f\"{comparison['significance'].replace('_', ' ')}\"\n",
    "        ),\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    pair_slug = selected_pair_result[\"pair_id\"].lower()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        f\"per_test_case_paired_plot_{pair_slug}\",\n",
    "        title=f\"Subrun Comparison - {PHASE} ({METRIC_LABEL}) - {selected_pair_result['pair_id']}\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")\n",
    "else:\n",
    "    print(\"Run Cell 4 first to generate at least one valid pairwise comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Threshold Sweep Overlay (All Fingerprint Groups)\n",
    "# Supports both metric paths:\n",
    "# - post_rerank: rerank score threshold sweep from rerank_strategies[STRATEGY].threshold_sweep\n",
    "# - pre_rerank: distance threshold sweep from baseline.distance_threshold_sweep\n",
    "\n",
    "if not ordered_hashes:\n",
    "    print(\"Cannot plot: no fingerprint groups loaded.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=max(3, len(ordered_hashes)))\n",
    "    line_styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "    is_pre_rerank_mode = METRIC_PATH == \"pre_rerank\"\n",
    "    threshold_label = (\n",
    "        \"Distance Threshold (cosine distance)\"\n",
    "        if is_pre_rerank_mode\n",
    "        else \"Rerank Score Threshold\"\n",
    "    )\n",
    "    annotation_prefix = \"d\" if is_pre_rerank_mode else \"t\"\n",
    "    plot_mode_label = \"Distance\" if is_pre_rerank_mode else \"Rerank Score\"\n",
    "\n",
    "    plotted_groups = 0\n",
    "\n",
    "    for group_index, hash_key in enumerate(ordered_hashes):\n",
    "        group_summaries = fingerprint_groups[hash_key]\n",
    "        label = group_labels.get(hash_key, hash_key[:8])\n",
    "        color = palette[group_index % len(palette)]\n",
    "\n",
    "        all_sweep_dicts: list[dict[float, float]] = []\n",
    "        for summary in group_summaries:\n",
    "            if is_pre_rerank_mode:\n",
    "                sweep_data = summary.get(\"baseline\", {}).get(\"distance_threshold_sweep\", {})\n",
    "            else:\n",
    "                sweep_data = (\n",
    "                    summary.get(\"rerank_strategies\", {})\n",
    "                    .get(STRATEGY, {})\n",
    "                    .get(\"threshold_sweep\", {})\n",
    "                )\n",
    "\n",
    "            full_sweep = sweep_data.get(\"full_sweep\", [])\n",
    "            if full_sweep:\n",
    "                all_sweep_dicts.append({entry[\"threshold\"]: entry[\"f1\"] for entry in full_sweep})\n",
    "\n",
    "        if not all_sweep_dicts:\n",
    "            continue\n",
    "\n",
    "        all_thresholds = sorted(set().union(*[set(sweep.keys()) for sweep in all_sweep_dicts]))\n",
    "\n",
    "        mean_f1_values = []\n",
    "        min_f1_values = []\n",
    "        max_f1_values = []\n",
    "        for threshold in all_thresholds:\n",
    "            f1_at_threshold = [sweep[threshold] for sweep in all_sweep_dicts if threshold in sweep]\n",
    "            if f1_at_threshold:\n",
    "                mean_f1_values.append(float(np.mean(f1_at_threshold)))\n",
    "                min_f1_values.append(min(f1_at_threshold))\n",
    "                max_f1_values.append(max(f1_at_threshold))\n",
    "\n",
    "        ax.plot(\n",
    "            all_thresholds,\n",
    "            mean_f1_values,\n",
    "            label=f\"Group {label} ({hash_key[:8]}, n={len(all_sweep_dicts)})\",\n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "            linestyle=line_styles[group_index % len(line_styles)],\n",
    "        )\n",
    "        plotted_groups += 1\n",
    "\n",
    "        if len(all_sweep_dicts) > 1:\n",
    "            ax.fill_between(all_thresholds, min_f1_values, max_f1_values, color=color, alpha=0.15)\n",
    "\n",
    "        optimal_index = int(np.argmax(mean_f1_values))\n",
    "        optimal_threshold = all_thresholds[optimal_index]\n",
    "        optimal_f1 = mean_f1_values[optimal_index]\n",
    "        ax.axvline(\n",
    "            optimal_threshold,\n",
    "            color=color,\n",
    "            linestyle=\":\",\n",
    "            alpha=0.55,\n",
    "            linewidth=1.2,\n",
    "        )\n",
    "        ax.annotate(\n",
    "            f\"{annotation_prefix}={optimal_threshold:.3f}\\nF1={optimal_f1:.3f}\",\n",
    "            xy=(optimal_threshold, optimal_f1),\n",
    "            xytext=(8, 8),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=7,\n",
    "            color=color,\n",
    "            ha=\"left\",\n",
    "        )\n",
    "\n",
    "    if plotted_groups == 0:\n",
    "        if is_pre_rerank_mode:\n",
    "            print(\n",
    "                \"No distance threshold sweep data found for METRIC_PATH='pre_rerank'. \"\n",
    "                \"Expected baseline.distance_threshold_sweep.full_sweep in run summaries.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"No rerank threshold sweep data found for METRIC_PATH='post_rerank' \"\n",
    "                f\"and STRATEGY='{STRATEGY}'.\"\n",
    "            )\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        ax.set_xlabel(threshold_label)\n",
    "        ax.set_ylabel(\"Macro F1\")\n",
    "        ax.set_title(f\"{plot_mode_label} Threshold Sweep by Fingerprint Group - {PHASE} ({METRIC_LABEL})\")\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        saved_paths = save_figure(\n",
    "            fig,\n",
    "            FIGURE_SESSION,\n",
    "            \"threshold_sweep_overlay_all_groups\",\n",
    "            title=f\"{plot_mode_label} Threshold Sweep by Fingerprint Group - {PHASE} ({METRIC_LABEL})\",\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"Saved: {saved_paths['png']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Per-Test-Case Heatmaps (All Groups + Selected Pair Delta)\n",
    "\n",
    "if not ordered_hashes:\n",
    "    print(\"Cannot plot: no fingerprint groups loaded.\")\n",
    "else:\n",
    "    all_test_case_ids = set()\n",
    "    for summary in all_summaries:\n",
    "        all_test_case_ids.update(summary.get(\"per_test_case\", {}).keys())\n",
    "    all_test_case_ids = sorted(all_test_case_ids)\n",
    "\n",
    "    heatmap_data = np.full((len(all_test_case_ids), len(ordered_hashes)), np.nan)\n",
    "    column_labels = []\n",
    "\n",
    "    for col_index, hash_key in enumerate(ordered_hashes):\n",
    "        label = group_labels.get(hash_key, hash_key[:8])\n",
    "        column_labels.append(f\"{label}\\n{hash_key[:8]}\")\n",
    "        group_summaries = fingerprint_groups[hash_key]\n",
    "\n",
    "        for row_index, test_case_id in enumerate(all_test_case_ids):\n",
    "            f1_values = []\n",
    "            for summary in group_summaries:\n",
    "                per_tc = summary.get(\"per_test_case\", {}).get(test_case_id, {})\n",
    "                f1_value = _extract_f1_from_per_test_case(per_tc, METRIC_PATH, STRATEGY)\n",
    "                if f1_value is not None:\n",
    "                    f1_values.append(f1_value)\n",
    "            if f1_values:\n",
    "                heatmap_data[row_index, col_index] = float(np.mean(f1_values))\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(max(8, len(ordered_hashes) * 2.2), max(6, len(all_test_case_ids) * 0.55))\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        xticklabels=column_labels,\n",
    "        yticklabels=all_test_case_ids,\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        linewidths=0.5,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(f\"Per-Test-Case F1 by Fingerprint Group - {PHASE} ({METRIC_LABEL})\")\n",
    "    ax.set_ylabel(\"Test Case\")\n",
    "    ax.set_xlabel(\"Fingerprint Group\")\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"per_test_case_f1_heatmap_all_groups\",\n",
    "        title=f\"Per-Test-Case F1 by Fingerprint Group - {PHASE} ({METRIC_LABEL})\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")\n",
    "\n",
    "    if selected_pair_result and comparison and \"paired_differences\" in comparison:\n",
    "        label_a = selected_pair_result[\"label_a\"]\n",
    "        label_b = selected_pair_result[\"label_b\"]\n",
    "        pair_id = selected_pair_result[\"pair_id\"]\n",
    "\n",
    "        delta_by_case = {\n",
    "            entry[\"test_case_id\"]: entry[\"diff\"] for entry in comparison.get(\"paired_differences\", [])\n",
    "        }\n",
    "        delta_rows = [delta_by_case.get(test_case_id, np.nan) for test_case_id in all_test_case_ids]\n",
    "        delta_data = np.array(delta_rows).reshape(-1, 1)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(3.3, max(6, len(all_test_case_ids) * 0.55)))\n",
    "        delta_max = max(float(np.nanmax(np.abs(delta_data))), 0.01)\n",
    "        sns.heatmap(\n",
    "            delta_data,\n",
    "            annot=True,\n",
    "            fmt=\"+.2f\",\n",
    "            xticklabels=[f\"Delta\\n({label_b}-{label_a})\"],\n",
    "            yticklabels=all_test_case_ids,\n",
    "            cmap=\"RdYlGn\",\n",
    "            vmin=-delta_max,\n",
    "            vmax=delta_max,\n",
    "            center=0,\n",
    "            linewidths=0.5,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(f\"Selected Pair Delta - {pair_id}\")\n",
    "        plt.tight_layout()\n",
    "        saved_paths = save_figure(\n",
    "            fig,\n",
    "            FIGURE_SESSION,\n",
    "            f\"per_test_case_delta_heatmap_{pair_id.lower()}\",\n",
    "            title=f\"Selected Pair Delta - {pair_id}\",\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"Saved: {saved_paths['png']}\")\n",
    "\n",
    "        print(\"\\nSelected pair test-case breakdown:\")\n",
    "        print(f\"  {'Test Case':<35} {'F1 A':>8} {'F1 B':>8} {'Delta':>8}\")\n",
    "        print(f\"  {'-' * 35} {'-' * 8} {'-' * 8} {'-' * 8}\")\n",
    "        for entry in sorted(comparison[\"paired_differences\"], key=lambda item: item[\"diff\"], reverse=True):\n",
    "            print(\n",
    "                f\"  {entry['test_case_id']:<35} {entry['f1_a']:>8.3f} {entry['f1_b']:>8.3f} {entry['diff']:>+8.3f}\"\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
