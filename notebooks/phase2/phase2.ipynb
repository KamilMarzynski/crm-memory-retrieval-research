{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Phase 2 — Full Pipeline with Reranking\n",
    "\n",
    "Independent pipeline that extracts its own memories, builds its own database and test cases,\n",
    "then runs retrieval experiments with cross-encoder reranking.\n",
    "\n",
    "Unlike `phase1_reranking_comparison.ipynb` (which reuses Phase 1 data), this notebook\n",
    "owns its entire pipeline end-to-end, allowing independent prompt/extraction iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENROUTER_API_KEY is set.\n",
      "Project root: /Users/mayk/Projects/private/crm-memory-retrieval-research\n",
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from memory_retrieval.memories.extractor import extract_memories, ExtractionConfig, SituationFormat\n",
    "from memory_retrieval.search.vector import VectorBackend\n",
    "from memory_retrieval.search.reranker import Reranker\n",
    "from memory_retrieval.experiments.runner import run_all_experiments, ExperimentConfig\n",
    "from memory_retrieval.experiments.test_cases import build_test_cases\n",
    "from memory_retrieval.memories.schema import FIELD_SITUATION, FIELD_DISTANCE, FIELD_RERANK_SCORE\n",
    "from memory_retrieval.infra.io import load_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    create_run, get_latest_run, get_run, list_runs, update_run_status,\n",
    "    PHASE2,\n",
    ")\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT == PROJECT_ROOT.parent:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Verify API key\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and experiments will fail.\")\n",
    "else:\n",
    "    print(\"OPENROUTER_API_KEY is set.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2 — Configuration\n\nPROMPT_VERSION = \"2.0.0\"\nMODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"    # LLM for memory extraction\nMODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\"  # LLM for query generation\n\nRAW_DATA_DIR = \"data/review_data\"\n\n# Reranking configuration\nRERANK_TOP_N = 4          # Final results after reranking\nSEARCH_LIMIT = 20         # Vector search candidates per query\nDISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics comparison\n\n# Rerank text strategies: compare reranking on situation-only vs situation+lesson\nRERANK_TEXT_STRATEGIES = {\n    \"situation_only\": lambda c: c[\"situation\"],\n    \"situation_and_lesson\": lambda c: f\"situation: {c['situation']}; lesson: {c.get('lesson', '')}\",\n}\n\n# Run selection: use latest run or select a specific one\n# To create a new run: RUN_DIR = None (will be created in Step 1)\n# To see available runs: print(list_runs(PHASE2))\n# To select specific run: RUN_DIR = get_run(PHASE2, \"run_20260209_120000\")\nRUN_DIR = get_latest_run(PHASE2)\n\n# Derived paths (automatic from run directory)\nif RUN_DIR is not None:\n    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n    RESULTS_DIR = str(RUN_DIR / \"results\")\n\n# Initialize backends\nvector_backend = VectorBackend()\n\nprint(\"Configuration:\")\nprint(f\"  Using run: {RUN_DIR.name if RUN_DIR else 'None (will create new)'}\")\nprint(f\"  Prompt version: {PROMPT_VERSION}\")\nprint(f\"  Model (memories): {MODEL_MEMORIES}\")\nprint(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\nprint(f\"  Rerank top-n: {RERANK_TOP_N}\")\nprint(f\"  Search limit: {SEARCH_LIMIT}\")\nprint(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\nprint(f\"  Rerank strategies: {list(RERANK_TEXT_STRATEGIES.keys())}\")\nprint(f\"  Raw data dir: {RAW_DATA_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words) and an **actionable lesson** (max 160 chars).\n",
    "\n",
    "Uses Phase 2 prompts from `data/prompts/phase2`.\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Build Memories: Single File\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(PHASE2)\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} raw data files:\")\n",
    "for i, f in enumerate(raw_files):\n",
    "    print(f\"  [{i}] {f.name}\")\n",
    "\n",
    "if raw_files:\n",
    "    target_file = raw_files[0]\n",
    "    print(f\"\\nProcessing: {target_file.name}\")\n",
    "    extraction_config = ExtractionConfig(\n",
    "        situation_format=SituationFormat.SINGLE,\n",
    "        prompts_dir=\"data/prompts/phase2\",\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "        model=MODEL_MEMORIES,\n",
    "    )\n",
    "    output_path = extract_memories(\n",
    "        raw_path=str(target_file),\n",
    "        out_dir=MEMORIES_DIR,\n",
    "        config=extraction_config,\n",
    "    )\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No raw data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Build Memories: All Files\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(PHASE2)\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing all {len(raw_files)} raw data files...\\n\")\n",
    "\n",
    "extraction_config = ExtractionConfig(\n",
    "    situation_format=SituationFormat.SINGLE,\n",
    "    prompts_dir=\"data/prompts/phase2\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_MEMORIES,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for f in raw_files:\n",
    "    print(f\"Processing: {f.name}\")\n",
    "    try:\n",
    "        output_path = extract_memories(\n",
    "            raw_path=str(f),\n",
    "            out_dir=MEMORIES_DIR,\n",
    "            config=extraction_config,\n",
    "        )\n",
    "        results.append({\"file\": f.name, \"output\": output_path, \"status\": \"ok\"})\n",
    "    except Exception as e:\n",
    "        results.append({\"file\": f.name, \"output\": None, \"status\": str(e)})\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "success_count = sum(1 for r in results if r['status'] == 'ok')\n",
    "print(f\"\\nSummary: {success_count}/{len(results)} files processed successfully.\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"build_memories\", {\n",
    "    \"count\": success_count,\n",
    "    \"failed\": len(results) - success_count,\n",
    "    \"prompt_version\": PROMPT_VERSION,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions\n",
    "as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rebuild-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Rebuild Database\n",
    "print(f\"Rebuilding database for run: {RUN_DIR.name}...\")\n",
    "vector_backend.rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n",
    "\n",
    "count = vector_backend.get_memory_count(DB_PATH)\n",
    "print(f\"Database rebuilt. Total memories indexed: {count}\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"db\", {\"memory_count\": count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Verify Database: Sample Search\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f'Sample search: \"{sample_query}\"\\n')\n",
    "\n",
    "results = vector_backend.search(db_path=DB_PATH, query=sample_query, limit=5)\n",
    "\n",
    "if results:\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"--- Result {i + 1} (distance: {r.raw_score:.4f}) ---\")\n",
    "        print(f\"  ID: {r.id}\")\n",
    "        print(f\"  Situation: {r.situation}\")\n",
    "        print(f\"  Lesson: {r.lesson}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found. Check that the database is populated and Ollama is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-test-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Build Test Cases\n",
    "print(f\"Building test cases for run: {RUN_DIR.name}...\\n\")\n",
    "build_test_cases(\n",
    "    raw_dir=RAW_DATA_DIR,\n",
    "    memories_dir=MEMORIES_DIR,\n",
    "    output_dir=TEST_CASES_DIR,\n",
    ")\n",
    "\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "print(f\"\\nGenerated {len(test_case_files)} test cases:\")\n",
    "for f in test_case_files:\n",
    "    tc = load_json(str(f))\n",
    "    gt_count = tc.get(\"ground_truth_count\", len(tc.get(\"ground_truth_memory_ids\", [])))\n",
    "    print(f\"  {f.name} — {gt_count} ground truth memories\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"test_cases\", {\"count\": len(test_case_files)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 — Run Experiments with Reranking\n",
    "\n",
    "For each test case:\n",
    "1. Generate search queries from PR context and diff via LLM\n",
    "2. Vector search for each query (top-20 candidates)\n",
    "3. Pool and deduplicate results across queries\n",
    "4. Rerank candidates with cross-encoder (bge-reranker-v2-m3)\n",
    "5. Take top-N results after reranking\n",
    "6. Compute metrics before and after reranking\n",
    "\n",
    "Requires `OPENROUTER_API_KEY` and Ollama with `mxbai-embed-large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-reranker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Test Reranker on Sample Query\n",
    "reranker = Reranker()\n",
    "\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f'Sample query: \"{sample_query}\"\\n')\n",
    "\n",
    "# Vector search\n",
    "results = vector_backend.search(DB_PATH, sample_query, limit=SEARCH_LIMIT)\n",
    "print(f\"Vector search returned {len(results)} candidates\\n\")\n",
    "\n",
    "# Convert SearchResult objects to dicts for reranker\n",
    "candidates = [\n",
    "    {\n",
    "        \"id\": r.id,\n",
    "        FIELD_SITUATION: r.situation,\n",
    "        \"lesson\": r.lesson,\n",
    "        FIELD_DISTANCE: r.raw_score,\n",
    "    }\n",
    "    for r in results\n",
    "]\n",
    "\n",
    "# Show top-5 by distance\n",
    "print(\"--- Top 5 by Vector Distance ---\")\n",
    "for i, c in enumerate(candidates[:5], 1):\n",
    "    print(f\"  [{i}] dist={c[FIELD_DISTANCE]:.4f} | {c['id']} | {c[FIELD_SITUATION][:80]}...\")\n",
    "\n",
    "# Rerank\n",
    "reranked = reranker.rerank(sample_query, candidates, top_n=RERANK_TOP_N)\n",
    "\n",
    "print(f\"\\n--- Top {RERANK_TOP_N} after Reranking ---\")\n",
    "for i, r in enumerate(reranked, 1):\n",
    "    print(f\"  [{i}] rerank={r[FIELD_RERANK_SCORE]:.4f} | dist={r[FIELD_DISTANCE]:.4f} | {r['id']}\")\n",
    "    print(f\"      {r[FIELD_SITUATION][:100]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 13 — Run All Experiments with Reranking\nprint(f\"Running all experiments for run: {RUN_DIR.name}...\\n\")\n\nconfig = ExperimentConfig(\n    search_backend=vector_backend,\n    prompts_dir=\"data/prompts/phase2\",\n    prompt_version=PROMPT_VERSION,\n    model=MODEL_EXPERIMENT,\n    search_limit=SEARCH_LIMIT,\n    distance_threshold=DISTANCE_THRESHOLD,\n    reranker=reranker,\n    rerank_top_n=RERANK_TOP_N,\n    rerank_text_strategies=RERANK_TEXT_STRATEGIES,\n)\nall_results = run_all_experiments(\n    test_cases_dir=TEST_CASES_DIR,\n    db_path=DB_PATH,\n    results_dir=RESULTS_DIR,\n    config=config,\n)\n\n# Update run status\nsuccessful = [r for r in all_results if \"post_rerank_metrics\" in r]\navg_f1 = sum(r[\"post_rerank_metrics\"][\"f1\"] for r in successful) / len(successful) if successful else 0\nupdate_run_status(RUN_DIR, \"experiment\", {\n    \"count\": len(successful),\n    \"failed\": len(all_results) - len(successful),\n    \"avg_f1_post_rerank\": round(avg_f1, 4),\n    \"rerank_top_n\": RERANK_TOP_N,\n    \"rerank_strategies\": list(RERANK_TEXT_STRATEGIES.keys()),\n    \"prompt_version\": PROMPT_VERSION,\n})"
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 — Results Analysis\n",
    "\n",
    "Per-test-case comparison of pre-rerank vs post-rerank metrics,\n",
    "top-N sweep, and score distribution analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 15 — Results Summary Table (Strategy Comparison)\n\nsuccessful = [r for r in all_results if \"post_rerank_metrics\" in r]\nhas_strategies = successful and \"rerank_strategies\" in successful[0]\nstrategy_names = list(RERANK_TEXT_STRATEGIES.keys()) if has_strategies else []\n\nif has_strategies:\n    # Multi-strategy comparison table\n    for strategy_name in strategy_names:\n        print(f\"\\n{'='*95}\")\n        print(f\"Strategy: {strategy_name}\")\n        print(f\"{'='*95}\")\n        print(f\"{'Test Case':<30} {'Pre-F1':>8} {'Post-F1':>9} {'Delta':>8} {'Pre-P':>7} {'Post-P':>8} {'Pre-R':>7} {'Post-R':>8}\")\n        print(\"-\" * 95)\n\n        for r in successful:\n            name = r.get(\"test_case_id\", \"?\")[:30]\n            pre = r[\"pre_rerank_metrics\"]\n            post = r[\"rerank_strategies\"][strategy_name][\"post_rerank_metrics\"]\n            delta = post[\"f1\"] - pre[\"f1\"]\n            marker = \"\\u2191\" if delta > 0 else \"\\u2193\" if delta < 0 else \"=\"\n            print(f\"{name:<30} {pre['f1']:>8.3f} {post['f1']:>9.3f} {delta:>+7.3f}{marker} {pre['precision']:>7.3f} {post['precision']:>8.3f} {pre['recall']:>7.3f} {post['recall']:>8.3f}\")\n\n        avg_pre_f1 = sum(r[\"pre_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n        avg_post_f1 = sum(r[\"rerank_strategies\"][strategy_name][\"post_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n        avg_pre_p = sum(r[\"pre_rerank_metrics\"][\"precision\"] for r in successful) / len(successful)\n        avg_post_p = sum(r[\"rerank_strategies\"][strategy_name][\"post_rerank_metrics\"][\"precision\"] for r in successful) / len(successful)\n        avg_pre_r = sum(r[\"pre_rerank_metrics\"][\"recall\"] for r in successful) / len(successful)\n        avg_post_r = sum(r[\"rerank_strategies\"][strategy_name][\"post_rerank_metrics\"][\"recall\"] for r in successful) / len(successful)\n        delta = avg_post_f1 - avg_pre_f1\n        marker = \"\\u2191\" if delta > 0 else \"\\u2193\" if delta < 0 else \"=\"\n        print(\"-\" * 95)\n        print(f\"{'AVERAGE':<30} {avg_pre_f1:>8.3f} {avg_post_f1:>9.3f} {delta:>+7.3f}{marker} {avg_pre_p:>7.3f} {avg_post_p:>8.3f} {avg_pre_r:>7.3f} {avg_post_r:>8.3f}\")\n\n    # Head-to-head comparison\n    print(f\"\\n{'='*60}\")\n    print(\"STRATEGY COMPARISON (avg F1)\")\n    print(f\"{'='*60}\")\n    avg_pre_f1 = sum(r[\"pre_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n    print(f\"  Pre-rerank:          {avg_pre_f1:.3f}\")\n    for strategy_name in strategy_names:\n        avg_f1 = sum(r[\"rerank_strategies\"][strategy_name][\"post_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n        delta = avg_f1 - avg_pre_f1\n        print(f\"  {strategy_name:<22} {avg_f1:.3f} ({delta:+.3f})\")\nelse:\n    # Single strategy (backward compat)\n    print(f\"{'Test Case':<30} {'Pre-F1':>8} {'Post-F1':>9} {'Delta':>8} {'Pre-P':>7} {'Post-P':>8} {'Pre-R':>7} {'Post-R':>8}\")\n    print(\"-\" * 95)\n\n    for r in successful:\n        name = r.get(\"test_case_id\", \"?\")[:30]\n        pre = r[\"pre_rerank_metrics\"]\n        post = r[\"post_rerank_metrics\"]\n        delta = post[\"f1\"] - pre[\"f1\"]\n        marker = \"\\u2191\" if delta > 0 else \"\\u2193\" if delta < 0 else \"=\"\n        print(f\"{name:<30} {pre['f1']:>8.3f} {post['f1']:>9.3f} {delta:>+7.3f}{marker} {pre['precision']:>7.3f} {post['precision']:>8.3f} {pre['recall']:>7.3f} {post['recall']:>8.3f}\")\n\n    if successful:\n        avg_pre_f1 = sum(r[\"pre_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n        avg_post_f1 = sum(r[\"post_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n        avg_pre_p = sum(r[\"pre_rerank_metrics\"][\"precision\"] for r in successful) / len(successful)\n        avg_post_p = sum(r[\"post_rerank_metrics\"][\"precision\"] for r in successful) / len(successful)\n        avg_pre_r = sum(r[\"pre_rerank_metrics\"][\"recall\"] for r in successful) / len(successful)\n        avg_post_r = sum(r[\"post_rerank_metrics\"][\"recall\"] for r in successful) / len(successful)\n        delta = avg_post_f1 - avg_pre_f1\n        marker = \"\\u2191\" if delta > 0 else \"\\u2193\" if delta < 0 else \"=\"\n        print(\"-\" * 95)\n        print(f\"{'AVERAGE':<30} {avg_pre_f1:>8.3f} {avg_post_f1:>9.3f} {delta:>+7.3f}{marker} {avg_pre_p:>7.3f} {avg_post_p:>8.3f} {avg_pre_r:>7.3f} {avg_post_r:>8.3f}\")\n        print(f\"\\nTarget F1 > 0.75: {'\\u2705 ACHIEVED' if avg_post_f1 > 0.75 else '\\u274c NOT YET'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topn-sweep",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 16 — Rerank Top-N Sweep Analysis (Per Strategy)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nFIGURES_DIR = Path(\"notebooks/phase2/figures\")\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\n# Load result files\nresults_path = Path(RESULTS_DIR)\nresult_files = sorted(results_path.glob(\"*.json\"))\n\nif not result_files:\n    print(\"No result files found. Run experiments first.\")\nelse:\n    all_data = [load_json(str(f)) for f in result_files]\n    n_cases = len(all_data)\n    has_strategies = \"rerank_strategies\" in all_data[0]\n\n    max_n = 20\n    n_values = list(range(1, max_n + 1))\n\n    strategy_keys = list(RERANK_TEXT_STRATEGIES.keys()) if has_strategies else [\"default\"]\n\n    def sweep_for_reranked(all_data, reranked_key_fn):\n        \"\"\"Compute precision/recall/F1 sweep for a given reranked results accessor.\"\"\"\n        sweep_p, sweep_r, sweep_f1 = [], [], []\n        for n in n_values:\n            n_p, n_r, n_f1 = [], [], []\n            for data in all_data:\n                gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n                gt_count = len(gt_ids)\n                reranked = reranked_key_fn(data)\n                top_n_ids = {r[\"id\"] for r in reranked[:n]}\n                hits = len(top_n_ids & gt_ids)\n                actual_n = len(top_n_ids)\n                p = hits / actual_n if actual_n > 0 else 0.0\n                r = hits / gt_count if gt_count > 0 else 0.0\n                f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n                n_p.append(p)\n                n_r.append(r)\n                n_f1.append(f1)\n            sweep_p.append(np.mean(n_p))\n            sweep_r.append(np.mean(n_r))\n            sweep_f1.append(np.mean(n_f1))\n        return sweep_p, sweep_r, sweep_f1\n\n    # Compute sweeps per strategy\n    strategy_sweeps = {}\n    for key in strategy_keys:\n        if has_strategies:\n            accessor = lambda data, k=key: data[\"rerank_strategies\"][k][\"reranked_results\"]\n        else:\n            accessor = lambda data: data.get(\"reranked_results\", [])\n        strategy_sweeps[key] = sweep_for_reranked(all_data, accessor)\n\n    # Print table per strategy\n    for key in strategy_keys:\n        sweep_p, sweep_r, sweep_f1 = strategy_sweeps[key]\n        print(f\"\\nRerank Top-N Sweep: {key} (averaged over {n_cases} test cases)\\n\")\n        print(f\"{'N':>4} {'Precision':>10} {'Recall':>8} {'F1':>8}\")\n        print(\"-\" * 34)\n        for n in [1, 2, 3, 4, 5, 6, 8, 10]:\n            if n <= max_n:\n                i = n - 1\n                print(f\"{n:>4} {sweep_p[i]:>10.3f} {sweep_r[i]:>8.3f} {sweep_f1[i]:>8.3f}\")\n\n    # Plot F1 comparison across strategies\n    colors = [\"#9b59b6\", \"#e67e22\", \"#3498db\", \"#2ecc71\"]\n    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\n    # Left: F1 comparison\n    ax = axes[0]\n    for i, key in enumerate(strategy_keys):\n        _, _, sweep_f1 = strategy_sweeps[key]\n        ax.plot(n_values, sweep_f1, label=key, color=colors[i % len(colors)], linewidth=2, marker=\"o\", markersize=4)\n    ax.axvline(x=RERANK_TOP_N, color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"Default N={RERANK_TOP_N}\")\n    ax.set_xlabel(\"Top-N (results kept after reranking)\")\n    ax.set_ylabel(\"F1 Score\")\n    ax.set_title(f\"F1 vs Rerank Top-N by Strategy (avg over {n_cases} test cases)\")\n    ax.set_xticks(n_values)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Right: P/R/F1 for first strategy (detailed view)\n    ax = axes[1]\n    sweep_p, sweep_r, sweep_f1 = strategy_sweeps[strategy_keys[0]]\n    ax.plot(n_values, sweep_p, label=\"Precision\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=4)\n    ax.plot(n_values, sweep_r, label=\"Recall\", color=\"#2ecc71\", linewidth=2, marker=\"s\", markersize=4)\n    ax.plot(n_values, sweep_f1, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=4)\n    ax.axvline(x=RERANK_TOP_N, color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"Default N={RERANK_TOP_N}\")\n    ax.set_xlabel(\"Top-N (results kept after reranking)\")\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"P/R/F1 vs Top-N: {strategy_keys[0]}\")\n    ax.set_xticks(n_values)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    fig.savefig(FIGURES_DIR / \"rerank_topn_sweep.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved: {FIGURES_DIR / 'rerank_topn_sweep.png'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-distribution",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 17 — Rerank Score Distribution (Per Strategy)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nif not result_files:\n    print(\"No result files found.\")\nelse:\n    has_strategies = \"rerank_strategies\" in all_data[0]\n    strategy_keys = list(RERANK_TEXT_STRATEGIES.keys()) if has_strategies else [\"default\"]\n\n    n_strategies = len(strategy_keys)\n    fig, axes = plt.subplots(n_strategies, 2, figsize=(14, 5 * n_strategies), squeeze=False)\n\n    for row, key in enumerate(strategy_keys):\n        gt_scores, non_gt_scores = [], []\n        gt_distances, non_gt_distances = [], []\n\n        for data in all_data:\n            reranked = (\n                data[\"rerank_strategies\"][key][\"reranked_results\"]\n                if has_strategies\n                else data.get(\"reranked_results\", [])\n            )\n            for r in reranked:\n                if r.get(\"is_ground_truth\"):\n                    gt_scores.append(r[\"rerank_score\"])\n                    gt_distances.append(r[\"distance\"])\n                else:\n                    non_gt_scores.append(r[\"rerank_score\"])\n                    non_gt_distances.append(r[\"distance\"])\n\n        # Rerank score distribution\n        ax = axes[row][0]\n        if gt_scores:\n            ax.hist(gt_scores, bins=20, alpha=0.7, label=f\"Ground Truth (n={len(gt_scores)})\", color=\"#2ecc71\")\n        if non_gt_scores:\n            ax.hist(non_gt_scores, bins=20, alpha=0.7, label=f\"Non-GT (n={len(non_gt_scores)})\", color=\"#e74c3c\")\n        ax.set_xlabel(\"Rerank Score\")\n        ax.set_ylabel(\"Count\")\n        ax.set_title(f\"Rerank Score Distribution: {key}\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # Rerank score vs distance scatter\n        ax = axes[row][1]\n        if gt_scores:\n            ax.scatter(gt_distances, gt_scores, alpha=0.7, label=\"Ground Truth\", color=\"#2ecc71\", s=60, zorder=3)\n        if non_gt_scores:\n            ax.scatter(non_gt_distances, non_gt_scores, alpha=0.5, label=\"Non-GT\", color=\"#e74c3c\", s=40, zorder=2)\n        ax.set_xlabel(\"Vector Distance\")\n        ax.set_ylabel(\"Rerank Score\")\n        ax.set_title(f\"Rerank Score vs Distance: {key}\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # Statistics\n        if gt_scores:\n            print(f\"[{key}] GT rerank scores:     min={min(gt_scores):.4f}, max={max(gt_scores):.4f}, mean={np.mean(gt_scores):.4f}\")\n        if non_gt_scores:\n            print(f\"[{key}] Non-GT rerank scores: min={min(non_gt_scores):.4f}, max={max(non_gt_scores):.4f}, mean={np.mean(non_gt_scores):.4f}\")\n        if gt_scores and non_gt_scores:\n            separation = np.mean(gt_scores) - np.mean(non_gt_scores)\n            print(f\"[{key}] Mean separation: {separation:.4f} ({'good' if separation > 1.0 else 'moderate' if separation > 0.5 else 'weak'})\")\n        print()\n\n    plt.tight_layout()\n    fig.savefig(FIGURES_DIR / \"rerank_score_distribution.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-test-case-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18 — Per-Test-Case Detail\n",
    "\n",
    "print(f\"{'Test Case':<30} {'Queries':>8} {'Candidates':>11} {'GT Count':>9} {'Hits':>6} {'Post-F1':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in all_results:\n",
    "    name = r.get(\"test_case_id\", \"?\")[:30]\n",
    "    n_queries = r.get(\"query_count\", len(r.get(\"queries\", [])))\n",
    "    n_candidates = len(r.get(\"reranked_results\", []))\n",
    "    gt_count = len(r.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    post = r.get(\"post_rerank_metrics\", {})\n",
    "    hits = post.get(\"ground_truth_retrieved\", 0)\n",
    "    f1 = post.get(\"f1\", 0)\n",
    "    print(f\"{name:<30} {n_queries:>8} {n_candidates:>11} {gt_count:>9} {hits:>6} {f1:>8.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}