{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 2 — Full Pipeline with Reranking\n",
    "\n",
    "Independent pipeline that extracts its own memories, builds its own database and test cases,\n",
    "then runs retrieval experiments with cross-encoder reranking.\n",
    "\n",
    "Unlike `phase1_reranking_comparison.ipynb` (which reuses Phase 1 data), this notebook\n",
    "owns its entire pipeline end-to-end, allowing independent prompt/extraction iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from memory_retrieval.experiments.query_generation import (\n",
    "    QueryGenerationConfig,\n",
    "    generate_all_queries,\n",
    ")\n",
    "from memory_retrieval.experiments.runner import ExperimentConfig, run_all_experiments\n",
    "from memory_retrieval.experiments.test_cases import build_test_cases\n",
    "from memory_retrieval.infra.figures import create_figure_session, save_figure\n",
    "from memory_retrieval.infra.io import load_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    PHASE2,\n",
    "    create_run,\n",
    "    update_run_status,\n",
    "    get_latest_run,\n",
    ")\n",
    "from memory_retrieval.memories.extractor import ExtractionConfig, SituationFormat, extract_memories\n",
    "from memory_retrieval.search.reranker import Reranker\n",
    "from memory_retrieval.search.vector import VectorBackend\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Verify API key\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and query generation will fail.\")\n",
    "else:\n",
    "    print(\"OPENROUTER_API_KEY is set.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration\n",
    "\n",
    "PROMPT_VERSION = \"3.0.0\"\n",
    "MODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"  # LLM for memory extraction\n",
    "MODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\"  # LLM for query generation\n",
    "\n",
    "RAW_DATA_DIR = \"data/review_data\"\n",
    "\n",
    "# Search & reranking configuration\n",
    "SEARCH_LIMIT = 20  # Vector search candidates per query\n",
    "DISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics comparison\n",
    "\n",
    "# Rerank text strategies: compare reranking on situation-only vs situation+lesson\n",
    "RERANK_TEXT_STRATEGIES = {\n",
    "    \"situation_only\": lambda c: c[\"situation\"],\n",
    "    \"situation_and_lesson\": lambda c: f\"situation: {c['situation']}; lesson: {c.get('lesson', '')}\",\n",
    "}\n",
    "\n",
    "# Run selection: use latest run or select a specific one\n",
    "# To create a new run: RUN_DIR = None (will be created in Step 1)\n",
    "# To see available runs: print(list_runs(PHASE2))\n",
    "# To select specific run: RUN_DIR = get_run(PHASE2, \"run_20260209_120000\")\n",
    "# RUN_DIR = get_latest_run(PHASE2)\n",
    "RUN_DIR = None\n",
    "\n",
    "# Derived paths (automatic from run directory)\n",
    "if RUN_DIR is not None:\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "\n",
    "# Initialize backends\n",
    "vector_backend = VectorBackend()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Using run: {RUN_DIR.name if RUN_DIR else 'None (will create new)'}\")\n",
    "print(f\"  Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"  Model (memories): {MODEL_MEMORIES}\")\n",
    "print(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\n",
    "print(f\"  Search limit: {SEARCH_LIMIT}\")\n",
    "print(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\n",
    "print(f\"  Rerank strategies: {list(RERANK_TEXT_STRATEGIES.keys())}\")\n",
    "print(f\"  Raw data dir: {RAW_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words) and an **actionable lesson** (max 160 chars).\n",
    "\n",
    "Uses Phase 2 prompts from `data/prompts/phase2`.\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Build Memories: Single File\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(PHASE2)\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} raw data files:\")\n",
    "for i, f in enumerate(raw_files):\n",
    "    print(f\"  [{i}] {f.name}\")\n",
    "\n",
    "if raw_files:\n",
    "    target_file = raw_files[0]\n",
    "    print(f\"\\nProcessing: {target_file.name}\")\n",
    "    extraction_config = ExtractionConfig(\n",
    "        situation_format=SituationFormat.SINGLE,\n",
    "        prompts_dir=\"data/prompts/phase2\",\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "        model=MODEL_MEMORIES,\n",
    "    )\n",
    "    output_path = extract_memories(\n",
    "        raw_path=str(target_file),\n",
    "        out_dir=MEMORIES_DIR,\n",
    "        config=extraction_config,\n",
    "    )\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No raw data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Build Memories: All Files\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(PHASE2)\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing all {len(raw_files)} raw data files...\\n\")\n",
    "\n",
    "extraction_config = ExtractionConfig(\n",
    "    situation_format=SituationFormat.SINGLE,\n",
    "    prompts_dir=\"data/prompts/phase2\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_MEMORIES,\n",
    ")\n",
    "\n",
    "extraction_results = []\n",
    "for raw_file in raw_files:\n",
    "    print(f\"Processing: {raw_file.name}\")\n",
    "    try:\n",
    "        output_path = extract_memories(\n",
    "            raw_path=str(raw_file),\n",
    "            out_dir=MEMORIES_DIR,\n",
    "            config=extraction_config,\n",
    "        )\n",
    "        extraction_results.append({\"file\": raw_file.name, \"output\": output_path, \"status\": \"ok\"})\n",
    "    except Exception as exc:\n",
    "        extraction_results.append({\"file\": raw_file.name, \"output\": None, \"status\": str(exc)})\n",
    "        print(f\"  ERROR: {exc}\")\n",
    "\n",
    "success_count = sum(1 for result in extraction_results if result[\"status\"] == \"ok\")\n",
    "print(f\"\\nSummary: {success_count}/{len(extraction_results)} files processed successfully.\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"build_memories\",\n",
    "    {\n",
    "        \"count\": success_count,\n",
    "        \"failed\": len(extraction_results) - success_count,\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions\n",
    "as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Rebuild Database\n",
    "print(f\"Rebuilding database for run: {RUN_DIR.name}...\")\n",
    "vector_backend.rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n",
    "\n",
    "count = vector_backend.get_memory_count(DB_PATH)\n",
    "print(f\"Database rebuilt. Total memories indexed: {count}\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"db\", {\"memory_count\": count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Verify Database: Sample Search\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f'Sample search: \"{sample_query}\"\\n')\n",
    "\n",
    "results = vector_backend.search(db_path=DB_PATH, query=sample_query, limit=5)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"--- Result {i + 1} (distance: {result.raw_score:.4f}) ---\")\n",
    "        print(f\"  ID: {result.id}\")\n",
    "        print(f\"  Situation: {result.situation}\")\n",
    "        print(f\"  Lesson: {result.lesson}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found. Check that the database is populated and Ollama is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Build Test Cases\n",
    "print(f\"Building test cases for run: {RUN_DIR.name}...\\n\")\n",
    "build_test_cases(\n",
    "    raw_dir=RAW_DATA_DIR,\n",
    "    memories_dir=MEMORIES_DIR,\n",
    "    output_dir=TEST_CASES_DIR,\n",
    ")\n",
    "\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "print(f\"\\nGenerated {len(test_case_files)} test cases:\")\n",
    "for test_case_file in test_case_files:\n",
    "    test_case = load_json(str(test_case_file))\n",
    "    ground_truth_count = test_case.get(\n",
    "        \"ground_truth_count\", len(test_case.get(\"ground_truth_memory_ids\", []))\n",
    "    )\n",
    "    print(f\"  {test_case_file.name} — {ground_truth_count} ground truth memories\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"test_cases\", {\"count\": len(test_case_files)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 4 — Generate Queries\n",
    "\n",
    "Generates search queries from each test case's PR context and diff via LLM.\n",
    "Queries are saved as separate JSON files in the `queries/` directory so they can be\n",
    "reused across multiple experiment runs without re-calling the API.\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Generate Queries for All Test Cases\n",
    "print(f\"Generating queries for run: {RUN_DIR.name}...\\n\")\n",
    "\n",
    "query_config = QueryGenerationConfig(\n",
    "    prompts_dir=\"data/prompts/phase2\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_EXPERIMENT,\n",
    ")\n",
    "all_query_data = generate_all_queries(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    queries_dir=QUERIES_DIR,\n",
    "    config=query_config,\n",
    "    db_path=DB_PATH,\n",
    "    search_backend=vector_backend,\n",
    ")\n",
    "\n",
    "successful_queries = [data for data in all_query_data if \"queries\" in data]\n",
    "total_queries = sum(len(data[\"queries\"]) for data in successful_queries)\n",
    "print(\n",
    "    f\"\\nGenerated queries for {len(successful_queries)} test cases ({total_queries} total queries)\"\n",
    ")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"query_generation\",\n",
    "    {\n",
    "        \"count\": len(successful_queries),\n",
    "        \"total_queries\": total_queries,\n",
    "        \"model\": MODEL_EXPERIMENT,\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5 — Run Experiments with Reranking\n",
    "\n",
    "For each test case:\n",
    "1. Loads pre-generated search queries from the `queries/` directory\n",
    "2. Vector search for each query (top-20 candidates)\n",
    "3. Pool and deduplicate results across queries\n",
    "4. Rerank all candidates with cross-encoder (bge-reranker-v2-m3)\n",
    "5. Store all reranked results for downstream analysis (top-N sweep, threshold sweep)\n",
    "6. Compute pre-rerank metrics as baseline\n",
    "\n",
    "Requires Ollama with `mxbai-embed-large`. Does NOT require `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Run All Experiments with Reranking\n",
    "print(f\"Running all experiments for run: {RUN_DIR.name}...\\n\")\n",
    "\n",
    "reranker = Reranker()\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    search_backend=vector_backend,\n",
    "    search_limit=SEARCH_LIMIT,\n",
    "    distance_threshold=DISTANCE_THRESHOLD,\n",
    "    reranker=reranker,\n",
    "    rerank_text_strategies=RERANK_TEXT_STRATEGIES,\n",
    ")\n",
    "all_results = run_all_experiments(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    queries_dir=QUERIES_DIR,\n",
    "    db_path=DB_PATH,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Update run status\n",
    "successful = [result for result in all_results if \"pre_rerank_metrics\" in result]\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"experiment\",\n",
    "    {\n",
    "        \"count\": len(successful),\n",
    "        \"failed\": len(all_results) - len(successful),\n",
    "        \"rerank_strategies\": list(RERANK_TEXT_STRATEGIES.keys()),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6 — Results Analysis\n",
    "\n",
    "Fair comparison of distance-based vs rerank-based retrieval at the same N,\n",
    "top-N sweep with distance baseline, score distribution analysis,\n",
    "and **rerank score threshold sweep** (main deliverable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A — Fair Top-N Comparison (same N for distance and rerank)\n",
    "import numpy as np\n",
    "\n",
    "from retrieval_metrics.compute import compute_threshold_metrics, compute_top_n_metrics\n",
    "from retrieval_metrics.sweeps import find_optimal_entry\n",
    "\n",
    "from memory_retrieval.experiments.metrics import pool_and_deduplicate_by_distance\n",
    "from memory_retrieval.experiments.metrics_adapter import (\n",
    "    restriction_evaluation_to_dict,\n",
    "    threshold_sweep_from_experiments as sweep_threshold,\n",
    "    top_n_sweep_from_experiments as sweep_top_n,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics_at_top_n(ranked_results, ground_truth_ids, top_n, id_field=\"id\"):\n",
    "    evaluation = compute_top_n_metrics(ranked_results, ground_truth_ids, top_n, id_key=id_field)\n",
    "    return restriction_evaluation_to_dict(evaluation)\n",
    "\n",
    "\n",
    "def compute_metrics_at_threshold(\n",
    "    ranked_results,\n",
    "    ground_truth_ids,\n",
    "    threshold,\n",
    "    score_field,\n",
    "    higher_is_better,\n",
    "    id_field=\"id\",\n",
    "):\n",
    "    evaluation = compute_threshold_metrics(\n",
    "        ranked_results,\n",
    "        ground_truth_ids,\n",
    "        threshold,\n",
    "        score_key=score_field,\n",
    "        higher_is_better=higher_is_better,\n",
    "        id_key=id_field,\n",
    "    )\n",
    "    return restriction_evaluation_to_dict(evaluation, include_accepted_count=True)\n",
    "\n",
    "\n",
    "def find_optimal_threshold(sweep_results, metric=\"f1\"):\n",
    "    return find_optimal_entry(sweep_results, metric_key=metric)\n",
    "\n",
    "# Analysis parameter: how many results to compare at\n",
    "ANALYSIS_TOP_N = 4\n",
    "\n",
    "\n",
    "# --- Helper (strategy-specific, stays in notebook) ---\n",
    "def get_reranked_results(data, strategy_name):\n",
    "    \"\"\"Get reranked_results for a strategy, handling single vs multi-strategy format.\"\"\"\n",
    "    if \"rerank_strategies\" in data and strategy_name in data[\"rerank_strategies\"]:\n",
    "        return data[\"rerank_strategies\"][strategy_name][\"reranked_results\"]\n",
    "    return data.get(\"reranked_results\", [])\n",
    "\n",
    "\n",
    "# --- Load results ---\n",
    "from pathlib import Path\n",
    "\n",
    "results_path = Path(RESULTS_DIR)\n",
    "result_files = sorted(results_path.glob(\"*.json\"))\n",
    "all_data = [load_json(str(file_path)) for file_path in result_files]\n",
    "\n",
    "successful = [data for data in all_data if \"pre_rerank_metrics\" in data]\n",
    "has_strategies = successful and \"rerank_strategies\" in successful[0]\n",
    "strategy_names = list(RERANK_TEXT_STRATEGIES.keys()) if has_strategies else [\"default\"]\n",
    "\n",
    "top_n = ANALYSIS_TOP_N\n",
    "\n",
    "for strategy_name in strategy_names:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"Fair Top-{top_n} Comparison: distance vs rerank — Strategy: {strategy_name}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    print(\n",
    "        f\"{'Test Case':<25} {'Dist F1':>8} {'Rank F1':>8} {'Delta':>8} {'Dist P':>7} {'Rank P':>7} {'Dist R':>7} {'Rank R':>7} {'GT':>4}\"\n",
    "    )\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    distance_f1_scores, rerank_f1_scores = [], []\n",
    "\n",
    "    for data in successful:\n",
    "        test_case_id = data.get(\"test_case_id\", \"?\")[:25]\n",
    "        ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "\n",
    "        # Distance baseline: pool + dedup + top-N by distance\n",
    "        pooled_by_dist = pool_and_deduplicate_by_distance(data.get(\"queries\", []))\n",
    "        distance_metrics = compute_metrics_at_top_n(pooled_by_dist, ground_truth_ids, top_n)\n",
    "\n",
    "        # Rerank: top-N by rerank score\n",
    "        reranked = get_reranked_results(data, strategy_name)\n",
    "        rerank_metrics = compute_metrics_at_top_n(reranked, ground_truth_ids, top_n)\n",
    "\n",
    "        delta = rerank_metrics[\"f1\"] - distance_metrics[\"f1\"]\n",
    "        marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "        print(\n",
    "            f\"{test_case_id:<25} {distance_metrics['f1']:>8.3f} {rerank_metrics['f1']:>8.3f} {delta:>+7.3f}{marker} {distance_metrics['precision']:>7.3f} {rerank_metrics['precision']:>7.3f} {distance_metrics['recall']:>7.3f} {rerank_metrics['recall']:>7.3f} {len(ground_truth_ids):>4}\"\n",
    "        )\n",
    "\n",
    "        distance_f1_scores.append(distance_metrics[\"f1\"])\n",
    "        rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "\n",
    "    avg_distance_f1 = np.mean(distance_f1_scores)\n",
    "    avg_rerank_f1 = np.mean(rerank_f1_scores)\n",
    "    delta = avg_rerank_f1 - avg_distance_f1\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'AVERAGE':<25} {avg_distance_f1:>8.3f} {avg_rerank_f1:>8.3f} {delta:>+7.3f}{marker}\")\n",
    "\n",
    "    improved = sum(\n",
    "        1\n",
    "        for dist_f1, rank_f1 in zip(distance_f1_scores, rerank_f1_scores)\n",
    "        if rank_f1 > dist_f1 + 0.001\n",
    "    )\n",
    "    same = sum(\n",
    "        1\n",
    "        for dist_f1, rank_f1 in zip(distance_f1_scores, rerank_f1_scores)\n",
    "        if abs(rank_f1 - dist_f1) <= 0.001\n",
    "    )\n",
    "    worse = sum(\n",
    "        1\n",
    "        for dist_f1, rank_f1 in zip(distance_f1_scores, rerank_f1_scores)\n",
    "        if rank_f1 < dist_f1 - 0.001\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nReranking helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\"\n",
    "    )\n",
    "\n",
    "# Strategy comparison summary\n",
    "if len(strategy_names) > 1:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"STRATEGY COMPARISON (fair top-{top_n} avg F1)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Distance baseline (same for all strategies)\n",
    "    distance_experiments = [\n",
    "        {\n",
    "            \"ground_truth_ids\": set(data.get(\"ground_truth\", {}).get(\"memory_ids\", [])),\n",
    "            \"ranked_results\": pool_and_deduplicate_by_distance(data.get(\"queries\", [])),\n",
    "        }\n",
    "        for data in successful\n",
    "    ]\n",
    "    distance_sweep_at_n = sweep_top_n(distance_experiments, [top_n])\n",
    "    print(f\"  Distance top-{top_n}:        {distance_sweep_at_n[0]['f1']:.3f}\")\n",
    "\n",
    "    for strategy_name in strategy_names:\n",
    "        strategy_experiments = [\n",
    "            {\n",
    "                \"ground_truth_ids\": set(data.get(\"ground_truth\", {}).get(\"memory_ids\", [])),\n",
    "                \"ranked_results\": get_reranked_results(data, strategy_name),\n",
    "            }\n",
    "            for data in successful\n",
    "        ]\n",
    "        strategy_sweep_at_n = sweep_top_n(strategy_experiments, [top_n])\n",
    "        delta = strategy_sweep_at_n[0][\"f1\"] - distance_sweep_at_n[0][\"f1\"]\n",
    "        print(\n",
    "            f\"  {strategy_name:<22} {strategy_sweep_at_n[0]['f1']:.3f} ({delta:+.3f} vs distance)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B — Top-N Sweep: Distance Baseline vs Rerank Strategies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != RUN_DIR.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=RUN_DIR / \"figures\",\n",
    "        notebook_slug=\"phase2\",\n",
    "        context_key=RUN_DIR.name,\n",
    "        context={\"phase\": PHASE2, \"run_id\": RUN_DIR.name},\n",
    "    )\n",
    "print(f\"Figure export session: {FIGURE_SESSION.session_dir}\")\n",
    "\n",
    "num_test_cases = len(successful)\n",
    "max_n = 20\n",
    "n_values = list(range(1, max_n + 1))\n",
    "\n",
    "# Distance baseline sweep\n",
    "distance_experiments = [\n",
    "    {\n",
    "        \"ground_truth_ids\": set(data.get(\"ground_truth\", {}).get(\"memory_ids\", [])),\n",
    "        \"ranked_results\": pool_and_deduplicate_by_distance(data.get(\"queries\", [])),\n",
    "    }\n",
    "    for data in successful\n",
    "]\n",
    "distance_sweep = sweep_top_n(distance_experiments, n_values)\n",
    "distance_f1_scores = [entry[\"f1\"] for entry in distance_sweep]\n",
    "\n",
    "# Per-strategy rerank sweeps\n",
    "strategy_sweeps = {}\n",
    "for key in strategy_names:\n",
    "    rerank_experiments = [\n",
    "        {\n",
    "            \"ground_truth_ids\": set(data.get(\"ground_truth\", {}).get(\"memory_ids\", [])),\n",
    "            \"ranked_results\": get_reranked_results(data, key),\n",
    "        }\n",
    "        for data in successful\n",
    "    ]\n",
    "    strategy_sweeps[key] = sweep_top_n(rerank_experiments, n_values)\n",
    "\n",
    "# Print table\n",
    "print(f\"Top-N Sweep: Distance vs Rerank (averaged over {num_test_cases} test cases)\\n\")\n",
    "header = f\"{'N':>4} {'Dist F1':>8}\"\n",
    "for key in strategy_names:\n",
    "    header += f\" {key[:16] + ' F1':>18} {'Delta':>7}\"\n",
    "print(header)\n",
    "print(\"-\" * (14 + 27 * len(strategy_names)))\n",
    "for top_n in [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]:\n",
    "    if top_n <= max_n:\n",
    "        index = top_n - 1\n",
    "        row = f\"{top_n:>4} {distance_f1_scores[index]:>8.3f}\"\n",
    "        for key in strategy_names:\n",
    "            strategy_f1 = strategy_sweeps[key][index][\"f1\"]\n",
    "            delta = strategy_f1 - distance_f1_scores[index]\n",
    "            row += f\" {strategy_f1:>18.3f} {delta:>+7.3f}\"\n",
    "        print(row)\n",
    "\n",
    "# --- Plots ---\n",
    "colors_rerank = [\"#9b59b6\", \"#e67e22\", \"#2ecc71\", \"#e74c3c\"]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Left: F1 comparison — distance baseline + all strategies\n",
    "ax = axes[0]\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    distance_f1_scores,\n",
    "    label=\"Distance (baseline)\",\n",
    "    color=\"#3498db\",\n",
    "    linewidth=2.5,\n",
    "    marker=\"o\",\n",
    "    markersize=5,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "for i, key in enumerate(strategy_names):\n",
    "    strategy_f1_values = [entry[\"f1\"] for entry in strategy_sweeps[key]]\n",
    "    ax.plot(\n",
    "        n_values,\n",
    "        strategy_f1_values,\n",
    "        label=f\"Rerank: {key}\",\n",
    "        color=colors_rerank[i % len(colors_rerank)],\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        markersize=4,\n",
    "    )\n",
    "ax.axvline(\n",
    "    x=ANALYSIS_TOP_N, color=\"gray\", linestyle=\":\", alpha=0.5, label=f\"Default N={ANALYSIS_TOP_N}\"\n",
    ")\n",
    "ax.set_xlabel(\"Top-N (results kept)\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(f\"F1 vs Top-N: Distance vs Rerank (avg over {num_test_cases} test cases)\")\n",
    "ax.set_xticks(n_values)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: P/R/F1 detail for best strategy + distance baseline\n",
    "best_strategy = max(strategy_names, key=lambda k: strategy_sweeps[k][ANALYSIS_TOP_N - 1][\"f1\"])\n",
    "best_sweep = strategy_sweeps[best_strategy]\n",
    "best_precisions = [entry[\"precision\"] for entry in best_sweep]\n",
    "best_recalls = [entry[\"recall\"] for entry in best_sweep]\n",
    "best_f1_values = [entry[\"f1\"] for entry in best_sweep]\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    distance_f1_scores,\n",
    "    label=\"F1 (distance)\",\n",
    "    color=\"#3498db\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    best_precisions,\n",
    "    label=f\"Precision ({best_strategy})\",\n",
    "    color=\"#e74c3c\",\n",
    "    linewidth=1.5,\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    ")\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    best_recalls,\n",
    "    label=f\"Recall ({best_strategy})\",\n",
    "    color=\"#2ecc71\",\n",
    "    linewidth=1.5,\n",
    "    marker=\"s\",\n",
    "    markersize=3,\n",
    ")\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    best_f1_values,\n",
    "    label=f\"F1 ({best_strategy})\",\n",
    "    color=\"#9b59b6\",\n",
    "    linewidth=2.5,\n",
    "    marker=\"^\",\n",
    "    markersize=4,\n",
    ")\n",
    "ax.axvline(\n",
    "    x=ANALYSIS_TOP_N, color=\"gray\", linestyle=\":\", alpha=0.5, label=f\"Default N={ANALYSIS_TOP_N}\"\n",
    ")\n",
    "ax.set_xlabel(\"Top-N (results kept)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"P/R/F1 vs Top-N: {best_strategy} (with distance F1 baseline)\")\n",
    "ax.set_xticks(n_values)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig,\n",
    "    FIGURE_SESSION,\n",
    "    \"rerank_topn_sweep\",\n",
    "    title=\"F1 vs Top-N: Distance vs Rerank\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C — Rerank Score Distribution (Per Strategy)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != RUN_DIR.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=RUN_DIR / \"figures\",\n",
    "        notebook_slug=\"phase2\",\n",
    "        context_key=RUN_DIR.name,\n",
    "        context={\"phase\": PHASE2, \"run_id\": RUN_DIR.name},\n",
    "    )\n",
    "\n",
    "n_strategies = len(strategy_names)\n",
    "fig, axes = plt.subplots(n_strategies, 3, figsize=(20, 5 * n_strategies), squeeze=False)\n",
    "\n",
    "for row, key in enumerate(strategy_names):\n",
    "    gt_scores, non_gt_scores = [], []\n",
    "    gt_distances, non_gt_distances = [], []\n",
    "    per_test_case_gt_scores = {}\n",
    "    per_test_case_non_gt_scores = {}\n",
    "\n",
    "    for data in successful:\n",
    "        test_case_id = data.get(\"test_case_id\", \"?\")\n",
    "        test_case_label = test_case_id[:20]\n",
    "        reranked = get_reranked_results(data, key)\n",
    "        tc_gt, tc_non_gt = [], []\n",
    "        for result in reranked:\n",
    "            if result.get(\"is_ground_truth\"):\n",
    "                gt_scores.append(result[\"rerank_score\"])\n",
    "                gt_distances.append(result[\"distance\"])\n",
    "                tc_gt.append(result[\"rerank_score\"])\n",
    "            else:\n",
    "                non_gt_scores.append(result[\"rerank_score\"])\n",
    "                non_gt_distances.append(result[\"distance\"])\n",
    "                tc_non_gt.append(result[\"rerank_score\"])\n",
    "        if tc_gt or tc_non_gt:\n",
    "            per_test_case_gt_scores[test_case_label] = tc_gt\n",
    "            per_test_case_non_gt_scores[test_case_label] = tc_non_gt\n",
    "\n",
    "    gt_arr = np.array(gt_scores)\n",
    "    non_gt_arr = np.array(non_gt_scores)\n",
    "\n",
    "    # --- Column 1: Density histogram (non-GT) + KDE overlay + GT strip markers ---\n",
    "    ax = axes[row][0]\n",
    "    all_scores = (\n",
    "        np.concatenate([gt_arr, non_gt_arr])\n",
    "        if len(gt_arr) and len(non_gt_arr)\n",
    "        else (gt_arr if len(gt_arr) else non_gt_arr)\n",
    "    )\n",
    "    score_min, score_max = all_scores.min(), all_scores.max()\n",
    "    bins = np.linspace(score_min, score_max, 30)\n",
    "\n",
    "    if len(non_gt_arr):\n",
    "        ax.hist(\n",
    "            non_gt_arr,\n",
    "            bins=bins,\n",
    "            alpha=0.5,\n",
    "            density=True,\n",
    "            label=f\"Non-GT (n={len(non_gt_arr)})\",\n",
    "            color=\"#e74c3c\",\n",
    "            edgecolor=\"white\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        # KDE overlay for non-GT\n",
    "        if len(non_gt_arr) > 3:\n",
    "            kde = gaussian_kde(non_gt_arr)\n",
    "            x_kde = np.linspace(score_min, score_max, 200)\n",
    "            ax.plot(x_kde, kde(x_kde), color=\"#c0392b\", linewidth=2, label=\"Non-GT KDE\")\n",
    "        ax.axvline(\n",
    "            np.median(non_gt_arr),\n",
    "            color=\"#c0392b\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            label=f\"Non-GT median: {np.median(non_gt_arr):.4f}\",\n",
    "        )\n",
    "\n",
    "    if len(gt_arr):\n",
    "        # GT as rug/strip plot — individual vertical lines at bottom\n",
    "        y_max = ax.get_ylim()[1]\n",
    "        strip_height = y_max * 0.08\n",
    "        for score in gt_arr:\n",
    "            ax.plot(\n",
    "                [score, score],\n",
    "                [0, strip_height],\n",
    "                color=\"#2ecc71\",\n",
    "                linewidth=2.5,\n",
    "                alpha=0.8,\n",
    "                zorder=5,\n",
    "            )\n",
    "        # Invisible scatter for legend entry\n",
    "        ax.scatter(\n",
    "            [],\n",
    "            [],\n",
    "            color=\"#2ecc71\",\n",
    "            marker=\"|\",\n",
    "            s=100,\n",
    "            linewidth=2.5,\n",
    "            label=f\"GT individuals (n={len(gt_arr)})\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            np.median(gt_arr),\n",
    "            color=\"#27ae60\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            label=f\"GT median: {np.median(gt_arr):.4f}\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Rerank Score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(f\"Rerank Score Distribution: {key}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- Column 2: Rerank score vs distance scatter (unchanged) ---\n",
    "    ax = axes[row][1]\n",
    "    if gt_scores:\n",
    "        ax.scatter(\n",
    "            gt_distances,\n",
    "            gt_scores,\n",
    "            alpha=0.7,\n",
    "            label=\"Ground Truth\",\n",
    "            color=\"#2ecc71\",\n",
    "            s=60,\n",
    "            zorder=3,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "    if non_gt_scores:\n",
    "        ax.scatter(\n",
    "            non_gt_distances,\n",
    "            non_gt_scores,\n",
    "            alpha=0.4,\n",
    "            label=\"Non-GT\",\n",
    "            color=\"#e74c3c\",\n",
    "            s=30,\n",
    "            zorder=2,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "    ax.set_xlabel(\"Vector Distance\")\n",
    "    ax.set_ylabel(\"Rerank Score\")\n",
    "    ax.set_title(f\"Rerank Score vs Distance: {key}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- Column 3: Per-test-case box/strip comparison ---\n",
    "    ax = axes[row][2]\n",
    "    test_case_labels = sorted(per_test_case_gt_scores.keys() | per_test_case_non_gt_scores.keys())\n",
    "    y_positions = list(range(len(test_case_labels)))\n",
    "\n",
    "    for y_pos, label in enumerate(test_case_labels):\n",
    "        tc_gt = per_test_case_gt_scores.get(label, [])\n",
    "        tc_non_gt = per_test_case_non_gt_scores.get(label, [])\n",
    "\n",
    "        # Non-GT as box plot\n",
    "        if tc_non_gt:\n",
    "            box = ax.boxplot(\n",
    "                [tc_non_gt],\n",
    "                positions=[y_pos],\n",
    "                vert=False,\n",
    "                widths=0.4,\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor=\"#e74c3c\", alpha=0.3, edgecolor=\"#c0392b\"),\n",
    "                medianprops=dict(color=\"#c0392b\", linewidth=1.5),\n",
    "                whiskerprops=dict(color=\"#c0392b\", alpha=0.5),\n",
    "                capprops=dict(color=\"#c0392b\", alpha=0.5),\n",
    "                flierprops=dict(marker=\".\", markerfacecolor=\"#e74c3c\", markersize=3, alpha=0.4),\n",
    "                manage_ticks=False,\n",
    "            )\n",
    "\n",
    "        # GT as individual strip markers\n",
    "        if tc_gt:\n",
    "            ax.scatter(\n",
    "                tc_gt,\n",
    "                [y_pos] * len(tc_gt),\n",
    "                color=\"#2ecc71\",\n",
    "                marker=\"D\",\n",
    "                s=50,\n",
    "                zorder=5,\n",
    "                edgecolors=\"white\",\n",
    "                linewidth=0.5,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(test_case_labels, fontsize=7)\n",
    "    ax.set_xlabel(\"Rerank Score\")\n",
    "    ax.set_title(f\"Per-Test-Case Scores: {key}\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "    # Legend\n",
    "    ax.scatter([], [], color=\"#2ecc71\", marker=\"D\", s=50, label=\"GT\")\n",
    "    ax.plot([], [], color=\"#e74c3c\", linewidth=6, alpha=0.3, label=\"Non-GT (box)\")\n",
    "    ax.legend(fontsize=8, loc=\"lower right\")\n",
    "\n",
    "    # --- Statistics ---\n",
    "    if len(gt_arr):\n",
    "        print(\n",
    "            f\"[{key}] GT rerank scores:     n={len(gt_arr):>3}, min={gt_arr.min():.4f}, max={gt_arr.max():.4f}, mean={gt_arr.mean():.4f}, median={np.median(gt_arr):.4f}\"\n",
    "        )\n",
    "    if len(non_gt_arr):\n",
    "        print(\n",
    "            f\"[{key}] Non-GT rerank scores: n={len(non_gt_arr):>3}, min={non_gt_arr.min():.4f}, max={non_gt_arr.max():.4f}, mean={non_gt_arr.mean():.4f}, median={np.median(non_gt_arr):.4f}\"\n",
    "        )\n",
    "    if len(gt_arr) and len(non_gt_arr):\n",
    "        separation = gt_arr.mean() - non_gt_arr.mean()\n",
    "        print(\n",
    "            f\"[{key}] Mean separation: {separation:.4f} ({'good' if separation > 1.0 else 'moderate' if separation > 0.5 else 'weak'})\"\n",
    "        )\n",
    "    # Per-test-case counts\n",
    "    gt_per_tc = [len(per_test_case_gt_scores.get(label, [])) for label in test_case_labels]\n",
    "    non_gt_per_tc = [len(per_test_case_non_gt_scores.get(label, [])) for label in test_case_labels]\n",
    "    print(f\"[{key}] Per-test-case GT counts: {gt_per_tc} (total unique memories: {len(gt_arr)})\")\n",
    "    print(\n",
    "        f\"[{key}] Per-test-case Non-GT counts: {non_gt_per_tc} (total unique memories: {len(non_gt_arr)})\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "plt.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig,\n",
    "    FIGURE_SESSION,\n",
    "    \"rerank_score_distribution\",\n",
    "    title=\"Rerank Score Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D — Rerank Score Threshold Sweep (MAIN DELIVERABLE)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != RUN_DIR.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=RUN_DIR / \"figures\",\n",
    "        notebook_slug=\"phase2\",\n",
    "        context_key=RUN_DIR.name,\n",
    "        context={\"phase\": PHASE2, \"run_id\": RUN_DIR.name},\n",
    "    )\n",
    "\n",
    "# Build per-strategy experiment lists for sweep\n",
    "experiments_by_strategy = {}\n",
    "for key in strategy_names:\n",
    "    experiment_results = []\n",
    "    for data in successful:\n",
    "        ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        reranked = get_reranked_results(data, key)\n",
    "        experiment_results.append(\n",
    "            {\n",
    "                \"test_case_id\": data[\"test_case_id\"],\n",
    "                \"ground_truth_ids\": ground_truth_ids,\n",
    "                \"ranked_results\": reranked,\n",
    "                \"reranked\": reranked,\n",
    "            }\n",
    "        )\n",
    "    experiments_by_strategy[key] = experiment_results\n",
    "\n",
    "# Determine sweep range from data\n",
    "all_max_scores = []\n",
    "for key in strategy_names:\n",
    "    for experiment_result in experiments_by_strategy[key]:\n",
    "        if experiment_result[\"ranked_results\"]:\n",
    "            all_max_scores.append(\n",
    "                max(result[\"rerank_score\"] for result in experiment_result[\"ranked_results\"])\n",
    "            )\n",
    "global_max = max(all_max_scores) if all_max_scores else 1.0\n",
    "sweep_thresholds = list(np.arange(0.0, global_max + 0.01, 0.005))\n",
    "\n",
    "# Sweep thresholds per strategy using centralized metrics\n",
    "strategy_sweep_results = {}\n",
    "for key in strategy_names:\n",
    "    sweep_results = sweep_threshold(\n",
    "        experiments_by_strategy[key],\n",
    "        sweep_thresholds,\n",
    "        score_field=\"rerank_score\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "    optimal = find_optimal_threshold(sweep_results, metric=\"f1\")\n",
    "    strategy_sweep_results[key] = {\n",
    "        \"sweep\": sweep_results,\n",
    "        \"best_f1_index\": optimal[\"index\"],\n",
    "        \"best_threshold\": optimal[\"threshold\"],\n",
    "    }\n",
    "\n",
    "# --- Print optimal thresholds ---\n",
    "print(\"OPTIMAL THRESHOLDS PER STRATEGY\")\n",
    "best_overall_strategy = None\n",
    "best_overall_f1 = -1\n",
    "for key in strategy_names:\n",
    "    sweep_data = strategy_sweep_results[key]\n",
    "    best_entry = sweep_data[\"sweep\"][sweep_data[\"best_f1_index\"]]\n",
    "    print(\n",
    "        f\"  {key + ':':30} threshold={sweep_data['best_threshold']:.4f}  F1={best_entry['f1']:.3f}  P={best_entry['precision']:.3f}  R={best_entry['recall']:.3f}  MRR={best_entry['mrr']:.3f}\"\n",
    "    )\n",
    "    if best_entry[\"f1\"] > best_overall_f1:\n",
    "        best_overall_f1 = best_entry[\"f1\"]\n",
    "        best_overall_strategy = key\n",
    "\n",
    "print(f\"\\nBest strategy: {best_overall_strategy}\")\n",
    "\n",
    "# --- Plots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Left: P/R/F1/MRR vs threshold for best strategy\n",
    "best_sweep = strategy_sweep_results[best_overall_strategy][\"sweep\"]\n",
    "best_precisions = [entry[\"precision\"] for entry in best_sweep]\n",
    "best_recalls = [entry[\"recall\"] for entry in best_sweep]\n",
    "best_f1_values = [entry[\"f1\"] for entry in best_sweep]\n",
    "best_mrr_values = [entry[\"mrr\"] for entry in best_sweep]\n",
    "best_threshold = strategy_sweep_results[best_overall_strategy][\"best_threshold\"]\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(sweep_thresholds, best_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, best_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, best_f1_values, label=\"F1\", color=\"#9b59b6\", linewidth=2.5)\n",
    "ax.plot(sweep_thresholds, best_mrr_values, label=\"MRR\", color=\"#e67e22\", linewidth=1.5, alpha=0.7)\n",
    "ax.axvline(\n",
    "    best_threshold,\n",
    "    color=\"#e74c3c\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Optimal @ {best_threshold:.4f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Rerank Score Threshold (accept >= threshold)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"P/R/F1/MRR vs Threshold: {best_overall_strategy}\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: F1 vs threshold for ALL strategies\n",
    "ax = axes[1]\n",
    "colors_strategy = [\"#9b59b6\", \"#e67e22\", \"#2ecc71\", \"#e74c3c\"]\n",
    "for i, key in enumerate(strategy_names):\n",
    "    strategy_f1_values = [entry[\"f1\"] for entry in strategy_sweep_results[key][\"sweep\"]]\n",
    "    ax.plot(\n",
    "        sweep_thresholds,\n",
    "        strategy_f1_values,\n",
    "        label=key,\n",
    "        color=colors_strategy[i % len(colors_strategy)],\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.axvline(\n",
    "        strategy_sweep_results[key][\"best_threshold\"],\n",
    "        color=colors_strategy[i % len(colors_strategy)],\n",
    "        linestyle=\":\",\n",
    "        alpha=0.5,\n",
    "        linewidth=1,\n",
    "    )\n",
    "ax.set_xlabel(\"Rerank Score Threshold\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"F1 vs Threshold: All Strategies\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig,\n",
    "    FIGURE_SESSION,\n",
    "    \"rerank_threshold_sweep\",\n",
    "    title=\"Rerank Score Threshold Sweep\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "# --- Threshold table for best strategy ---\n",
    "print(\n",
    "    f\"\\nThreshold table: {best_overall_strategy} (macro-averaged over {len(successful)} test cases)\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "table_thresholds = sorted(\n",
    "    set(\n",
    "        [\n",
    "            0.001,\n",
    "            0.005,\n",
    "            0.01,\n",
    "            0.02,\n",
    "            0.03,\n",
    "            0.05,\n",
    "            0.08,\n",
    "            0.10,\n",
    "            0.15,\n",
    "            0.20,\n",
    "            0.30,\n",
    "            0.50,\n",
    "            round(best_threshold, 4),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "experiment_results = experiments_by_strategy[best_overall_strategy]\n",
    "for threshold in table_thresholds:\n",
    "    if threshold > global_max + 0.01:\n",
    "        continue\n",
    "    # Find closest sweep index\n",
    "    index = min(\n",
    "        range(len(sweep_thresholds)), key=lambda idx: abs(sweep_thresholds[idx] - threshold)\n",
    "    )\n",
    "    sweep_entry = strategy_sweep_results[best_overall_strategy][\"sweep\"][index]\n",
    "    avg_accepted = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                [\n",
    "                    result\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"rerank_score\"] >= threshold\n",
    "                ]\n",
    "            )\n",
    "            for experiment_result in experiment_results\n",
    "        ]\n",
    "    )\n",
    "    avg_ground_truth_kept = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                {\n",
    "                    result[\"id\"]\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"rerank_score\"] >= threshold\n",
    "                }\n",
    "                & experiment_result[\"ground_truth_ids\"]\n",
    "            )\n",
    "            for experiment_result in experiment_results\n",
    "        ]\n",
    "    )\n",
    "    marker = \" <-- optimal\" if abs(threshold - best_threshold) < 0.003 else \"\"\n",
    "    print(\n",
    "        f\"{threshold:>10.4f} {sweep_entry['precision']:>10.3f} {sweep_entry['recall']:>8.3f} {sweep_entry['f1']:>8.3f} {sweep_entry['mrr']:>8.3f} {avg_accepted:>13.1f} {avg_ground_truth_kept:>12.1f}{marker}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell E — Per-Test-Case Impact at Optimal Threshold\n",
    "import numpy as np\n",
    "\n",
    "top_n = ANALYSIS_TOP_N\n",
    "key = best_overall_strategy\n",
    "best_threshold = strategy_sweep_results[key][\"best_threshold\"]\n",
    "experiment_results = experiments_by_strategy[key]\n",
    "\n",
    "print(\"Per-test-case comparison at best strategy's optimal threshold\")\n",
    "print(f\"  Strategy: {key}\")\n",
    "print(f\"  Threshold: score >= {best_threshold:.4f}\")\n",
    "print(f\"  Distance baseline: top-{top_n} by distance\")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    f\"{'Test Case':<20} {'Dist F1':>8} {'Thr F1':>8} {'Delta':>8} {'Dist P':>7} {'Thr P':>7} {'Dist R':>7} {'Thr R':>7} {'D Acc':>6} {'T Acc':>6} {'GT':>4}\"\n",
    ")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "distance_f1_scores, threshold_f1_scores = [], []\n",
    "\n",
    "for data, experiment_result in zip(successful, experiment_results):\n",
    "    test_case_id = data[\"test_case_id\"]\n",
    "    ground_truth_ids = experiment_result[\"ground_truth_ids\"]\n",
    "    ground_truth_count = len(ground_truth_ids)\n",
    "\n",
    "    # Distance baseline: top-N\n",
    "    pooled = pool_and_deduplicate_by_distance(data.get(\"queries\", []))\n",
    "    distance_metrics = compute_metrics_at_top_n(pooled, ground_truth_ids, top_n)\n",
    "\n",
    "    # Threshold-based\n",
    "    threshold_metrics = compute_metrics_at_threshold(\n",
    "        experiment_result[\"reranked\"],\n",
    "        ground_truth_ids,\n",
    "        best_threshold,\n",
    "        score_field=\"rerank_score\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "\n",
    "    delta = threshold_metrics[\"f1\"] - distance_metrics[\"f1\"]\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "    print(\n",
    "        f\"{test_case_id:<20} {distance_metrics['f1']:>8.3f} {threshold_metrics['f1']:>8.3f} {delta:>+7.3f}{marker} {distance_metrics['precision']:>7.1%} {threshold_metrics['precision']:>7.1%} {distance_metrics['recall']:>7.1%} {threshold_metrics['recall']:>7.1%} {len(distance_metrics['retrieved_ids']):>6} {threshold_metrics['accepted_count']:>6} {ground_truth_count:>4}\"\n",
    "    )\n",
    "\n",
    "    # Show missed GT with their rerank scores\n",
    "    missed = ground_truth_ids - threshold_metrics[\"retrieved_ids\"]\n",
    "    if missed:\n",
    "        for memory_id in sorted(missed):\n",
    "            score = next(\n",
    "                (\n",
    "                    result[\"rerank_score\"]\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"id\"] == memory_id\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if score is not None:\n",
    "                print(\n",
    "                    f\"  {'':20} Missed: {memory_id}  score={score:.4f} (below threshold {best_threshold:.4f})\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  {'':20} Missed: {memory_id}  NOT IN CANDIDATE POOL (query miss)\")\n",
    "\n",
    "    distance_f1_scores.append(distance_metrics[\"f1\"])\n",
    "    threshold_f1_scores.append(threshold_metrics[\"f1\"])\n",
    "\n",
    "avg_distance_f1 = np.mean(distance_f1_scores)\n",
    "avg_threshold_f1 = np.mean(threshold_f1_scores)\n",
    "delta = avg_threshold_f1 - avg_distance_f1\n",
    "print(\"-\" * 105)\n",
    "print(\n",
    "    f\"{'AVERAGE':<20} {avg_distance_f1:>8.3f} {avg_threshold_f1:>8.3f} {delta:>+7.3f}{'+' if delta > 0.001 else '-' if delta < -0.001 else '='}\"\n",
    ")\n",
    "\n",
    "improved = sum(\n",
    "    1\n",
    "    for dist_f1, thr_f1 in zip(distance_f1_scores, threshold_f1_scores)\n",
    "    if thr_f1 > dist_f1 + 0.001\n",
    ")\n",
    "same = sum(\n",
    "    1\n",
    "    for dist_f1, thr_f1 in zip(distance_f1_scores, threshold_f1_scores)\n",
    "    if abs(thr_f1 - dist_f1) <= 0.001\n",
    ")\n",
    "worse = sum(\n",
    "    1\n",
    "    for dist_f1, thr_f1 in zip(distance_f1_scores, threshold_f1_scores)\n",
    "    if thr_f1 < dist_f1 - 0.001\n",
    ")\n",
    "print(\n",
    "    f\"\\nThreshold helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 7 — Store Config Fingerprint & Run Summary\n",
    "\n",
    "Stores a config fingerprint in `run.json` and generates a `run_summary.json` for\n",
    "cross-run comparison. The fingerprint captures all pipeline parameters that affect results.\n",
    "The summary precomputes metrics for fast comparison without loading full result files.\n",
    "\n",
    "See `notebooks/comparison/cross_run_comparison.ipynb` for cross-run analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell F — Store Config Fingerprint & Generate Run Summary\n",
    "from memory_retrieval.experiments.comparison import (\n",
    "    build_config_fingerprint,\n",
    "    generate_run_summary,\n",
    ")\n",
    "from memory_retrieval.infra.runs import update_config_fingerprint\n",
    "\n",
    "# Build fingerprint from this run's configuration\n",
    "fingerprint = build_config_fingerprint(\n",
    "    extraction_prompt_version=PROMPT_VERSION,\n",
    "    embedding_model=\"mxbai-embed-large\",\n",
    "    search_backend=\"vector\",\n",
    "    search_limit=SEARCH_LIMIT,\n",
    "    distance_threshold=DISTANCE_THRESHOLD,\n",
    "    query_model=MODEL_EXPERIMENT,\n",
    "    query_prompt_version=PROMPT_VERSION,\n",
    "    reranker_model=\"BAAI/bge-reranker-v2-m3\",\n",
    "    rerank_text_strategies=list(RERANK_TEXT_STRATEGIES.keys()),\n",
    ")\n",
    "\n",
    "# Store in run.json\n",
    "update_config_fingerprint(RUN_DIR, fingerprint)\n",
    "print(f\"Config fingerprint stored: {fingerprint['fingerprint_hash']}\")\n",
    "for key, value in fingerprint.items():\n",
    "    if key != \"fingerprint_hash\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Generate run summary\n",
    "summary = generate_run_summary(RUN_DIR, strategies=list(RERANK_TEXT_STRATEGIES.keys()))\n",
    "macro = summary.get(\"macro_averaged\", {})\n",
    "print(f\"\\nRun summary generated: {RUN_DIR / 'run_summary.json'}\")\n",
    "print(f\"  Test cases: {summary['num_test_cases']}\")\n",
    "\n",
    "# Pre-rerank: overfetched (raw) metrics\n",
    "pre_rerank = macro.get(\"pre_rerank\", {})\n",
    "overfetched = pre_rerank.get(\"overfetched\", {})\n",
    "print(\n",
    "    f\"\\n  Pre-rerank (overfetched, raw): F1={overfetched.get('f1', 0):.3f} \"\n",
    "    f\"P={overfetched.get('precision', 0):.3f} R={overfetched.get('recall', 0):.3f}\"\n",
    ")\n",
    "\n",
    "# Pre-rerank: at optimal distance threshold\n",
    "at_optimal_distance = pre_rerank.get(\"at_optimal_distance_threshold\", {})\n",
    "if at_optimal_distance:\n",
    "    print(\n",
    "        f\"  Pre-rerank (optimal distance threshold): F1={at_optimal_distance.get('f1', 0):.3f} \"\n",
    "        f\"P={at_optimal_distance.get('precision', 0):.3f} R={at_optimal_distance.get('recall', 0):.3f} \"\n",
    "        f\"@ threshold={at_optimal_distance.get('optimal_threshold', 0):.4f}\"\n",
    "    )\n",
    "\n",
    "# Pre-rerank: at optimal top-N\n",
    "at_optimal_top_n = pre_rerank.get(\"at_optimal_top_n\", {})\n",
    "if at_optimal_top_n:\n",
    "    print(\n",
    "        f\"  Pre-rerank (optimal top-N): F1={at_optimal_top_n.get('f1', 0):.3f} \"\n",
    "        f\"P={at_optimal_top_n.get('precision', 0):.3f} R={at_optimal_top_n.get('recall', 0):.3f} \"\n",
    "        f\"@ N={at_optimal_top_n.get('optimal_n', 0)}\"\n",
    "    )\n",
    "\n",
    "# Post-rerank per strategy\n",
    "post_rerank = macro.get(\"post_rerank\", {})\n",
    "for strategy_name, strategy_data in post_rerank.items():\n",
    "    at_threshold = strategy_data.get(\"at_optimal_threshold\", {})\n",
    "    at_top_n = strategy_data.get(\"at_optimal_top_n\", {})\n",
    "    print(f\"\\n  Post-rerank ({strategy_name}):\")\n",
    "    if at_threshold:\n",
    "        print(\n",
    "            f\"    At optimal threshold: F1={at_threshold.get('f1', 0):.3f} \"\n",
    "            f\"P={at_threshold.get('precision', 0):.3f} R={at_threshold.get('recall', 0):.3f} \"\n",
    "            f\"@ threshold={at_threshold.get('optimal_threshold', 0):.4f}\"\n",
    "        )\n",
    "    if at_top_n:\n",
    "        print(\n",
    "            f\"    At optimal top-N:     F1={at_top_n.get('f1', 0):.3f} \"\n",
    "            f\"P={at_top_n.get('precision', 0):.3f} R={at_top_n.get('recall', 0):.3f} \"\n",
    "            f\"@ N={at_top_n.get('optimal_n', 0)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
