{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 2 — Full Pipeline with Reranking\n",
    "\n",
    "Independent pipeline that extracts its own memories, builds its own database and test cases,\n",
    "then runs retrieval experiments with cross-encoder reranking.\n",
    "\n",
    "Unlike `phase1_reranking_comparison.ipynb` (which reuses Phase 1 data), this notebook\n",
    "owns its entire pipeline end-to-end, allowing independent prompt/extraction iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from memory_retrieval.experiments.query_generation import (\n",
    "    QueryGenerationConfig,\n",
    "    generate_all_queries,\n",
    ")\n",
    "from memory_retrieval.experiments.runner import ExperimentConfig, run_all_experiments\n",
    "from memory_retrieval.experiments.test_cases import build_test_cases\n",
    "from memory_retrieval.infra.io import load_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    PHASE2,\n",
    "    create_run,\n",
    "    update_run_status,\n",
    ")\n",
    "from memory_retrieval.memories.extractor import ExtractionConfig, SituationFormat, extract_memories\n",
    "from memory_retrieval.search.reranker import Reranker\n",
    "from memory_retrieval.search.vector import VectorBackend\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Verify API key\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and query generation will fail.\")\n",
    "else:\n",
    "    print(\"OPENROUTER_API_KEY is set.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration\n",
    "\n",
    "PROMPT_VERSION = \"2.0.0\"\n",
    "MODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"  # LLM for memory extraction\n",
    "MODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\"  # LLM for query generation\n",
    "\n",
    "RAW_DATA_DIR = \"data/review_data\"\n",
    "\n",
    "# Reranking configuration\n",
    "RERANK_TOP_N = 6  # Final results after reranking\n",
    "SEARCH_LIMIT = 20  # Vector search candidates per query\n",
    "DISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics comparison\n",
    "\n",
    "# Rerank text strategies: compare reranking on situation-only vs situation+lesson\n",
    "RERANK_TEXT_STRATEGIES = {\n",
    "    \"situation_only\": lambda c: c[\"situation\"],\n",
    "    \"situation_and_lesson\": lambda c: f\"situation: {c['situation']}; lesson: {c.get('lesson', '')}\",\n",
    "}\n",
    "\n",
    "# Run selection: use latest run or select a specific one\n",
    "# To create a new run: RUN_DIR = None (will be created in Step 1)\n",
    "# To see available runs: print(list_runs(PHASE2))\n",
    "# To select specific run: RUN_DIR = get_run(PHASE2, \"run_20260209_120000\")\n",
    "# RUN_DIR = get_latest_run(PHASE2)\n",
    "RUN_DIR = None\n",
    "\n",
    "# Derived paths (automatic from run directory)\n",
    "if RUN_DIR is not None:\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "\n",
    "# Initialize backends\n",
    "vector_backend = VectorBackend()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Using run: {RUN_DIR.name if RUN_DIR else 'None (will create new)'}\")\n",
    "print(f\"  Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"  Model (memories): {MODEL_MEMORIES}\")\n",
    "print(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\n",
    "print(f\"  Rerank top-n: {RERANK_TOP_N}\")\n",
    "print(f\"  Search limit: {SEARCH_LIMIT}\")\n",
    "print(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\n",
    "print(f\"  Rerank strategies: {list(RERANK_TEXT_STRATEGIES.keys())}\")\n",
    "print(f\"  Raw data dir: {RAW_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words) and an **actionable lesson** (max 160 chars).\n",
    "\n",
    "Uses Phase 2 prompts from `data/prompts/phase2`.\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Build Memories: Single File\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(PHASE2)\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} raw data files:\")\n",
    "for i, f in enumerate(raw_files):\n",
    "    print(f\"  [{i}] {f.name}\")\n",
    "\n",
    "if raw_files:\n",
    "    target_file = raw_files[0]\n",
    "    print(f\"\\nProcessing: {target_file.name}\")\n",
    "    extraction_config = ExtractionConfig(\n",
    "        situation_format=SituationFormat.SINGLE,\n",
    "        prompts_dir=\"data/prompts/phase2\",\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "        model=MODEL_MEMORIES,\n",
    "    )\n",
    "    output_path = extract_memories(\n",
    "        raw_path=str(target_file),\n",
    "        out_dir=MEMORIES_DIR,\n",
    "        config=extraction_config,\n",
    "    )\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No raw data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Build Memories: All Files\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(PHASE2)\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing all {len(raw_files)} raw data files...\\n\")\n",
    "\n",
    "extraction_config = ExtractionConfig(\n",
    "    situation_format=SituationFormat.SINGLE,\n",
    "    prompts_dir=\"data/prompts/phase2\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_MEMORIES,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for f in raw_files:\n",
    "    print(f\"Processing: {f.name}\")\n",
    "    try:\n",
    "        output_path = extract_memories(\n",
    "            raw_path=str(f),\n",
    "            out_dir=MEMORIES_DIR,\n",
    "            config=extraction_config,\n",
    "        )\n",
    "        results.append({\"file\": f.name, \"output\": output_path, \"status\": \"ok\"})\n",
    "    except Exception as e:\n",
    "        results.append({\"file\": f.name, \"output\": None, \"status\": str(e)})\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "success_count = sum(1 for r in results if r[\"status\"] == \"ok\")\n",
    "print(f\"\\nSummary: {success_count}/{len(results)} files processed successfully.\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"build_memories\",\n",
    "    {\n",
    "        \"count\": success_count,\n",
    "        \"failed\": len(results) - success_count,\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions\n",
    "as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Rebuild Database\n",
    "print(f\"Rebuilding database for run: {RUN_DIR.name}...\")\n",
    "vector_backend.rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n",
    "\n",
    "count = vector_backend.get_memory_count(DB_PATH)\n",
    "print(f\"Database rebuilt. Total memories indexed: {count}\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"db\", {\"memory_count\": count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Verify Database: Sample Search\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f'Sample search: \"{sample_query}\"\\n')\n",
    "\n",
    "results = vector_backend.search(db_path=DB_PATH, query=sample_query, limit=5)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"--- Result {i + 1} (distance: {result.raw_score:.4f}) ---\")\n",
    "        print(f\"  ID: {result.id}\")\n",
    "        print(f\"  Situation: {result.situation}\")\n",
    "        print(f\"  Lesson: {result.lesson}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found. Check that the database is populated and Ollama is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Build Test Cases\n",
    "print(f\"Building test cases for run: {RUN_DIR.name}...\\n\")\n",
    "build_test_cases(\n",
    "    raw_dir=RAW_DATA_DIR,\n",
    "    memories_dir=MEMORIES_DIR,\n",
    "    output_dir=TEST_CASES_DIR,\n",
    ")\n",
    "\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "print(f\"\\nGenerated {len(test_case_files)} test cases:\")\n",
    "for test_case_file in test_case_files:\n",
    "    test_case = load_json(str(test_case_file))\n",
    "    ground_truth_count = test_case.get(\n",
    "        \"ground_truth_count\", len(test_case.get(\"ground_truth_memory_ids\", []))\n",
    "    )\n",
    "    print(f\"  {test_case_file.name} — {ground_truth_count} ground truth memories\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"test_cases\", {\"count\": len(test_case_files)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 4 — Generate Queries\n",
    "\n",
    "Generates search queries from each test case's PR context and diff via LLM.\n",
    "Queries are saved as separate JSON files in the `queries/` directory so they can be\n",
    "reused across multiple experiment runs without re-calling the API.\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Generate Queries for All Test Cases\n",
    "print(f\"Generating queries for run: {RUN_DIR.name}...\\n\")\n",
    "\n",
    "query_config = QueryGenerationConfig(\n",
    "    prompts_dir=\"data/prompts/phase2\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_EXPERIMENT,\n",
    ")\n",
    "all_query_data = generate_all_queries(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    queries_dir=QUERIES_DIR,\n",
    "    config=query_config,\n",
    "    db_path=DB_PATH,\n",
    "    search_backend=vector_backend,\n",
    ")\n",
    "\n",
    "successful_queries = [data for data in all_query_data if \"queries\" in data]\n",
    "total_queries = sum(len(data[\"queries\"]) for data in successful_queries)\n",
    "print(\n",
    "    f\"\\nGenerated queries for {len(successful_queries)} test cases ({total_queries} total queries)\"\n",
    ")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"query_generation\",\n",
    "    {\n",
    "        \"count\": len(successful_queries),\n",
    "        \"total_queries\": total_queries,\n",
    "        \"model\": MODEL_EXPERIMENT,\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5 — Run Experiments with Reranking\n",
    "\n",
    "For each test case:\n",
    "1. Loads pre-generated search queries from the `queries/` directory\n",
    "2. Vector search for each query (top-20 candidates)\n",
    "3. Pool and deduplicate results across queries\n",
    "4. Rerank candidates with cross-encoder (bge-reranker-v2-m3)\n",
    "5. Take top-N results after reranking\n",
    "6. Compute metrics before and after reranking\n",
    "\n",
    "Requires Ollama with `mxbai-embed-large`. Does NOT require `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Run All Experiments with Reranking\n",
    "print(f\"Running all experiments for run: {RUN_DIR.name}...\\n\")\n",
    "\n",
    "reranker = Reranker()\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    search_backend=vector_backend,\n",
    "    search_limit=SEARCH_LIMIT,\n",
    "    distance_threshold=DISTANCE_THRESHOLD,\n",
    "    reranker=reranker,\n",
    "    rerank_top_n=RERANK_TOP_N,\n",
    "    rerank_text_strategies=RERANK_TEXT_STRATEGIES,\n",
    ")\n",
    "all_results = run_all_experiments(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    queries_dir=QUERIES_DIR,\n",
    "    db_path=DB_PATH,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Update run status\n",
    "successful = [result for result in all_results if \"post_rerank_metrics\" in result]\n",
    "avg_f1_score = (\n",
    "    sum(result[\"post_rerank_metrics\"][\"f1\"] for result in successful) / len(successful)\n",
    "    if successful\n",
    "    else 0\n",
    ")\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"experiment\",\n",
    "    {\n",
    "        \"count\": len(successful),\n",
    "        \"failed\": len(all_results) - len(successful),\n",
    "        \"avg_f1_post_rerank\": round(avg_f1_score, 4),\n",
    "        \"rerank_top_n\": RERANK_TOP_N,\n",
    "        \"rerank_strategies\": list(RERANK_TEXT_STRATEGIES.keys()),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6 — Results Analysis\n",
    "\n",
    "Fair comparison of distance-based vs rerank-based retrieval at the same N,\n",
    "top-N sweep with distance baseline, score distribution analysis,\n",
    "and **rerank score threshold sweep** (main deliverable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A — Fair Top-N Comparison (same N for distance and rerank)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Helpers ---\n",
    "def pool_and_deduplicate_by_distance(query_results):\n",
    "    \"\"\"Pool results from all queries and deduplicate by memory ID (keep best distance).\"\"\"\n",
    "    best = {}\n",
    "    for qr in query_results:\n",
    "        for r in qr.get(\"results\", []):\n",
    "            mid = r[\"id\"]\n",
    "            dist = r.get(\"distance\", float(\"inf\"))\n",
    "            if mid not in best or dist < best[mid].get(\"distance\", float(\"inf\")):\n",
    "                best[mid] = r\n",
    "    return sorted(best.values(), key=lambda x: x.get(\"distance\", 0))\n",
    "\n",
    "\n",
    "def get_reranked_results(data, strategy_name):\n",
    "    \"\"\"Get reranked_results for a strategy, handling single vs multi-strategy format.\"\"\"\n",
    "    if \"rerank_strategies\" in data and strategy_name in data[\"rerank_strategies\"]:\n",
    "        return data[\"rerank_strategies\"][strategy_name][\"reranked_results\"]\n",
    "    return data.get(\"reranked_results\", [])\n",
    "\n",
    "\n",
    "# --- Load results ---\n",
    "from pathlib import Path\n",
    "\n",
    "results_path = Path(RESULTS_DIR)\n",
    "result_files = sorted(results_path.glob(\"*.json\"))\n",
    "all_data = [load_json(str(f)) for f in result_files]\n",
    "\n",
    "successful = [d for d in all_data if \"post_rerank_metrics\" in d]\n",
    "has_strategies = successful and \"rerank_strategies\" in successful[0]\n",
    "strategy_names = list(RERANK_TEXT_STRATEGIES.keys()) if has_strategies else [\"default\"]\n",
    "\n",
    "N = RERANK_TOP_N\n",
    "\n",
    "for strategy_name in strategy_names:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"Fair Top-{N} Comparison: distance vs rerank — Strategy: {strategy_name}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    print(\n",
    "        f\"{'Test Case':<25} {'Dist F1':>8} {'Rank F1':>8} {'Delta':>8} {'Dist P':>7} {'Rank P':>7} {'Dist R':>7} {'Rank R':>7} {'GT':>4}\"\n",
    "    )\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    dist_f1s, rank_f1s = [], []\n",
    "\n",
    "    for data in successful:\n",
    "        tc_id = data.get(\"test_case_id\", \"?\")[:25]\n",
    "        gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        gt_count = len(gt_ids)\n",
    "\n",
    "        # Distance baseline: pool + dedup + top-N by distance\n",
    "        pooled_by_dist = pool_and_deduplicate_by_distance(data.get(\"queries\", []))\n",
    "        dist_top_n = pooled_by_dist[:N]\n",
    "        dist_ids = {r[\"id\"] for r in dist_top_n}\n",
    "        d_hits = len(dist_ids & gt_ids)\n",
    "        d_n = len(dist_ids)\n",
    "        d_p = d_hits / d_n if d_n > 0 else 0.0\n",
    "        d_r = d_hits / gt_count if gt_count > 0 else 0.0\n",
    "        d_f1 = 2 * d_p * d_r / (d_p + d_r) if (d_p + d_r) > 0 else 0.0\n",
    "\n",
    "        # Rerank: top-N by rerank score\n",
    "        reranked = get_reranked_results(data, strategy_name)\n",
    "        rank_top_n = reranked[:N]\n",
    "        rank_ids = {r[\"id\"] for r in rank_top_n}\n",
    "        r_hits = len(rank_ids & gt_ids)\n",
    "        r_n = len(rank_ids)\n",
    "        r_p = r_hits / r_n if r_n > 0 else 0.0\n",
    "        r_r = r_hits / gt_count if gt_count > 0 else 0.0\n",
    "        r_f1 = 2 * r_p * r_r / (r_p + r_r) if (r_p + r_r) > 0 else 0.0\n",
    "\n",
    "        delta = r_f1 - d_f1\n",
    "        marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "        print(\n",
    "            f\"{tc_id:<25} {d_f1:>8.3f} {r_f1:>8.3f} {delta:>+7.3f}{marker} {d_p:>7.3f} {r_p:>7.3f} {d_r:>7.3f} {r_r:>7.3f} {gt_count:>4}\"\n",
    "        )\n",
    "\n",
    "        dist_f1s.append(d_f1)\n",
    "        rank_f1s.append(r_f1)\n",
    "\n",
    "    avg_d = np.mean(dist_f1s)\n",
    "    avg_r = np.mean(rank_f1s)\n",
    "    delta = avg_r - avg_d\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'AVERAGE':<25} {avg_d:>8.3f} {avg_r:>8.3f} {delta:>+7.3f}{marker}\")\n",
    "\n",
    "    improved = sum(1 for d, r in zip(dist_f1s, rank_f1s) if r > d + 0.001)\n",
    "    same = sum(1 for d, r in zip(dist_f1s, rank_f1s) if abs(r - d) <= 0.001)\n",
    "    worse = sum(1 for d, r in zip(dist_f1s, rank_f1s) if r < d - 0.001)\n",
    "    print(\n",
    "        f\"\\nReranking helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\"\n",
    "    )\n",
    "\n",
    "# Strategy comparison summary\n",
    "if len(strategy_names) > 1:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"STRATEGY COMPARISON (fair top-{N} avg F1)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    # Distance baseline (same for all strategies)\n",
    "    dist_f1_all = []\n",
    "    for data in successful:\n",
    "        gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        pooled = pool_and_deduplicate_by_distance(data.get(\"queries\", []))\n",
    "        ids = {r[\"id\"] for r in pooled[:N]}\n",
    "        h = len(ids & gt_ids)\n",
    "        n = len(ids)\n",
    "        gc = len(gt_ids)\n",
    "        p = h / n if n else 0\n",
    "        r = h / gc if gc else 0\n",
    "        dist_f1_all.append(2 * p * r / (p + r) if (p + r) else 0)\n",
    "    print(f\"  Distance top-{N}:        {np.mean(dist_f1_all):.3f}\")\n",
    "    for sname in strategy_names:\n",
    "        sf1s = []\n",
    "        for data in successful:\n",
    "            gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "            rr = get_reranked_results(data, sname)[:N]\n",
    "            ids = {r[\"id\"] for r in rr}\n",
    "            h = len(ids & gt_ids)\n",
    "            n = len(ids)\n",
    "            gc = len(gt_ids)\n",
    "            p = h / n if n else 0\n",
    "            r = h / gc if gc else 0\n",
    "            sf1s.append(2 * p * r / (p + r) if (p + r) else 0)\n",
    "        delta = np.mean(sf1s) - np.mean(dist_f1_all)\n",
    "        print(f\"  {sname:<22} {np.mean(sf1s):.3f} ({delta:+.3f} vs distance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B — Top-N Sweep: Distance Baseline vs Rerank Strategies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FIGURES_DIR = RUN_DIR / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_cases = len(successful)\n",
    "max_n = 20\n",
    "n_values = list(range(1, max_n + 1))\n",
    "\n",
    "\n",
    "def compute_sweep(all_data, n_values, results_fn):\n",
    "    \"\"\"Compute P/R/F1 sweep for a given results accessor (returns list of dicts at each N).\"\"\"\n",
    "    sweep_p, sweep_r, sweep_f1 = [], [], []\n",
    "    for n in n_values:\n",
    "        ps, rs, f1s = [], [], []\n",
    "        for data in all_data:\n",
    "            gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "            gt_count = len(gt_ids)\n",
    "            results = results_fn(data)\n",
    "            top_n_ids = {r[\"id\"] for r in results[:n]}\n",
    "            hits = len(top_n_ids & gt_ids)\n",
    "            actual_n = len(top_n_ids)\n",
    "            p = hits / actual_n if actual_n > 0 else 0.0\n",
    "            r = hits / gt_count if gt_count > 0 else 0.0\n",
    "            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "            ps.append(p)\n",
    "            rs.append(r)\n",
    "            f1s.append(f1)\n",
    "        sweep_p.append(np.mean(ps))\n",
    "        sweep_r.append(np.mean(rs))\n",
    "        sweep_f1.append(np.mean(f1s))\n",
    "    return np.array(sweep_p), np.array(sweep_r), np.array(sweep_f1)\n",
    "\n",
    "\n",
    "# Distance baseline sweep\n",
    "dist_p, dist_r, dist_f1 = compute_sweep(\n",
    "    successful, n_values, lambda data: pool_and_deduplicate_by_distance(data.get(\"queries\", []))\n",
    ")\n",
    "\n",
    "# Per-strategy rerank sweeps\n",
    "strategy_sweeps = {}\n",
    "for key in strategy_names:\n",
    "    strategy_sweeps[key] = compute_sweep(\n",
    "        successful, n_values, lambda data, k=key: get_reranked_results(data, k)\n",
    "    )\n",
    "\n",
    "# Print table\n",
    "print(f\"Top-N Sweep: Distance vs Rerank (averaged over {n_cases} test cases)\\n\")\n",
    "header = f\"{'N':>4} {'Dist F1':>8}\"\n",
    "for key in strategy_names:\n",
    "    header += f\" {key[:16] + ' F1':>18} {'Delta':>7}\"\n",
    "print(header)\n",
    "print(\"-\" * (14 + 27 * len(strategy_names)))\n",
    "for n in [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]:\n",
    "    if n <= max_n:\n",
    "        i = n - 1\n",
    "        row = f\"{n:>4} {dist_f1[i]:>8.3f}\"\n",
    "        for key in strategy_names:\n",
    "            _, _, sf1 = strategy_sweeps[key]\n",
    "            delta = sf1[i] - dist_f1[i]\n",
    "            row += f\" {sf1[i]:>18.3f} {delta:>+7.3f}\"\n",
    "        print(row)\n",
    "\n",
    "# --- Plots ---\n",
    "colors_rerank = [\"#9b59b6\", \"#e67e22\", \"#2ecc71\", \"#e74c3c\"]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Left: F1 comparison — distance baseline + all strategies\n",
    "ax = axes[0]\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    dist_f1,\n",
    "    label=\"Distance (baseline)\",\n",
    "    color=\"#3498db\",\n",
    "    linewidth=2.5,\n",
    "    marker=\"o\",\n",
    "    markersize=5,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "for i, key in enumerate(strategy_names):\n",
    "    _, _, sf1 = strategy_sweeps[key]\n",
    "    ax.plot(\n",
    "        n_values,\n",
    "        sf1,\n",
    "        label=f\"Rerank: {key}\",\n",
    "        color=colors_rerank[i % len(colors_rerank)],\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        markersize=4,\n",
    "    )\n",
    "ax.axvline(\n",
    "    x=RERANK_TOP_N, color=\"gray\", linestyle=\":\", alpha=0.5, label=f\"Default N={RERANK_TOP_N}\"\n",
    ")\n",
    "ax.set_xlabel(\"Top-N (results kept)\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(f\"F1 vs Top-N: Distance vs Rerank (avg over {n_cases} test cases)\")\n",
    "ax.set_xticks(n_values)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: P/R/F1 detail for best strategy + distance baseline\n",
    "best_strategy = max(strategy_names, key=lambda k: strategy_sweeps[k][2][RERANK_TOP_N - 1])\n",
    "bp, br, bf1 = strategy_sweeps[best_strategy]\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    dist_f1,\n",
    "    label=\"F1 (distance)\",\n",
    "    color=\"#3498db\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    bp,\n",
    "    label=f\"Precision ({best_strategy})\",\n",
    "    color=\"#e74c3c\",\n",
    "    linewidth=1.5,\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    ")\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    br,\n",
    "    label=f\"Recall ({best_strategy})\",\n",
    "    color=\"#2ecc71\",\n",
    "    linewidth=1.5,\n",
    "    marker=\"s\",\n",
    "    markersize=3,\n",
    ")\n",
    "ax.plot(\n",
    "    n_values,\n",
    "    bf1,\n",
    "    label=f\"F1 ({best_strategy})\",\n",
    "    color=\"#9b59b6\",\n",
    "    linewidth=2.5,\n",
    "    marker=\"^\",\n",
    "    markersize=4,\n",
    ")\n",
    "ax.axvline(\n",
    "    x=RERANK_TOP_N, color=\"gray\", linestyle=\":\", alpha=0.5, label=f\"Default N={RERANK_TOP_N}\"\n",
    ")\n",
    "ax.set_xlabel(\"Top-N (results kept)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"P/R/F1 vs Top-N: {best_strategy} (with distance F1 baseline)\")\n",
    "ax.set_xticks(n_values)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"rerank_topn_sweep.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'rerank_topn_sweep.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C — Rerank Score Distribution (Per Strategy)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_strategies = len(strategy_names)\n",
    "fig, axes = plt.subplots(n_strategies, 2, figsize=(14, 5 * n_strategies), squeeze=False)\n",
    "\n",
    "for row, key in enumerate(strategy_names):\n",
    "    gt_scores, non_gt_scores = [], []\n",
    "    gt_distances, non_gt_distances = [], []\n",
    "\n",
    "    for data in successful:\n",
    "        reranked = get_reranked_results(data, key)\n",
    "        for r in reranked:\n",
    "            if r.get(\"is_ground_truth\"):\n",
    "                gt_scores.append(r[\"rerank_score\"])\n",
    "                gt_distances.append(r[\"distance\"])\n",
    "            else:\n",
    "                non_gt_scores.append(r[\"rerank_score\"])\n",
    "                non_gt_distances.append(r[\"distance\"])\n",
    "\n",
    "    gt_arr = np.array(gt_scores)\n",
    "    non_gt_arr = np.array(non_gt_scores)\n",
    "\n",
    "    # Left: Rerank score histogram (density-normalized)\n",
    "    ax = axes[row][0]\n",
    "    all_scores = np.concatenate([gt_arr, non_gt_arr])\n",
    "    bins = np.linspace(all_scores.min(), all_scores.max(), 30)\n",
    "    if len(gt_arr):\n",
    "        ax.hist(\n",
    "            gt_arr,\n",
    "            bins=bins,\n",
    "            alpha=0.6,\n",
    "            density=True,\n",
    "            label=f\"GT (n={len(gt_arr)})\",\n",
    "            color=\"#2ecc71\",\n",
    "            edgecolor=\"white\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        ax.axvline(\n",
    "            np.median(gt_arr),\n",
    "            color=\"#27ae60\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            label=f\"GT median: {np.median(gt_arr):.4f}\",\n",
    "        )\n",
    "    if len(non_gt_arr):\n",
    "        ax.hist(\n",
    "            non_gt_arr,\n",
    "            bins=bins,\n",
    "            alpha=0.6,\n",
    "            density=True,\n",
    "            label=f\"Non-GT (n={len(non_gt_arr)})\",\n",
    "            color=\"#e74c3c\",\n",
    "            edgecolor=\"white\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        ax.axvline(\n",
    "            np.median(non_gt_arr),\n",
    "            color=\"#c0392b\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            label=f\"Non-GT median: {np.median(non_gt_arr):.4f}\",\n",
    "        )\n",
    "    ax.set_xlabel(\"Rerank Score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(f\"Rerank Score Distribution: {key}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Right: Rerank score vs distance scatter\n",
    "    ax = axes[row][1]\n",
    "    if gt_scores:\n",
    "        ax.scatter(\n",
    "            gt_distances,\n",
    "            gt_scores,\n",
    "            alpha=0.7,\n",
    "            label=\"Ground Truth\",\n",
    "            color=\"#2ecc71\",\n",
    "            s=60,\n",
    "            zorder=3,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "    if non_gt_scores:\n",
    "        ax.scatter(\n",
    "            non_gt_distances,\n",
    "            non_gt_scores,\n",
    "            alpha=0.4,\n",
    "            label=\"Non-GT\",\n",
    "            color=\"#e74c3c\",\n",
    "            s=30,\n",
    "            zorder=2,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "    ax.set_xlabel(\"Vector Distance\")\n",
    "    ax.set_ylabel(\"Rerank Score\")\n",
    "    ax.set_title(f\"Rerank Score vs Distance: {key}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Statistics\n",
    "    if len(gt_arr):\n",
    "        print(\n",
    "            f\"[{key}] GT rerank scores:     min={gt_arr.min():.4f}, max={gt_arr.max():.4f}, mean={gt_arr.mean():.4f}, median={np.median(gt_arr):.4f}\"\n",
    "        )\n",
    "    if len(non_gt_arr):\n",
    "        print(\n",
    "            f\"[{key}] Non-GT rerank scores: min={non_gt_arr.min():.4f}, max={non_gt_arr.max():.4f}, mean={non_gt_arr.mean():.4f}, median={np.median(non_gt_arr):.4f}\"\n",
    "        )\n",
    "    if len(gt_arr) and len(non_gt_arr):\n",
    "        separation = gt_arr.mean() - non_gt_arr.mean()\n",
    "        print(\n",
    "            f\"[{key}] Mean separation: {separation:.4f} ({'good' if separation > 1.0 else 'moderate' if separation > 0.5 else 'weak'})\"\n",
    "        )\n",
    "    print()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"rerank_score_distribution.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'rerank_score_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D — Rerank Score Threshold Sweep (MAIN DELIVERABLE)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Build per-experiment reranked data for sweep\n",
    "exp_reranked_by_strategy = {}\n",
    "for key in strategy_names:\n",
    "    exps = []\n",
    "    for data in successful:\n",
    "        gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        reranked = get_reranked_results(data, key)\n",
    "        exps.append(\n",
    "            {\n",
    "                \"tc_id\": data[\"test_case_id\"],\n",
    "                \"gt_ids\": gt_ids,\n",
    "                \"reranked\": reranked,\n",
    "            }\n",
    "        )\n",
    "    exp_reranked_by_strategy[key] = exps\n",
    "\n",
    "# Sweep thresholds per strategy\n",
    "all_max_scores = []\n",
    "for key in strategy_names:\n",
    "    for er in exp_reranked_by_strategy[key]:\n",
    "        if er[\"reranked\"]:\n",
    "            all_max_scores.append(max(r[\"rerank_score\"] for r in er[\"reranked\"]))\n",
    "global_max = max(all_max_scores) if all_max_scores else 1.0\n",
    "sweep_thresholds = np.arange(0.0, global_max + 0.01, 0.005)\n",
    "\n",
    "strategy_sweep_results = {}\n",
    "for key in strategy_names:\n",
    "    sweep_p, sweep_r, sweep_f1, sweep_mrr = [], [], [], []\n",
    "    for t in sweep_thresholds:\n",
    "        ps, rs, f1s, rrs = [], [], [], []\n",
    "        for er in exp_reranked_by_strategy[key]:\n",
    "            accepted = {r[\"id\"] for r in er[\"reranked\"] if r[\"rerank_score\"] >= t}\n",
    "            gt_hit = len(accepted & er[\"gt_ids\"])\n",
    "            n_acc = len(accepted)\n",
    "            gt_count = len(er[\"gt_ids\"])\n",
    "            p = gt_hit / n_acc if n_acc > 0 else 0.0\n",
    "            r = gt_hit / gt_count if gt_count > 0 else 0.0\n",
    "            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "            ps.append(p)\n",
    "            rs.append(r)\n",
    "            f1s.append(f1)\n",
    "            # MRR\n",
    "            accepted_sorted = [r for r in er[\"reranked\"] if r[\"rerank_score\"] >= t]\n",
    "            rr = 0.0\n",
    "            for rank, res in enumerate(accepted_sorted, 1):\n",
    "                if res[\"id\"] in er[\"gt_ids\"]:\n",
    "                    rr = 1.0 / rank\n",
    "                    break\n",
    "            rrs.append(rr)\n",
    "        sweep_p.append(np.mean(ps))\n",
    "        sweep_r.append(np.mean(rs))\n",
    "        sweep_f1.append(np.mean(f1s))\n",
    "        sweep_mrr.append(np.mean(rrs))\n",
    "\n",
    "    sweep_p = np.array(sweep_p)\n",
    "    sweep_r = np.array(sweep_r)\n",
    "    sweep_f1 = np.array(sweep_f1)\n",
    "    sweep_mrr = np.array(sweep_mrr)\n",
    "    best_idx = np.argmax(sweep_f1)\n",
    "    strategy_sweep_results[key] = {\n",
    "        \"p\": sweep_p,\n",
    "        \"r\": sweep_r,\n",
    "        \"f1\": sweep_f1,\n",
    "        \"mrr\": sweep_mrr,\n",
    "        \"best_idx\": best_idx,\n",
    "        \"best_threshold\": sweep_thresholds[best_idx],\n",
    "    }\n",
    "\n",
    "# --- Print optimal thresholds ---\n",
    "print(\"OPTIMAL THRESHOLDS PER STRATEGY\")\n",
    "best_overall_strategy = None\n",
    "best_overall_f1 = -1\n",
    "for key in strategy_names:\n",
    "    sr = strategy_sweep_results[key]\n",
    "    bi = sr[\"best_idx\"]\n",
    "    print(\n",
    "        f\"  {key + ':':30} threshold={sr['best_threshold']:.4f}  F1={sr['f1'][bi]:.3f}  P={sr['p'][bi]:.3f}  R={sr['r'][bi]:.3f}  MRR={sr['mrr'][bi]:.3f}\"\n",
    "    )\n",
    "    if sr[\"f1\"][bi] > best_overall_f1:\n",
    "        best_overall_f1 = sr[\"f1\"][bi]\n",
    "        best_overall_strategy = key\n",
    "\n",
    "print(f\"\\nBest strategy: {best_overall_strategy}\")\n",
    "\n",
    "# --- Plots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Left: P/R/F1/MRR vs threshold for best strategy\n",
    "sr = strategy_sweep_results[best_overall_strategy]\n",
    "ax = axes[0]\n",
    "ax.plot(sweep_thresholds, sr[\"p\"], label=\"Precision\", color=\"#3498db\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, sr[\"r\"], label=\"Recall\", color=\"#2ecc71\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, sr[\"f1\"], label=\"F1\", color=\"#9b59b6\", linewidth=2.5)\n",
    "ax.plot(sweep_thresholds, sr[\"mrr\"], label=\"MRR\", color=\"#e67e22\", linewidth=1.5, alpha=0.7)\n",
    "ax.axvline(\n",
    "    sr[\"best_threshold\"],\n",
    "    color=\"#e74c3c\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Optimal @ {sr['best_threshold']:.4f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Rerank Score Threshold (accept >= threshold)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"P/R/F1/MRR vs Threshold: {best_overall_strategy}\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: F1 vs threshold for ALL strategies\n",
    "ax = axes[1]\n",
    "colors_strat = [\"#9b59b6\", \"#e67e22\", \"#2ecc71\", \"#e74c3c\"]\n",
    "for i, key in enumerate(strategy_names):\n",
    "    sr_i = strategy_sweep_results[key]\n",
    "    ax.plot(\n",
    "        sweep_thresholds,\n",
    "        sr_i[\"f1\"],\n",
    "        label=key,\n",
    "        color=colors_strat[i % len(colors_strat)],\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.axvline(\n",
    "        sr_i[\"best_threshold\"],\n",
    "        color=colors_strat[i % len(colors_strat)],\n",
    "        linestyle=\":\",\n",
    "        alpha=0.5,\n",
    "        linewidth=1,\n",
    "    )\n",
    "ax.set_xlabel(\"Rerank Score Threshold\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"F1 vs Threshold: All Strategies\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"rerank_threshold_sweep.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'rerank_threshold_sweep.png'}\")\n",
    "\n",
    "# --- Threshold table for best strategy ---\n",
    "sr = strategy_sweep_results[best_overall_strategy]\n",
    "bt = sr[\"best_threshold\"]\n",
    "print(\n",
    "    f\"\\nThreshold table: {best_overall_strategy} (macro-averaged over {len(successful)} test cases)\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "table_thresholds = sorted(\n",
    "    set(\n",
    "        [\n",
    "            0.001,\n",
    "            0.005,\n",
    "            0.01,\n",
    "            0.02,\n",
    "            0.03,\n",
    "            0.05,\n",
    "            0.08,\n",
    "            0.10,\n",
    "            0.15,\n",
    "            0.20,\n",
    "            0.30,\n",
    "            0.50,\n",
    "            round(bt, 4),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "exps = exp_reranked_by_strategy[best_overall_strategy]\n",
    "for t in table_thresholds:\n",
    "    if t > global_max + 0.01:\n",
    "        continue\n",
    "    idx = np.argmin(np.abs(sweep_thresholds - t))\n",
    "    avg_accepted = np.mean(\n",
    "        [len([r for r in er[\"reranked\"] if r[\"rerank_score\"] >= t]) for er in exps]\n",
    "    )\n",
    "    avg_gt_kept = np.mean(\n",
    "        [\n",
    "            len({r[\"id\"] for r in er[\"reranked\"] if r[\"rerank_score\"] >= t} & er[\"gt_ids\"])\n",
    "            for er in exps\n",
    "        ]\n",
    "    )\n",
    "    marker = \" <-- optimal\" if abs(t - bt) < 0.003 else \"\"\n",
    "    print(\n",
    "        f\"{t:>10.4f} {sr['p'][idx]:>10.3f} {sr['r'][idx]:>8.3f} {sr['f1'][idx]:>8.3f} {sr['mrr'][idx]:>8.3f} {avg_accepted:>13.1f} {avg_gt_kept:>12.1f}{marker}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell E — Per-Test-Case Impact at Optimal Threshold\n",
    "import numpy as np\n",
    "\n",
    "N = RERANK_TOP_N\n",
    "key = best_overall_strategy\n",
    "bt = strategy_sweep_results[key][\"best_threshold\"]\n",
    "exps = exp_reranked_by_strategy[key]\n",
    "\n",
    "print(\"Per-test-case comparison at best strategy's optimal threshold\")\n",
    "print(f\"  Strategy: {key}\")\n",
    "print(f\"  Threshold: score >= {bt:.4f}\")\n",
    "print(f\"  Distance baseline: top-{N} by distance\")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    f\"{'Test Case':<20} {'Dist F1':>8} {'Thr F1':>8} {'Delta':>8} {'Dist P':>7} {'Thr P':>7} {'Dist R':>7} {'Thr R':>7} {'D Acc':>6} {'T Acc':>6} {'GT':>4}\"\n",
    ")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "dist_f1s, thr_f1s = [], []\n",
    "\n",
    "for data, er in zip(successful, exps):\n",
    "    tc_id = data[\"test_case_id\"]\n",
    "    gt_ids = er[\"gt_ids\"]\n",
    "    gt_count = len(gt_ids)\n",
    "\n",
    "    # Distance baseline: top-N\n",
    "    pooled = pool_and_deduplicate_by_distance(data.get(\"queries\", []))\n",
    "    dist_top = pooled[:N]\n",
    "    dist_ids = {r[\"id\"] for r in dist_top}\n",
    "    d_h = len(dist_ids & gt_ids)\n",
    "    d_n = len(dist_ids)\n",
    "    d_p = d_h / d_n if d_n else 0\n",
    "    d_r = d_h / gt_count if gt_count else 0\n",
    "    d_f1 = 2 * d_p * d_r / (d_p + d_r) if (d_p + d_r) else 0\n",
    "\n",
    "    # Threshold-based\n",
    "    thr_results = [r for r in er[\"reranked\"] if r[\"rerank_score\"] >= bt]\n",
    "    thr_ids = {r[\"id\"] for r in thr_results}\n",
    "    t_h = len(thr_ids & gt_ids)\n",
    "    t_n = len(thr_ids)\n",
    "    t_p = t_h / t_n if t_n else 0\n",
    "    t_r = t_h / gt_count if gt_count else 0\n",
    "    t_f1 = 2 * t_p * t_r / (t_p + t_r) if (t_p + t_r) else 0\n",
    "\n",
    "    delta = t_f1 - d_f1\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "    print(\n",
    "        f\"{tc_id:<20} {d_f1:>8.3f} {t_f1:>8.3f} {delta:>+7.3f}{marker} {d_p:>7.1%} {t_p:>7.1%} {d_r:>7.1%} {t_r:>7.1%} {d_n:>6} {t_n:>6} {gt_count:>4}\"\n",
    "    )\n",
    "\n",
    "    # Show missed GT with their rerank scores\n",
    "    missed = gt_ids - thr_ids\n",
    "    if missed:\n",
    "        for mid in sorted(missed):\n",
    "            score = next((r[\"rerank_score\"] for r in er[\"reranked\"] if r[\"id\"] == mid), None)\n",
    "            if score is not None:\n",
    "                print(f\"  {'':20} Missed: {mid}  score={score:.4f} (below threshold {bt:.4f})\")\n",
    "            else:\n",
    "                print(f\"  {'':20} Missed: {mid}  NOT IN CANDIDATE POOL (query miss)\")\n",
    "\n",
    "    dist_f1s.append(d_f1)\n",
    "    thr_f1s.append(t_f1)\n",
    "\n",
    "avg_d = np.mean(dist_f1s)\n",
    "avg_t = np.mean(thr_f1s)\n",
    "delta = avg_t - avg_d\n",
    "print(\"-\" * 105)\n",
    "print(\n",
    "    f\"{'AVERAGE':<20} {avg_d:>8.3f} {avg_t:>8.3f} {delta:>+7.3f}{'+' if delta > 0.001 else '-' if delta < -0.001 else '='}\"\n",
    ")\n",
    "\n",
    "improved = sum(1 for d, t in zip(dist_f1s, thr_f1s) if t > d + 0.001)\n",
    "same = sum(1 for d, t in zip(dist_f1s, thr_f1s) if abs(t - d) <= 0.001)\n",
    "worse = sum(1 for d, t in zip(dist_f1s, thr_f1s) if t < d - 0.001)\n",
    "print(\n",
    "    f\"\\nThreshold helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
