{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Phase 1 Reranking Comparison\n",
    "\n",
    "Applies cross-encoder reranking on top of **existing Phase 1 experiment results**.\n",
    "Reuses the exact same queries, vector search candidates, and ground truth from the Phase 1 run.\n",
    "\n",
    "This notebook does **not** generate new queries or run new searches — it loads the saved\n",
    "Phase 1 results and only adds the reranking step to measure its isolated impact on retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/mayk/Projects/private/crm-memory-retrieval-research\n",
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from memory_retrieval.search.reranker import Reranker\n",
    "from memory_retrieval.memories.schema import FIELD_SITUATION, FIELD_DISTANCE, FIELD_RERANK_SCORE\n",
    "from memory_retrieval.experiments.metrics import compute_metrics\n",
    "from memory_retrieval.infra.io import load_json, save_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    create_run, get_latest_run, get_run, list_runs, update_run_status,\n",
    "    PHASE1, PHASE2,\n",
    ")\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT == PROJECT_ROOT.parent:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2 — Configuration\n\n# Reranking configuration\nRERANK_TOP_N = 4          # Final results after reranking\nDISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics (must match Phase 1)\n\n# Phase 1 run selection (source of results, DB, and test cases)\n# To see available runs: print(list_runs(PHASE1))\n# To select specific run: PHASE1_RUN = get_run(PHASE1, \"run_20260208_143022\")\nPHASE1_RUN = get_latest_run(PHASE1)\n\n# Derived paths from Phase 1\nPHASE1_RESULTS_DIR = PHASE1_RUN / \"results\"\nphase1_result_files = sorted(PHASE1_RESULTS_DIR.glob(\"*.json\"))\n\nprint(\"Configuration:\")\nprint(f\"  Phase 1 run: {PHASE1_RUN.name}\")\nprint(f\"  Phase 1 results dir: {PHASE1_RESULTS_DIR}\")\nprint(f\"  Phase 1 result files: {len(phase1_result_files)}\")\nprint(f\"  Rerank top-n: {RERANK_TOP_N}\")\nprint(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\n\nif not phase1_result_files:\n    print(\"\\nERROR: No Phase 1 result files found. Run phase1.ipynb first.\")"
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Load Phase 1 Results & Initialize Reranker\n",
    "\n",
    "Load existing Phase 1 experiment results (queries + vector search candidates).\n",
    "Initialize the cross-encoder reranker (`bge-reranker-v2-m3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-reranker",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4 — Load Phase 1 Results & Initialize Reranker\n\n# Load all Phase 1 experiment results\n# When multiple results exist per test case, keep the latest experiment\nphase1_by_test_case: dict[str, dict] = {}\nfor result_file in phase1_result_files:\n    data = load_json(str(result_file))\n    test_case_id = data.get(\"test_case_id\", \"?\")\n    if test_case_id not in phase1_by_test_case or data.get(\"experiment_id\", \"\") > phase1_by_test_case[test_case_id].get(\"experiment_id\", \"\"):\n        phase1_by_test_case[test_case_id] = data\n\nprint(f\"Loaded {len(phase1_by_test_case)} unique test case results from Phase 1\\n\")\nfor test_case_id, data in sorted(phase1_by_test_case.items()):\n    ground_truth_count = data.get(\"ground_truth\", {}).get(\"count\", 0)\n    metrics = data.get(\"metrics\", {})\n    num_queries = len(data.get(\"queries\", []))\n    print(f\"  {test_case_id}: {num_queries} queries, {ground_truth_count} GT memories, F1={metrics.get('f1', 0):.3f}\")\n\n# Initialize reranker\nreranker = Reranker()\n\n# Quick test to load the model\nprint(\"\\nLoading reranker model (first use triggers download)...\")\n_ = reranker.score_pairs(\"test\", [\"test document\"])\nprint(\"Reranker ready.\")"
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Apply Reranking to Phase 1 Results\n",
    "\n",
    "For each Phase 1 test case result:\n",
    "1. Pool and deduplicate vector search candidates from all queries\n",
    "2. Compute **top-N by vector distance** baseline (apples-to-apples with reranking)\n",
    "3. Rerank candidates with cross-encoder, take top-N\n",
    "4. Compare both at the same N — the only difference is ranking method\n",
    "\n",
    "No new LLM calls or vector searches — everything is reused from Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6 — Apply Reranking to Phase 1 Results\n\ndef pool_and_deduplicate(query_results: list[dict]) -> list[dict]:\n    \"\"\"Pool results from all queries and deduplicate by memory ID (keep best distance).\"\"\"\n    best_by_memory_id: dict[str, dict] = {}\n    for query_result in query_results:\n        for result in query_result.get(\"results\", []):\n            memory_id = result[\"id\"]\n            distance = result.get(\"distance\", float(\"inf\"))\n            if memory_id not in best_by_memory_id or distance < best_by_memory_id[memory_id].get(\"distance\", float(\"inf\")):\n                best_by_memory_id[memory_id] = result\n    return sorted(best_by_memory_id.values(), key=lambda x: x.get(\"distance\", 0))\n\n\n# Create Phase 2 run for results\nrun_id, PHASE2_RUN = create_run(\n    PHASE2,\n    description=f\"Reranking on phase1 results ({PHASE1_RUN.name}, top-{RERANK_TOP_N})\",\n)\nRESULTS_DIR = str(PHASE2_RUN / \"results\")\n\nupdate_run_status(PHASE2_RUN, \"config\", {\n    \"phase1_run_id\": PHASE1_RUN.name,\n    \"reranker_model\": reranker.model_name,\n    \"rerank_top_n\": RERANK_TOP_N,\n    \"distance_threshold\": DISTANCE_THRESHOLD,\n    \"mode\": \"reuse_phase1_results\",\n})\n\nprint(f\"Phase 2 run: {run_id}\")\nprint(f\"Results dir: {RESULTS_DIR}\\n\")\n\nall_results: list[dict] = []\n\n# Iterate over all results from phase1\nfor i, (test_case_id, phase1_data) in enumerate(sorted(phase1_by_test_case.items()), 1):\n    print(f\"[{i}/{len(phase1_by_test_case)}] {test_case_id}\")\n\n    ground_truth_ids = set(phase1_data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n    query_results = phase1_data.get(\"queries\", [])\n\n    # Pool and deduplicate across all queries (sorted by distance, best first)\n    pooled = pool_and_deduplicate(query_results)\n    print(f\"  Deduplicated memories count: {len(pooled)}\")\n\n    # --- Baseline: top-N by vector distance (same N as reranking) ---\n    top_n_by_distance = pooled[:RERANK_TOP_N]\n    distance_top_n_ids = {result[\"id\"] for result in top_n_by_distance}\n    distance_top_n_metrics = compute_metrics(distance_top_n_ids, ground_truth_ids)\n\n    # --- Per-query reranking: rerank each query's results independently ---\n    all_reranked_per_query = []\n    # Iterate over all queries\n    for query_result in query_results:\n        candidates = [\n            {\n                \"id\": result[\"id\"],\n                FIELD_SITUATION: result.get(\"situation\", result.get(FIELD_SITUATION, \"\")),\n                FIELD_DISTANCE: result.get(\"distance\", 0),\n                \"is_ground_truth\": result.get(\"is_ground_truth\", result[\"id\"] in ground_truth_ids),\n            }\n            # Iterate over all results for this query\n            for result in query_result.get(\"results\", [])\n        ]\n        # Simple reranking, just using \"situation\" description - pass query string of given query result with all its results array\n        # mapped to candidates\n        reranked = reranker.rerank(query_result[\"query\"], candidates, top_n=None)\n        all_reranked_per_query.extend(reranked)\n\n    # Deduplicate by best rerank score - drop information about original query string\n    best_by_memory_id: dict[str, dict] = {}\n    for result in all_reranked_per_query:\n        memory_id = result[\"id\"]\n        if memory_id not in best_by_memory_id or result[FIELD_RERANK_SCORE] > best_by_memory_id[memory_id][FIELD_RERANK_SCORE]:\n            best_by_memory_id[memory_id] = result\n    all_reranked = sorted(best_by_memory_id.values(), key=lambda x: x[FIELD_RERANK_SCORE], reverse=True)\n\n    top_reranked = all_reranked[:RERANK_TOP_N]\n    reranked_ids = {result[\"id\"] for result in top_reranked}\n    rerank_top_n_metrics = compute_metrics(reranked_ids, ground_truth_ids)\n\n    f1_delta = rerank_top_n_metrics[\"f1\"] - distance_top_n_metrics[\"f1\"]\n    marker = \"+\" if f1_delta > 0 else \"\"\n    print(f\"  Top-{RERANK_TOP_N} by distance F1={distance_top_n_metrics['f1']:.3f} | Top-{RERANK_TOP_N} by rerank F1={rerank_top_n_metrics['f1']:.3f} ({marker}{f1_delta:.3f})\")\n\n    # Build result\n    result = {\n        \"test_case_id\": test_case_id,\n        \"source_file\": phase1_data.get(\"source_file\", \"unknown\"),\n        \"phase1_experiment_id\": phase1_data.get(\"experiment_id\", \"unknown\"),\n        \"model\": phase1_data.get(\"model\", \"unknown\"),\n        \"prompt_version\": phase1_data.get(\"prompt_version\", \"unknown\"),\n        \"reranker_model\": reranker.model_name,\n        \"rerank_top_n\": RERANK_TOP_N,\n        \"rerank_queries\": [query_result[\"query\"] for query_result in query_results],\n        \"distance_threshold\": DISTANCE_THRESHOLD,\n        \"ground_truth\": phase1_data.get(\"ground_truth\", {}),\n        \"queries\": query_results,\n        \"pooled_candidate_count\": len(pooled),\n        \"distance_top_n_metrics\": {\n            **distance_top_n_metrics,\n            \"n\": RERANK_TOP_N,\n            \"ground_truth_retrieved\": len(distance_top_n_ids & ground_truth_ids),\n        },\n        \"rerank_top_n_metrics\": {\n            **rerank_top_n_metrics,\n            \"n\": RERANK_TOP_N,\n            \"ground_truth_retrieved\": len(reranked_ids & ground_truth_ids),\n        },\n        \"reranked_results\": [\n            {\n                \"id\": result[\"id\"],\n                \"rerank_score\": result[FIELD_RERANK_SCORE],\n                \"distance\": result.get(FIELD_DISTANCE, 0),\n                \"situation\": result.get(FIELD_SITUATION, \"\"),\n                \"is_ground_truth\": result[\"id\"] in ground_truth_ids,\n            }\n            for result in all_reranked  # Store ALL reranked (for sweep analysis)\n        ],\n        \"distance_top_n_results\": [\n            {\n                \"id\": result[\"id\"],\n                \"distance\": result.get(\"distance\", 0),\n                \"situation\": result.get(\"situation\", result.get(FIELD_SITUATION, \"\")),\n                \"is_ground_truth\": result.get(\"is_ground_truth\", result[\"id\"] in ground_truth_ids),\n            }\n            for result in top_n_by_distance\n        ],\n        \"retrieved_ground_truth_ids\": sorted(list(reranked_ids & ground_truth_ids)),\n        \"missed_ground_truth_ids\": sorted(list(ground_truth_ids - reranked_ids)),\n    }\n\n    # Save\n    Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n    save_json(result, Path(RESULTS_DIR) / f\"rerank_{test_case_id}.json\")\n    all_results.append(result)\n\n# Summary\nsuccessful = [result for result in all_results if \"rerank_top_n_metrics\" in result]\nif successful:\n    avg_distance_metrics = {metric: sum(result[\"distance_top_n_metrics\"][metric] for result in successful) / len(successful) for metric in [\"recall\", \"precision\", \"f1\"]}\n    avg_rerank_metrics = {metric: sum(result[\"rerank_top_n_metrics\"][metric] for result in successful) / len(successful) for metric in [\"recall\", \"precision\", \"f1\"]}\n\n    print(f\"\\n{'='*55}\")\n    print(f\"SUMMARY ({len(successful)} test cases, top-{RERANK_TOP_N})\")\n    print(f\"{'='*55}\")\n    print(f\"{'Metric':<12} {'Top-N Distance':>15} {'Top-N Rerank':>13} {'Delta':>10}\")\n    print(\"-\" * 55)\n    for metric in [\"recall\", \"precision\", \"f1\"]:\n        delta = avg_rerank_metrics[metric] - avg_distance_metrics[metric]\n        print(f\"{metric:<12} {avg_distance_metrics[metric]:>15.3f} {avg_rerank_metrics[metric]:>13.3f} {delta:>+10.3f}\")\n\n# Update run status\nupdate_run_status(PHASE2_RUN, \"experiment\", {\n    \"count\": len(successful),\n    \"avg_f1_distance_top_n\": round(avg_distance_metrics[\"f1\"], 4) if successful else 0,\n    \"avg_f1_rerank_top_n\": round(avg_rerank_metrics[\"f1\"], 4) if successful else 0,\n    \"rerank_top_n\": RERANK_TOP_N,\n})"
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Results Summary Table\n",
    "\n",
    "Per-test-case comparison: **top-N by distance** vs **top-N by rerank score** (same N, same candidates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8 — Results Summary Table\n\nsuccessful = [result for result in all_results if \"rerank_top_n_metrics\" in result]\n\nprint(f\"Top-{RERANK_TOP_N} comparison: vector distance vs cross-encoder reranking\\n\")\nprint(f\"{'Test Case':<25} {'Dist F1':>8} {'Rank F1':>8} {'Delta':>8} {'Dist P':>7} {'Rank P':>7} {'Dist R':>7} {'Rank R':>7}\")\nprint(\"-\" * 85)\n\nfor result in successful:\n    name = result.get(\"test_case_id\", \"?\")[:25]\n    distance_metrics = result[\"distance_top_n_metrics\"]\n    rerank_metrics = result[\"rerank_top_n_metrics\"]\n    delta = rerank_metrics[\"f1\"] - distance_metrics[\"f1\"]\n    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n    print(f\"{name:<25} {distance_metrics['f1']:>8.3f} {rerank_metrics['f1']:>8.3f} {delta:>+7.3f}{marker} {distance_metrics['precision']:>7.3f} {rerank_metrics['precision']:>7.3f} {distance_metrics['recall']:>7.3f} {rerank_metrics['recall']:>7.3f}\")\n\nif successful:\n    avg_distance_metrics = {metric: sum(result[\"distance_top_n_metrics\"][metric] for result in successful) / len(successful) for metric in [\"recall\", \"precision\", \"f1\"]}\n    avg_rerank_metrics = {metric: sum(result[\"rerank_top_n_metrics\"][metric] for result in successful) / len(successful) for metric in [\"recall\", \"precision\", \"f1\"]}\n    delta = avg_rerank_metrics[\"f1\"] - avg_distance_metrics[\"f1\"]\n    marker = \"+\" if delta > 0 else \"-\" if delta < 0 else \"=\"\n    print(\"-\" * 85)\n    print(f\"{'AVERAGE':<25} {avg_distance_metrics['f1']:>8.3f} {avg_rerank_metrics['f1']:>8.3f} {delta:>+7.3f}{marker} {avg_distance_metrics['precision']:>7.3f} {avg_rerank_metrics['precision']:>7.3f} {avg_distance_metrics['recall']:>7.3f} {avg_rerank_metrics['recall']:>7.3f}\")\n\n    improved = sum(1 for result in successful if result[\"rerank_top_n_metrics\"][\"f1\"] > result[\"distance_top_n_metrics\"][\"f1\"] + 0.001)\n    same = sum(1 for result in successful if abs(result[\"rerank_top_n_metrics\"][\"f1\"] - result[\"distance_top_n_metrics\"][\"f1\"]) <= 0.001)\n    worse = sum(1 for result in successful if result[\"rerank_top_n_metrics\"][\"f1\"] < result[\"distance_top_n_metrics\"][\"f1\"] - 0.001)\n    print(f\"\\nReranking helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 — Top-N Sweep: Distance vs Reranking\n",
    "\n",
    "Compare top-N by vector distance vs top-N by rerank score across different N values.\n",
    "Shows whether reranking improves ranking quality at every cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topn-sweep",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10 — Top-N Sweep: Distance vs Reranking\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nFIGURES_DIR = Path(\"notebooks/phase2/figures\")\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\nall_data = all_results\nnum_test_cases = len(all_data)\n\nif not all_data:\n    print(\"No results found. Run Step 2 first.\")\nelse:\n    max_n = 20\n    n_values = list(range(1, max_n + 1))\n\n    # Sweep for both distance-based and rerank-based top-N\n    distance_f1_scores, rerank_f1_scores = [], []\n    distance_precisions, rerank_precisions = [], []\n    distance_recalls, rerank_recalls = [], []\n\n    for top_n in n_values:\n        distance_f1_list, distance_precision_list, distance_recall_list = [], [], []\n        rerank_f1_list, rerank_precision_list, rerank_recall_list = [], [], []\n\n        for data in all_data:\n            ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n            ground_truth_count = len(ground_truth_ids)\n\n            # Distance-based top-N (pooled is sorted by distance in queries)\n            pooled = pool_and_deduplicate(data.get(\"queries\", []))\n            distance_top_n_ids = {result[\"id\"] for result in pooled[:top_n]}\n            distance_hits = len(distance_top_n_ids & ground_truth_ids)\n            distance_count = len(distance_top_n_ids)\n\n            # Rerank-based top-N (reranked_results sorted by rerank score)\n            reranked = data.get(\"reranked_results\", [])\n            rerank_top_n_ids = {result[\"id\"] for result in reranked[:top_n]}\n            rerank_hits = len(rerank_top_n_ids & ground_truth_ids)\n            rerank_count = len(rerank_top_n_ids)\n\n            # Distance metrics\n            precision = distance_hits / distance_count if distance_count > 0 else 0.0\n            recall = distance_hits / ground_truth_count if ground_truth_count > 0 else 0.0\n            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n            distance_precision_list.append(precision)\n            distance_recall_list.append(recall)\n            distance_f1_list.append(f1_score)\n\n            # Rerank metrics\n            precision = rerank_hits / rerank_count if rerank_count > 0 else 0.0\n            recall = rerank_hits / ground_truth_count if ground_truth_count > 0 else 0.0\n            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n            rerank_precision_list.append(precision)\n            rerank_recall_list.append(recall)\n            rerank_f1_list.append(f1_score)\n\n        distance_f1_scores.append(np.mean(distance_f1_list))\n        distance_precisions.append(np.mean(distance_precision_list))\n        distance_recalls.append(np.mean(distance_recall_list))\n        rerank_f1_scores.append(np.mean(rerank_f1_list))\n        rerank_precisions.append(np.mean(rerank_precision_list))\n        rerank_recalls.append(np.mean(rerank_recall_list))\n\n    # Print table\n    print(f\"Top-N Sweep: Distance vs Reranking (averaged over {num_test_cases} test cases)\\n\")\n    print(f\"{'N':>4} {'Dist P':>8} {'Rank P':>8} {'Dist R':>8} {'Rank R':>8} {'Dist F1':>8} {'Rank F1':>8} {'F1 Delta':>9}\")\n    print(\"-\" * 70)\n    for top_n in [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]:\n        if top_n <= max_n:\n            index = top_n - 1\n            delta = rerank_f1_scores[index] - distance_f1_scores[index]\n            print(f\"{top_n:>4} {distance_precisions[index]:>8.3f} {rerank_precisions[index]:>8.3f} {distance_recalls[index]:>8.3f} {rerank_recalls[index]:>8.3f} {distance_f1_scores[index]:>8.3f} {rerank_f1_scores[index]:>8.3f} {delta:>+9.3f}\")\n\n    # Plot: F1 comparison\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    ax = axes[0]\n    ax.plot(n_values, distance_f1_scores, label=\"F1 (distance)\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=4)\n    ax.plot(n_values, rerank_f1_scores, label=\"F1 (reranked)\", color=\"#e74c3c\", linewidth=2, marker=\"^\", markersize=4)\n    ax.axvline(x=RERANK_TOP_N, color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"Default N={RERANK_TOP_N}\")\n    ax.set_xlabel(\"Top-N\")\n    ax.set_ylabel(\"F1 Score\")\n    ax.set_title(f\"F1: Distance vs Reranking (avg over {num_test_cases} test cases)\")\n    ax.set_xticks(n_values)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Plot: F1 delta\n    ax = axes[1]\n    f1_deltas = [rerank_f1_scores[i] - distance_f1_scores[i] for i in range(max_n)]\n    colors = [\"#2ecc71\" if delta > 0 else \"#e74c3c\" for delta in f1_deltas]\n    ax.bar(n_values, f1_deltas, color=colors, alpha=0.7)\n    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n    ax.set_xlabel(\"Top-N\")\n    ax.set_ylabel(\"F1 Delta (rerank - distance)\")\n    ax.set_title(\"Reranking F1 Improvement by N\")\n    ax.set_xticks(n_values)\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    fig.savefig(FIGURES_DIR / \"rerank_topn_sweep.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved: {FIGURES_DIR / 'rerank_topn_sweep.png'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 — Rerank Score Threshold Analysis\n",
    "\n",
    "Mirrors the Phase 1 distance threshold analysis, but uses **rerank scores** as the filtering signal.\n",
    "Instead of a fixed top-N cutoff, we sweep rerank score thresholds to find the optimal cutoff.\n",
    "\n",
    "All metrics are **macro-averaged**: computed per test case, then averaged.\n",
    "Within each test case, candidates are deduplicated by memory ID (best distance), then reranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-distribution",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12 — Rerank Score Distribution & Threshold Sweep\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nFIGURES_DIR = Path(\"notebooks/phase2/figures\")\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\n# Collect per-observation rerank scores (one entry per test_case x memory pair)\nground_truth_scores_all = []       # (score, test_case_id, memory_id)\nnon_ground_truth_scores_all = []   # (score, test_case_id, memory_id)\n\n# Build per-experiment deduped rerank data (mirrors phase1 threshold analysis structure)\nexperiments_reranked = []\nfor data in all_data:\n    test_case_id = data[\"test_case_id\"]\n    ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n    # reranked_results contains ALL candidates sorted by rerank score\n    reranked = data.get(\"reranked_results\", [])\n    scores_by_id = {result[\"id\"]: result for result in reranked}\n\n    experiments_reranked.append({\n        \"test_case_id\": test_case_id,\n        \"ground_truth_ids\": ground_truth_ids,\n        \"scores_by_id\": scores_by_id,\n        \"reranked\": reranked,\n    })\n\n    for result in reranked:\n        entry = (result[\"rerank_score\"], test_case_id, result[\"id\"])\n        if result.get(\"is_ground_truth\"):\n            ground_truth_scores_all.append(entry)\n        else:\n            non_ground_truth_scores_all.append(entry)\n\nground_truth_scores = np.array([score for score, _, _ in ground_truth_scores_all])\nnon_ground_truth_scores = np.array([score for score, _, _ in non_ground_truth_scores_all])\n\nprint(f\"Experiments: {len(experiments_reranked)} test cases\")\nprint(f\"GT observations:     {len(ground_truth_scores)} (unique — each memory is GT in exactly 1 test case)\")\nprint(f\"Non-GT observations: {len(non_ground_truth_scores)}\")\nprint()\nprint(f\"GT rerank score range:     [{ground_truth_scores.min():.4f}, {ground_truth_scores.max():.4f}]\")\nprint(f\"Non-GT rerank score range: [{non_ground_truth_scores.min():.4f}, {non_ground_truth_scores.max():.4f}]\")\nprint(f\"GT mean: {ground_truth_scores.mean():.4f}, median: {np.median(ground_truth_scores):.4f}\")\nprint(f\"Non-GT mean: {non_ground_truth_scores.mean():.4f}, median: {np.median(non_ground_truth_scores):.4f}\")\nprint(f\"Mean separation: {ground_truth_scores.mean() - non_ground_truth_scores.mean():.4f}\")\n\n# --- Figure 1: Score distribution histogram ---\nfig1, ax1 = plt.subplots(figsize=(10, 5))\nall_scores_combined = np.concatenate([ground_truth_scores, non_ground_truth_scores])\nbins = np.linspace(all_scores_combined.min(), all_scores_combined.max(), 40)\nax1.hist(ground_truth_scores, bins=bins, alpha=0.6, density=True,\n         label=f\"GT (n={len(ground_truth_scores)})\", color=\"#2ecc71\", edgecolor=\"white\", linewidth=0.5)\nax1.hist(non_ground_truth_scores, bins=bins, alpha=0.6, density=True,\n         label=f\"Non-GT (n={len(non_ground_truth_scores)})\", color=\"#e74c3c\", edgecolor=\"white\", linewidth=0.5)\nax1.axvline(np.median(ground_truth_scores), color=\"#27ae60\", linestyle=\"--\", linewidth=1.5,\n            label=f\"GT median: {np.median(ground_truth_scores):.4f}\")\nax1.axvline(np.median(non_ground_truth_scores), color=\"#c0392b\", linestyle=\"--\", linewidth=1.5,\n            label=f\"Non-GT median: {np.median(non_ground_truth_scores):.4f}\")\nax1.set_xlabel(\"Rerank Score\")\nax1.set_ylabel(\"Density\")\nax1.set_title(\"Rerank Score Distribution (normalized)\")\nax1.legend(fontsize=8)\nfig1.tight_layout()\nfig1.savefig(FIGURES_DIR / \"rerank_score_distribution.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Saved: {FIGURES_DIR / 'rerank_score_distribution.png'}\")\n\n# --- Rerank score vs distance scatter ---\nfig2, ax2 = plt.subplots(figsize=(10, 5))\nfor data in all_data:\n    for result in data.get(\"reranked_results\", []):\n        color = \"#2ecc71\" if result.get(\"is_ground_truth\") else \"#e74c3c\"\n        alpha = 0.7 if result.get(\"is_ground_truth\") else 0.3\n        size = 60 if result.get(\"is_ground_truth\") else 25\n        ax2.scatter(result[\"distance\"], result[\"rerank_score\"], c=color, alpha=alpha, s=size, edgecolors=\"white\", linewidth=0.3)\n\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0], [0], marker='o', color='w', markerfacecolor='#2ecc71', markersize=8, label='Ground Truth'),\n    Line2D([0], [0], marker='o', color='w', markerfacecolor='#e74c3c', markersize=8, label='Non-GT'),\n]\nax2.legend(handles=legend_elements)\nax2.set_xlabel(\"Vector Distance\")\nax2.set_ylabel(\"Rerank Score\")\nax2.set_title(\"Rerank Score vs Vector Distance\")\nax2.grid(True, alpha=0.3)\nfig2.tight_layout()\nfig2.savefig(FIGURES_DIR / \"rerank_vs_distance_scatter.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Saved: {FIGURES_DIR / 'rerank_vs_distance_scatter.png'}\")\n\nprint(f\"\\nPer GT memory details (sorted by rerank score, descending):\")\nfor score, test_case_id, memory_id in sorted(ground_truth_scores_all, key=lambda x: -x[0]):\n    # Find the distance for this memory\n    distance = 0\n    for data in all_data:\n        if data[\"test_case_id\"] == test_case_id:\n            for result in data.get(\"reranked_results\", []):\n                if result[\"id\"] == memory_id:\n                    distance = result[\"distance\"]\n                    break\n    print(f\"  score={score:.4f}  dist={distance:.4f}  {memory_id}  ({test_case_id})\")"
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 5b — Rerank Score Threshold Sweep\n",
    "\n",
    "Sweep rerank score thresholds: accept all candidates with score >= threshold.\n",
    "Higher threshold = stricter filtering (higher precision, lower recall).\n",
    "Compare with Phase 1 distance threshold sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase-comparison",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 14 — Rerank Score Threshold Sweep\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sweep rerank score thresholds (higher = stricter, opposite direction from distance)\nall_scores_flat = np.concatenate([ground_truth_scores, non_ground_truth_scores])\nsweep_thresholds = np.arange(0.0, max(all_scores_flat) + 0.01, 0.005)\n\nsweep_precisions, sweep_recalls, sweep_f1_scores, sweep_mrrs = [], [], [], []\n\nfor threshold in sweep_thresholds:\n    precisions, recalls, f1_scores, reciprocal_ranks = [], [], [], []\n    for experiment_result in experiments_reranked:\n        # Accept candidates with rerank score >= threshold\n        accepted = {result[\"id\"] for result in experiment_result[\"reranked\"] if result[\"rerank_score\"] >= threshold}\n        ground_truth_accepted = len(accepted & experiment_result[\"ground_truth_ids\"])\n        num_accepted = len(accepted)\n        ground_truth_count = len(experiment_result[\"ground_truth_ids\"])\n\n        precision = ground_truth_accepted / num_accepted if num_accepted > 0 else 0.0\n        recall = ground_truth_accepted / ground_truth_count if ground_truth_count > 0 else 0.0\n        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1_score)\n\n        # MRR: reciprocal rank of first GT hit in accepted results (sorted by rerank score desc)\n        accepted_sorted = [result for result in experiment_result[\"reranked\"] if result[\"rerank_score\"] >= threshold]\n        reciprocal_rank = 0.0\n        for rank, result in enumerate(accepted_sorted, 1):\n            if result[\"id\"] in experiment_result[\"ground_truth_ids\"]:\n                reciprocal_rank = 1.0 / rank\n                break\n        reciprocal_ranks.append(reciprocal_rank)\n\n    sweep_precisions.append(np.mean(precisions))\n    sweep_recalls.append(np.mean(recalls))\n    sweep_f1_scores.append(np.mean(f1_scores))\n    sweep_mrrs.append(np.mean(reciprocal_ranks))\n\nsweep_precisions = np.array(sweep_precisions)\nsweep_recalls = np.array(sweep_recalls)\nsweep_f1_scores = np.array(sweep_f1_scores)\nsweep_mrrs = np.array(sweep_mrrs)\n\nbest_f1_index = np.argmax(sweep_f1_scores)\nbest_threshold = sweep_thresholds[best_f1_index]\n\n# --- Figure: P/R/F1/MRR vs rerank score threshold ---\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(sweep_thresholds, sweep_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\nax.plot(sweep_thresholds, sweep_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\nax.plot(sweep_thresholds, sweep_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\nax.plot(sweep_thresholds, sweep_mrrs, label=\"MRR\", color=\"#e67e22\", linewidth=2)\nax.axvline(best_threshold, color=\"#e74c3c\", linestyle=\"--\", linewidth=1.5,\n           label=f\"Best F1 @ {best_threshold:.3f}\")\nax.set_xlabel(\"Rerank Score Threshold (accept >= threshold)\")\nax.set_ylabel(\"Score\")\nax.set_title(\"P/R/F1/MRR vs Rerank Score Threshold (macro-averaged)\")\nax.legend(fontsize=8)\nax.set_ylim(0, 1.05)\nax.grid(True, alpha=0.3)\nfig.tight_layout()\nfig.savefig(FIGURES_DIR / \"rerank_threshold_sweep.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Saved: {FIGURES_DIR / 'rerank_threshold_sweep.png'}\")\n\nprint(f\"\\nOptimal F1 threshold: {best_threshold:.4f}\")\nprint(f\"  F1:        {sweep_f1_scores[best_f1_index]:.3f}\")\nprint(f\"  Precision: {sweep_precisions[best_f1_index]:.3f}\")\nprint(f\"  Recall:    {sweep_recalls[best_f1_index]:.3f}\")\nprint(f\"  MRR:       {sweep_mrrs[best_f1_index]:.3f}\")\n\n# Threshold table\nprint(f\"\\nThreshold table (macro-averaged):\")\nprint(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12}\")\nprint(\"-\" * 75)\n\n# Pick representative thresholds around the interesting range\ntable_thresholds = sorted(set([0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.08, 0.10, 0.15, 0.20, 0.30, 0.50, round(best_threshold, 4)]))\n\nfor threshold in table_thresholds:\n    index = np.argmin(np.abs(sweep_thresholds - threshold))\n    avg_accepted = np.mean([len([result for result in experiment_result[\"reranked\"] if result[\"rerank_score\"] >= threshold]) for experiment_result in experiments_reranked])\n    avg_ground_truth_kept = np.mean([len({result[\"id\"] for result in experiment_result[\"reranked\"] if result[\"rerank_score\"] >= threshold} & experiment_result[\"ground_truth_ids\"]) for experiment_result in experiments_reranked])\n    marker = \" <--\" if abs(threshold - best_threshold) < 0.003 else \"\"\n    print(f\"{threshold:>10.4f} {sweep_precisions[index]:>10.3f} {sweep_recalls[index]:>8.3f} {sweep_f1_scores[index]:>8.3f} {sweep_mrrs[index]:>8.3f} {avg_accepted:>13.1f} {avg_ground_truth_kept:>12.1f}{marker}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 5c — Per-Experiment Impact at Optimal Rerank Threshold\n",
    "\n",
    "Show how the optimal rerank score threshold performs on each test case.\n",
    "Compare with Phase 1 distance threshold results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-cases",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 16 — Per-Experiment Impact at Optimal Rerank Threshold\nimport numpy as np\n\n# Phase 1 optimal threshold from threshold analysis notebook\nPHASE1_OPTIMAL_THRESHOLD = 0.76  # distance threshold\n\nprint(f\"Per-experiment comparison at optimal thresholds:\")\nprint(f\"  Phase 1: distance <= {PHASE1_OPTIMAL_THRESHOLD}\")\nprint(f\"  Rerank:  score >= {best_threshold:.4f}\")\nprint()\n\nprint(f\"{'Test Case':<20} {'P1 F1':>7} {'Rk F1':>7} {'Delta':>7} {'P1 P':>6} {'Rk P':>6} {'P1 R':>6} {'Rk R':>6} {'P1 Acc':>7} {'Rk Acc':>7} {'GT':>4}\")\nprint(\"-\" * 100)\n\nphase1_f1_scores, rerank_f1_scores = [], []\nfor experiment_result in experiments_reranked:\n    test_case_id = experiment_result[\"test_case_id\"]\n    ground_truth_ids = experiment_result[\"ground_truth_ids\"]\n    ground_truth_count = len(ground_truth_ids)\n\n    # Phase 1: distance threshold\n    phase1_data = phase1_by_test_case.get(test_case_id, {})\n    phase1_best_distances = {}\n    for query_result in phase1_data.get(\"queries\", []):\n        for result in query_result.get(\"results\", []):\n            memory_id = result[\"id\"]\n            distance = result.get(\"distance\", float(\"inf\"))\n            if memory_id not in phase1_best_distances or distance < phase1_best_distances[memory_id]:\n                phase1_best_distances[memory_id] = distance\n    phase1_accepted = {memory_id for memory_id, distance in phase1_best_distances.items() if distance <= PHASE1_OPTIMAL_THRESHOLD}\n    phase1_ground_truth_hits = len(phase1_accepted & ground_truth_ids)\n    phase1_accepted_count = len(phase1_accepted)\n    phase1_precision = phase1_ground_truth_hits / phase1_accepted_count if phase1_accepted_count > 0 else 0\n    phase1_recall = phase1_ground_truth_hits / ground_truth_count if ground_truth_count > 0 else 0\n    phase1_f1_score = 2 * phase1_precision * phase1_recall / (phase1_precision + phase1_recall) if (phase1_precision + phase1_recall) > 0 else 0\n\n    # Rerank: score threshold\n    rerank_accepted_list = [result for result in experiment_result[\"reranked\"] if result[\"rerank_score\"] >= best_threshold]\n    rerank_accepted_ids = {result[\"id\"] for result in rerank_accepted_list}\n    rerank_ground_truth_hits = len(rerank_accepted_ids & ground_truth_ids)\n    rerank_accepted_count = len(rerank_accepted_ids)\n    rerank_precision = rerank_ground_truth_hits / rerank_accepted_count if rerank_accepted_count > 0 else 0\n    rerank_recall = rerank_ground_truth_hits / ground_truth_count if ground_truth_count > 0 else 0\n    rerank_f1_score = 2 * rerank_precision * rerank_recall / (rerank_precision + rerank_recall) if (rerank_precision + rerank_recall) > 0 else 0\n\n    delta = rerank_f1_score - phase1_f1_score\n    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n\n    phase1_f1_scores.append(phase1_f1_score)\n    rerank_f1_scores.append(rerank_f1_score)\n\n    print(f\"{test_case_id:<20} {phase1_f1_score:>7.3f} {rerank_f1_score:>7.3f} {delta:>+6.3f}{marker} {phase1_precision:>6.1%} {rerank_precision:>6.1%} {phase1_recall:>6.1%} {rerank_recall:>6.1%} {phase1_accepted_count:>7} {rerank_accepted_count:>7} {ground_truth_count:>4}\")\n\n    # Show missed GT for rerank\n    rerank_missed = ground_truth_ids - rerank_accepted_ids\n    if rerank_missed:\n        for memory_id in sorted(rerank_missed):\n            # Find rerank score for this missed memory\n            score = next((result[\"rerank_score\"] for result in experiment_result[\"reranked\"] if result[\"id\"] == memory_id), None)\n            score_str = f\"score={score:.4f}\" if score is not None else \"NOT IN POOL\"\n            print(f\"  {'':20} Missed: {memory_id} ({score_str})\")\n\nprint(\"-\" * 100)\navg_phase1 = np.mean(phase1_f1_scores)\navg_rerank = np.mean(rerank_f1_scores)\ndelta = avg_rerank - avg_phase1\nprint(f\"{'AVERAGE':<20} {avg_phase1:>7.3f} {avg_rerank:>7.3f} {delta:>+6.3f}{'+'if delta>0 else '-' if delta<0 else '='}\")\n\nimproved = sum(1 for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_scores) if rerank_f1 > phase1_f1 + 0.001)\nsame = sum(1 for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_scores) if abs(rerank_f1 - phase1_f1) <= 0.001)\nworse = sum(1 for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_scores) if rerank_f1 < phase1_f1 - 0.001)\nprint(f\"\\nReranking helped: {improved}/{len(phase1_f1_scores)} | Same: {same}/{len(phase1_f1_scores)} | Hurt: {worse}/{len(phase1_f1_scores)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d9606-5f8b-41a8-bde5-1e67099c4923",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 17 — Final Summary\nimport numpy as np\n\nprint(\"=\" * 70)\nprint(\"RERANKING COMPARISON SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\nPhase 1 run: {PHASE1_RUN.name}\")\nprint(f\"Test cases: {len(experiments_reranked)}\")\nprint(f\"Reranker: {reranker.model_name}\")\n\n# Top-N comparison\nprint(f\"\\n--- Top-{RERANK_TOP_N} Comparison (distance vs rerank) ---\")\navg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in all_results) / len(all_results)\navg_rerank_f1 = sum(result[\"rerank_top_n_metrics\"][\"f1\"] for result in all_results) / len(all_results)\nprint(f\"  Distance top-{RERANK_TOP_N} avg F1: {avg_distance_f1:.3f}\")\nprint(f\"  Rerank top-{RERANK_TOP_N} avg F1:   {avg_rerank_f1:.3f}\")\nprint(f\"  Delta: {avg_rerank_f1 - avg_distance_f1:+.3f}\")\n\n# Threshold comparison\nprint(f\"\\n--- Threshold Comparison (Phase 1 distance vs rerank score) ---\")\nprint(f\"  Phase 1 distance threshold {PHASE1_OPTIMAL_THRESHOLD}: avg F1 = {avg_phase1:.3f}\")\nprint(f\"  Rerank score threshold {best_threshold:.4f}: avg F1 = {avg_rerank:.3f}\")\nprint(f\"  Delta: {avg_rerank - avg_phase1:+.3f} ({(avg_rerank - avg_phase1)/avg_phase1*100:+.1f}%)\")\n\n# Optimal rerank threshold details\nprint(f\"\\n--- Optimal Rerank Score Threshold ---\")\nprint(f\"  Threshold: {best_threshold:.4f}\")\nprint(f\"  F1:        {sweep_f1_scores[best_f1_index]:.3f}\")\nprint(f\"  Precision: {sweep_precisions[best_f1_index]:.3f}\")\nprint(f\"  Recall:    {sweep_recalls[best_f1_index]:.3f}\")\nprint(f\"  MRR:       {sweep_mrrs[best_f1_index]:.3f}\")\n\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44291127-f15d-4a86-9493-859c15f0b327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}