{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Phase 1 Reranking Comparison\n",
    "\n",
    "Applies cross-encoder reranking on top of **Phase 1 data** (memories, database, test cases).\n",
    "Uses Phase 1 prompts for query generation to ensure a clean apples-to-apples comparison.\n",
    "\n",
    "This notebook does **not** extract its own memories — it reuses the Phase 1 pipeline output\n",
    "and only adds the reranking step to measure its isolated impact on retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup-imports",
   "metadata": {},
   "source": "# Cell 1 — Setup & Imports\nimport os\nfrom pathlib import Path\n\nfrom memory_retrieval.search.reranker import Reranker\nfrom memory_retrieval.search.vector import VectorBackend\nfrom memory_retrieval.experiments.runner import run_experiment, run_all_experiments, ExperimentConfig\nfrom memory_retrieval.memories.schema import FIELD_SITUATION, FIELD_DISTANCE, FIELD_RERANK_SCORE\nfrom memory_retrieval.infra.io import load_json\nfrom memory_retrieval.infra.runs import (\n    create_run, get_latest_run, get_run, list_runs, update_run_status,\n    PHASE1, PHASE2,\n)\n\n# Find project root by walking up to pyproject.toml\nPROJECT_ROOT = Path.cwd()\nwhile not (PROJECT_ROOT / \"pyproject.toml\").exists():\n    if PROJECT_ROOT == PROJECT_ROOT.parent:\n        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n    PROJECT_ROOT = PROJECT_ROOT.parent\nos.chdir(PROJECT_ROOT)\n\n# Verify API key\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    print(\"WARNING: OPENROUTER_API_KEY is not set. Experiments will fail.\")\nelse:\n    print(\"OPENROUTER_API_KEY is set.\")\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(\"Imports OK.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "configuration",
   "metadata": {},
   "source": "# Cell 2 — Configuration\n\nPROMPT_VERSION = \"2.0.0\"\nMODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\"\n\n# Reranking configuration\nRERANK_TOP_N = 4          # Final results after reranking\nSEARCH_LIMIT = 20         # Vector search candidates per query\nDISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics comparison\n\n# Phase 1 run selection (source of DB and test cases)\n# To see available runs: print(list_runs(PHASE1))\n# To select specific run: PHASE1_RUN = get_run(PHASE1, \"run_20260208_143022\")\nPHASE1_RUN = get_latest_run(PHASE1)\n\n# Derived paths from Phase 1\nDB_PATH = str(PHASE1_RUN / \"memories\" / \"memories.db\")\nTEST_CASES_DIR = str(PHASE1_RUN / \"test_cases\")\n\n# Initialize backends\nvector_backend = VectorBackend()\n\nprint(\"Configuration:\")\nprint(f\"  Phase 1 run: {PHASE1_RUN.name}\")\nprint(f\"  Prompt version: {PROMPT_VERSION}\")\nprint(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\nprint(f\"  Rerank top-n: {RERANK_TOP_N}\")\nprint(f\"  Search limit: {SEARCH_LIMIT}\")\nprint(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\nprint(f\"  DB path: {DB_PATH}\")\nprint(f\"  Test cases dir: {TEST_CASES_DIR}\")\nprint(f\"  Memory count: {vector_backend.get_memory_count(DB_PATH)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Test Reranker\n",
    "\n",
    "Verify the cross-encoder reranker works correctly before running full experiments.\n",
    "Loads `bge-reranker-v2-m3` and tests it on a sample query."
   ]
  },
  {
   "cell_type": "code",
   "id": "test-reranker",
   "metadata": {},
   "source": "# Cell 4 — Test Reranker on Sample Query\nreranker = Reranker()\n\nsample_query = \"error handling in async functions\"\nprint(f\"Sample query: \\\"{sample_query}\\\"\\n\")\n\n# Vector search\nresults = vector_backend.search(DB_PATH, sample_query, limit=SEARCH_LIMIT)\nprint(f\"Vector search returned {len(results)} candidates\\n\")\n\n# Convert SearchResult objects to dicts for reranker\ncandidates = [\n    {\n        \"id\": r.id,\n        FIELD_SITUATION: r.situation,\n        \"lesson\": r.lesson,\n        FIELD_DISTANCE: r.raw_score,\n    }\n    for r in results\n]\n\n# Show top-5 by distance\nprint(\"--- Top 5 by Vector Distance ---\")\nfor i, c in enumerate(candidates[:5], 1):\n    print(f\"  [{i}] dist={c[FIELD_DISTANCE]:.4f} | {c['id']} | {c[FIELD_SITUATION][:80]}...\")\n\n# Rerank\nreranked = reranker.rerank(sample_query, candidates, top_n=RERANK_TOP_N)\n\nprint(f\"\\n--- Top {RERANK_TOP_N} after Reranking ---\")\nfor i, r in enumerate(reranked, 1):\n    print(f\"  [{i}] rerank={r[FIELD_RERANK_SCORE]:.4f} | dist={r[FIELD_DISTANCE]:.4f} | {r['id']}\")\n    print(f\"      {r[FIELD_SITUATION][:100]}\")\n    print()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Run All Experiments\n",
    "\n",
    "For each test case:\n",
    "1. Generate search queries from PR context via LLM\n",
    "2. Vector search for each query (top-20)\n",
    "3. Pool and deduplicate results\n",
    "4. Rerank with cross-encoder\n",
    "5. Take top-N results\n",
    "6. Compute metrics (before and after reranking)"
   ]
  },
  {
   "cell_type": "code",
   "id": "run-all-experiments",
   "metadata": {},
   "source": "# Cell 6 — Run All Experiments\n\n# Create Phase 2 run for results\nrun_id, PHASE2_RUN = create_run(\n    PHASE2,\n    description=f\"Reranking experiment (phase1: {PHASE1_RUN.name}, top-{RERANK_TOP_N})\",\n)\nRESULTS_DIR = str(PHASE2_RUN / \"results\")\n\n# Store config in run metadata\nupdate_run_status(PHASE2_RUN, \"config\", {\n    \"phase1_run_id\": PHASE1_RUN.name,\n    \"reranker_model\": reranker.model_name,\n    \"rerank_top_n\": RERANK_TOP_N,\n    \"search_limit\": SEARCH_LIMIT,\n})\n\nprint(f\"Phase 2 run: {run_id}\")\nprint(f\"Results dir: {RESULTS_DIR}\\n\")\n\nconfig = ExperimentConfig(\n    search_backend=vector_backend,\n    prompts_dir=\"data/prompts/phase1\",\n    prompt_version=PROMPT_VERSION,\n    model=MODEL_EXPERIMENT,\n    search_limit=SEARCH_LIMIT,\n    distance_threshold=DISTANCE_THRESHOLD,\n    reranker=reranker,\n    rerank_top_n=RERANK_TOP_N,\n)\nall_results = run_all_experiments(\n    test_cases_dir=TEST_CASES_DIR,\n    db_path=DB_PATH,\n    results_dir=RESULTS_DIR,\n    config=config,\n)\n\n# Update run status\nsuccessful = [r for r in all_results if \"post_rerank_metrics\" in r]\navg_f1 = sum(r[\"post_rerank_metrics\"][\"f1\"] for r in successful) / len(successful) if successful else 0\nupdate_run_status(PHASE2_RUN, \"experiment\", {\n    \"count\": len(successful),\n    \"failed\": len(all_results) - len(successful),\n    \"avg_f1_post_rerank\": round(avg_f1, 4),\n    \"rerank_top_n\": RERANK_TOP_N,\n    \"prompt_version\": PROMPT_VERSION,\n})",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Results Summary Table\n",
    "\n",
    "Per-test-case comparison of pre-rerank vs post-rerank metrics."
   ]
  },
  {
   "cell_type": "code",
   "id": "results-table",
   "metadata": {},
   "source": [
    "# Cell 8 — Results Summary Table\n",
    "\n",
    "successful = [r for r in all_results if \"post_rerank_metrics\" in r]\n",
    "\n",
    "print(f\"{'Test Case':<30} {'Pre-F1':>8} {'Post-F1':>9} {'Delta':>8} {'Pre-P':>7} {'Post-P':>8} {'Pre-R':>7} {'Post-R':>8}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for r in successful:\n",
    "    name = r.get(\"test_case_id\", \"?\")[:30]\n",
    "    pre = r[\"pre_rerank_metrics\"]\n",
    "    post = r[\"post_rerank_metrics\"]\n",
    "    delta = post[\"f1\"] - pre[\"f1\"]\n",
    "    marker = \"↑\" if delta > 0 else \"↓\" if delta < 0 else \"=\"\n",
    "    print(f\"{name:<30} {pre['f1']:>8.3f} {post['f1']:>9.3f} {delta:>+7.3f}{marker} {pre['precision']:>7.3f} {post['precision']:>8.3f} {pre['recall']:>7.3f} {post['recall']:>8.3f}\")\n",
    "\n",
    "if successful:\n",
    "    avg_pre_f1 = sum(r[\"pre_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n",
    "    avg_post_f1 = sum(r[\"post_rerank_metrics\"][\"f1\"] for r in successful) / len(successful)\n",
    "    avg_pre_p = sum(r[\"pre_rerank_metrics\"][\"precision\"] for r in successful) / len(successful)\n",
    "    avg_post_p = sum(r[\"post_rerank_metrics\"][\"precision\"] for r in successful) / len(successful)\n",
    "    avg_pre_r = sum(r[\"pre_rerank_metrics\"][\"recall\"] for r in successful) / len(successful)\n",
    "    avg_post_r = sum(r[\"post_rerank_metrics\"][\"recall\"] for r in successful) / len(successful)\n",
    "    delta = avg_post_f1 - avg_pre_f1\n",
    "    marker = \"↑\" if delta > 0 else \"↓\" if delta < 0 else \"=\"\n",
    "    print(\"-\" * 95)\n",
    "    print(f\"{'AVERAGE':<30} {avg_pre_f1:>8.3f} {avg_post_f1:>9.3f} {delta:>+7.3f}{marker} {avg_pre_p:>7.3f} {avg_post_p:>8.3f} {avg_pre_r:>7.3f} {avg_post_r:>8.3f}\")\n",
    "    print(f\"\\nTarget F1 > 0.75: {'✅ ACHIEVED' if avg_post_f1 > 0.75 else '❌ NOT YET'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 — Rerank Top-N Sweep\n",
    "\n",
    "Analyze how different top-N values after reranking affect metrics.\n",
    "Uses the same experiment data but varies the cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "id": "topn-sweep",
   "metadata": {},
   "source": [
    "# Cell 10 — Rerank Top-N Sweep Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FIGURES_DIR = Path(\"notebooks/phase2/figures\")\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load result files\n",
    "results_path = Path(RESULTS_DIR)\n",
    "result_files = sorted(results_path.glob(\"*.json\"))\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No result files found. Run experiments first.\")\n",
    "else:\n",
    "    all_data = [load_json(str(f)) for f in result_files]\n",
    "    n_cases = len(all_data)\n",
    "\n",
    "    # For each result, we have the full reranked_results list\n",
    "    # We can simulate different top-N by truncating\n",
    "    max_n = 20\n",
    "    n_values = list(range(1, max_n + 1))\n",
    "    \n",
    "    sweep_p, sweep_r, sweep_f1 = [], [], []\n",
    "    \n",
    "    for n in n_values:\n",
    "        n_p, n_r, n_f1 = [], [], []\n",
    "        for data in all_data:\n",
    "            gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "            gt_count = len(gt_ids)\n",
    "            \n",
    "            # Get all reranked results (already sorted by score)\n",
    "            reranked = data.get(\"reranked_results\", [])\n",
    "            \n",
    "            # If we need more than available reranked, we need the full candidate pool\n",
    "            # For now, we can only sweep up to len(reranked_results)\n",
    "            top_n_ids = {r[\"id\"] for r in reranked[:n]}\n",
    "            hits = len(top_n_ids & gt_ids)\n",
    "            actual_n = len(top_n_ids)\n",
    "            \n",
    "            p = hits / actual_n if actual_n > 0 else 0.0\n",
    "            r = hits / gt_count if gt_count > 0 else 0.0\n",
    "            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "            \n",
    "            n_p.append(p)\n",
    "            n_r.append(r)\n",
    "            n_f1.append(f1)\n",
    "        \n",
    "        sweep_p.append(np.mean(n_p))\n",
    "        sweep_r.append(np.mean(n_r))\n",
    "        sweep_f1.append(np.mean(n_f1))\n",
    "    \n",
    "    # Print table\n",
    "    print(f\"Rerank Top-N Sweep (averaged over {n_cases} test cases)\\n\")\n",
    "    print(f\"{'N':>4} {'Precision':>10} {'Recall':>8} {'F1':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for n in [1, 2, 3, 4, 5, 6, 8, 10]:\n",
    "        if n <= max_n:\n",
    "            i = n - 1\n",
    "            print(f\"{n:>4} {sweep_p[i]:>10.3f} {sweep_r[i]:>8.3f} {sweep_f1[i]:>8.3f}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(n_values, sweep_p, label=\"Precision\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=4)\n",
    "    ax.plot(n_values, sweep_r, label=\"Recall\", color=\"#2ecc71\", linewidth=2, marker=\"s\", markersize=4)\n",
    "    ax.plot(n_values, sweep_f1, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=4)\n",
    "    ax.axvline(x=RERANK_TOP_N, color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"Default N={RERANK_TOP_N}\")\n",
    "    ax.set_xlabel(\"Top-N (results kept after reranking)\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(f\"Precision / Recall / F1 vs Rerank Top-N (avg over {n_cases} test cases)\")\n",
    "    ax.set_xticks(n_values)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / \"rerank_topn_sweep.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'rerank_topn_sweep.png'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 — Rerank Score Distribution\n",
    "\n",
    "Analyze how rerank scores separate ground truth from non-ground-truth results.\n",
    "This helps determine if the reranker provides a cleaner separation than vector distance."
   ]
  },
  {
   "cell_type": "code",
   "id": "score-distribution",
   "metadata": {},
   "source": [
    "# Cell 12 — Rerank Score Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No result files found.\")\n",
    "else:\n",
    "    gt_scores = []\n",
    "    non_gt_scores = []\n",
    "    gt_distances = []\n",
    "    non_gt_distances = []\n",
    "    \n",
    "    for data in all_data:\n",
    "        for r in data.get(\"reranked_results\", []):\n",
    "            if r.get(\"is_ground_truth\"):\n",
    "                gt_scores.append(r[\"rerank_score\"])\n",
    "                gt_distances.append(r[\"distance\"])\n",
    "            else:\n",
    "                non_gt_scores.append(r[\"rerank_score\"])\n",
    "                non_gt_distances.append(r[\"distance\"])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Rerank score distribution\n",
    "    ax = axes[0]\n",
    "    if gt_scores:\n",
    "        ax.hist(gt_scores, bins=20, alpha=0.7, label=f\"Ground Truth (n={len(gt_scores)})\", color=\"#2ecc71\")\n",
    "    if non_gt_scores:\n",
    "        ax.hist(non_gt_scores, bins=20, alpha=0.7, label=f\"Non-GT (n={len(non_gt_scores)})\", color=\"#e74c3c\")\n",
    "    ax.set_xlabel(\"Rerank Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Rerank Score Distribution\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rerank score vs distance scatter\n",
    "    ax = axes[1]\n",
    "    if gt_scores:\n",
    "        ax.scatter(gt_distances, gt_scores, alpha=0.7, label=\"Ground Truth\", color=\"#2ecc71\", s=60, zorder=3)\n",
    "    if non_gt_scores:\n",
    "        ax.scatter(non_gt_distances, non_gt_scores, alpha=0.5, label=\"Non-GT\", color=\"#e74c3c\", s=40, zorder=2)\n",
    "    ax.set_xlabel(\"Vector Distance\")\n",
    "    ax.set_ylabel(\"Rerank Score\")\n",
    "    ax.set_title(\"Rerank Score vs Vector Distance\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / \"rerank_score_distribution.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    if gt_scores:\n",
    "        print(f\"GT rerank scores:     min={min(gt_scores):.4f}, max={max(gt_scores):.4f}, mean={np.mean(gt_scores):.4f}\")\n",
    "    if non_gt_scores:\n",
    "        print(f\"Non-GT rerank scores: min={min(non_gt_scores):.4f}, max={max(non_gt_scores):.4f}, mean={np.mean(non_gt_scores):.4f}\")\n",
    "    if gt_scores and non_gt_scores:\n",
    "        separation = np.mean(gt_scores) - np.mean(non_gt_scores)\n",
    "        print(f\"Mean separation: {separation:.4f} ({'good' if separation > 1.0 else 'moderate' if separation > 0.5 else 'weak'})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6 — Phase 1 vs Phase 2 Comparison\n",
    "\n",
    "Side-by-side comparison with Phase 1 results (if available)."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase-comparison",
   "metadata": {},
   "source": [
    "# Cell 14 — Phase 1 vs Phase 2 Comparison\n",
    "\n",
    "# Load Phase 1 results\n",
    "phase1_results_dir = PHASE1_RUN / \"results\"\n",
    "phase1_result_files = sorted(phase1_results_dir.glob(\"*.json\"))\n",
    "\n",
    "if not phase1_result_files:\n",
    "    print(\"No Phase 1 results found for comparison.\")\n",
    "else:\n",
    "    phase1_data = [load_json(str(f)) for f in phase1_result_files]\n",
    "    \n",
    "    # Build lookup by test_case_id\n",
    "    p1_by_tc = {}\n",
    "    for d in phase1_data:\n",
    "        tc_id = d.get(\"test_case_id\", \"?\")\n",
    "        if tc_id not in p1_by_tc or d.get(\"experiment_id\", \"\") > p1_by_tc[tc_id].get(\"experiment_id\", \"\"):\n",
    "            p1_by_tc[tc_id] = d\n",
    "    \n",
    "    p2_by_tc = {}\n",
    "    for d in all_data:\n",
    "        tc_id = d.get(\"test_case_id\", \"?\")\n",
    "        p2_by_tc[tc_id] = d\n",
    "    \n",
    "    # Compare\n",
    "    common_tcs = sorted(set(p1_by_tc.keys()) & set(p2_by_tc.keys()))\n",
    "    \n",
    "    print(f\"Comparing {len(common_tcs)} test cases\\n\")\n",
    "    print(f\"{'Test Case':<25} {'P1 F1':>7} {'P2 F1':>7} {'Delta':>8} {'P1 Prec':>8} {'P2 Prec':>8} {'P1 Rec':>7} {'P2 Rec':>7}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    p1_f1s, p2_f1s = [], []\n",
    "    for tc_id in common_tcs:\n",
    "        p1 = p1_by_tc[tc_id].get(\"metrics\", {})\n",
    "        p2 = p2_by_tc[tc_id].get(\"post_rerank_metrics\", {})\n",
    "        \n",
    "        p1_f1 = p1.get(\"f1\", 0)\n",
    "        p2_f1 = p2.get(\"f1\", 0)\n",
    "        delta = p2_f1 - p1_f1\n",
    "        marker = \"↑\" if delta > 0.01 else \"↓\" if delta < -0.01 else \"=\"\n",
    "        \n",
    "        p1_f1s.append(p1_f1)\n",
    "        p2_f1s.append(p2_f1)\n",
    "        \n",
    "        print(f\"{tc_id[:25]:<25} {p1_f1:>7.3f} {p2_f1:>7.3f} {delta:>+7.3f}{marker} {p1.get('precision', 0):>8.3f} {p2.get('precision', 0):>8.3f} {p1.get('recall', 0):>7.3f} {p2.get('recall', 0):>7.3f}\")\n",
    "    \n",
    "    if p1_f1s and p2_f1s:\n",
    "        avg_p1 = np.mean(p1_f1s)\n",
    "        avg_p2 = np.mean(p2_f1s)\n",
    "        delta = avg_p2 - avg_p1\n",
    "        print(\"-\" * 85)\n",
    "        print(f\"{'AVERAGE':<25} {avg_p1:>7.3f} {avg_p2:>7.3f} {delta:>+7.3f}{'↑' if delta > 0 else '↓'}\")\n",
    "        print(f\"\\nPhase 1 avg F1: {avg_p1:.3f}\")\n",
    "        print(f\"Phase 2 avg F1: {avg_p2:.3f}\")\n",
    "        print(f\"Improvement: {delta:+.3f} ({delta/avg_p1*100:+.1f}%)\")\n",
    "        print(f\"\\nTarget F1 > 0.75: {'✅ ACHIEVED' if avg_p2 > 0.75 else '❌ NOT YET'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 7 — Worst Cases Deep Dive\n",
    "\n",
    "Focus on review_2 and review_5 which were problematic in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "worst-cases",
   "metadata": {},
   "source": [
    "# Cell 16 — Worst Cases Analysis\n",
    "\n",
    "worst_cases = [\"tc_review_2\", \"tc_review_5\"]\n",
    "\n",
    "for tc_id in worst_cases:\n",
    "    data = p2_by_tc.get(tc_id)\n",
    "    if not data:\n",
    "        print(f\"\\n{tc_id}: not found in results\")\n",
    "        continue\n",
    "    \n",
    "    gt_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    pre = data.get(\"pre_rerank_metrics\", {})\n",
    "    post = data.get(\"post_rerank_metrics\", {})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{tc_id} — Ground truth: {len(gt_ids)} memories\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pre-rerank:  F1={pre.get('f1', 0):.3f}  P={pre.get('precision', 0):.3f}  R={pre.get('recall', 0):.3f}\")\n",
    "    print(f\"Post-rerank: F1={post.get('f1', 0):.3f}  P={post.get('precision', 0):.3f}  R={post.get('recall', 0):.3f}\")\n",
    "    \n",
    "    print(f\"\\nReranked results (top-{RERANK_TOP_N}):\")\n",
    "    for i, r in enumerate(data.get(\"reranked_results\", []), 1):\n",
    "        gt_marker = \"✅ GT\" if r.get(\"is_ground_truth\") else \"  --\"\n",
    "        print(f\"  [{i}] {gt_marker} | rerank={r['rerank_score']:.4f} dist={r['distance']:.4f} | {r['id']}\")\n",
    "        print(f\"       {r.get('situation', '')[:100]}\")\n",
    "    \n",
    "    missed = data.get(\"missed_ground_truth_ids\", [])\n",
    "    if missed:\n",
    "        print(f\"\\nMissed ground truth ({len(missed)}):\")\n",
    "        for mid in missed:\n",
    "            print(f\"  - {mid}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
