{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 1 Reranking Comparison\n",
    "\n",
    "Applies cross-encoder reranking on top of **existing Phase 1 experiment results**.\n",
    "Reuses the exact same queries, vector search candidates, and ground truth from the Phase 1 run.\n",
    "\n",
    "This notebook does **not** generate new queries or run new searches — it loads the saved\n",
    "Phase 1 results and only adds the reranking step to measure its isolated impact on retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from retrieval_metrics.compute import compute_set_metrics\n",
    "\n",
    "from memory_retrieval.experiments.metrics_adapter import metric_point_to_dict\n",
    "from memory_retrieval.infra.figures import create_figure_session, save_figure\n",
    "from memory_retrieval.infra.io import load_json, save_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    PHASE1,\n",
    "    PHASE2,\n",
    "    create_run,\n",
    "    get_latest_run,\n",
    "    update_run_status,\n",
    ")\n",
    "from memory_retrieval.memories.schema import FIELD_DISTANCE, FIELD_RERANK_SCORE, FIELD_SITUATION\n",
    "from memory_retrieval.search.reranker import Reranker\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "def compute_metrics(retrieved_ids, ground_truth_ids):\n",
    "    point = compute_set_metrics(retrieved_ids, ground_truth_ids)\n",
    "    rounded = metric_point_to_dict(point, round_digits=4)\n",
    "    return {\n",
    "        \"precision\": rounded[\"precision\"],\n",
    "        \"recall\": rounded[\"recall\"],\n",
    "        \"f1\": rounded[\"f1\"],\n",
    "    }\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration\n",
    "\n",
    "# Analysis parameter for top-N comparison tables\n",
    "ANALYSIS_TOP_N = 4\n",
    "DISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics (must match Phase 1)\n",
    "\n",
    "# Phase 1 run selection (source of results, DB, and test cases)\n",
    "# To see available runs: print(list_runs(PHASE1))\n",
    "# To select specific run: PHASE1_RUN = get_run(PHASE1, \"run_20260208_143022\")\n",
    "PHASE1_RUN = get_latest_run(PHASE1)\n",
    "\n",
    "# Derived paths from Phase 1\n",
    "PHASE1_RESULTS_DIR = PHASE1_RUN / \"results\"\n",
    "phase1_result_files = sorted(PHASE1_RESULTS_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Phase 1 run: {PHASE1_RUN.name}\")\n",
    "print(f\"  Phase 1 results dir: {PHASE1_RESULTS_DIR}\")\n",
    "print(f\"  Phase 1 result files: {len(phase1_result_files)}\")\n",
    "print(f\"  Analysis top-n: {ANALYSIS_TOP_N}\")\n",
    "print(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\n",
    "\n",
    "if not phase1_result_files:\n",
    "    print(\"\\nERROR: No Phase 1 result files found. Run phase1.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1 — Load Phase 1 Results & Initialize Reranker\n",
    "\n",
    "Load existing Phase 1 experiment results (queries + vector search candidates).\n",
    "Initialize the cross-encoder reranker (`bge-reranker-v2-m3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Load Phase 1 Results & Initialize Reranker\n",
    "\n",
    "# Load all Phase 1 experiment results\n",
    "# When multiple results exist per test case, keep the latest experiment\n",
    "phase1_by_test_case: dict[str, dict] = {}\n",
    "for result_file in phase1_result_files:\n",
    "    data = load_json(str(result_file))\n",
    "    test_case_id = data.get(\"test_case_id\", \"?\")\n",
    "    if test_case_id not in phase1_by_test_case or data.get(\n",
    "        \"experiment_id\", \"\"\n",
    "    ) > phase1_by_test_case[test_case_id].get(\"experiment_id\", \"\"):\n",
    "        phase1_by_test_case[test_case_id] = data\n",
    "\n",
    "print(f\"Loaded {len(phase1_by_test_case)} unique test case results from Phase 1\\n\")\n",
    "for test_case_id, data in sorted(phase1_by_test_case.items()):\n",
    "    ground_truth_count = data.get(\"ground_truth\", {}).get(\"count\", 0)\n",
    "    metrics = data.get(\"metrics\", {})\n",
    "    num_queries = len(data.get(\"queries\", []))\n",
    "    print(\n",
    "        f\"  {test_case_id}: {num_queries} queries, {ground_truth_count} GT memories, F1={metrics.get('f1', 0):.3f}\"\n",
    "    )\n",
    "\n",
    "# Initialize reranker\n",
    "reranker = Reranker()\n",
    "\n",
    "# Quick test to load the model\n",
    "print(\"\\nLoading reranker model (first use triggers download)...\")\n",
    "_ = reranker.score_pairs(\"test\", [\"test document\"])\n",
    "print(\"Reranker ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2 — Apply Reranking to Phase 1 Results\n",
    "\n",
    "For each Phase 1 test case result:\n",
    "1. Pool and deduplicate vector search candidates from all queries\n",
    "2. Compute **top-N by vector distance** baseline (apples-to-apples with reranking)\n",
    "3. Rerank candidates with cross-encoder, take top-N\n",
    "4. Compare both at the same N — the only difference is ranking method\n",
    "\n",
    "No new LLM calls or vector searches — everything is reused from Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Apply Reranking to Phase 1 Results\n",
    "\n",
    "from retrieval_metrics.compute import compute_threshold_metrics, compute_top_n_metrics\n",
    "from retrieval_metrics.sweeps import find_optimal_entry\n",
    "\n",
    "from memory_retrieval.experiments.metrics import pool_and_deduplicate_by_distance\n",
    "from memory_retrieval.experiments.metrics_adapter import (\n",
    "    restriction_evaluation_to_dict,\n",
    "    threshold_sweep_from_experiments as sweep_threshold,\n",
    "    top_n_sweep_from_experiments as sweep_top_n,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics_at_top_n(ranked_results, ground_truth_ids, top_n, id_field=\"id\"):\n",
    "    evaluation = compute_top_n_metrics(ranked_results, ground_truth_ids, top_n, id_key=id_field)\n",
    "    return restriction_evaluation_to_dict(evaluation)\n",
    "\n",
    "\n",
    "def compute_metrics_at_threshold(\n",
    "    ranked_results,\n",
    "    ground_truth_ids,\n",
    "    threshold,\n",
    "    score_field,\n",
    "    higher_is_better,\n",
    "    id_field=\"id\",\n",
    "):\n",
    "    evaluation = compute_threshold_metrics(\n",
    "        ranked_results,\n",
    "        ground_truth_ids,\n",
    "        threshold,\n",
    "        score_key=score_field,\n",
    "        higher_is_better=higher_is_better,\n",
    "        id_key=id_field,\n",
    "    )\n",
    "    return restriction_evaluation_to_dict(evaluation, include_accepted_count=True)\n",
    "\n",
    "\n",
    "def find_optimal_threshold(sweep_results, metric=\"f1\"):\n",
    "    return find_optimal_entry(sweep_results, metric_key=metric)\n",
    "\n",
    "\n",
    "# Create Phase 2 run for results\n",
    "run_id, PHASE2_RUN = create_run(\n",
    "    PHASE2,\n",
    "    description=f\"Reranking on phase1 results ({PHASE1_RUN.name})\",\n",
    ")\n",
    "RESULTS_DIR = str(PHASE2_RUN / \"results\")\n",
    "\n",
    "update_run_status(\n",
    "    PHASE2_RUN,\n",
    "    \"config\",\n",
    "    {\n",
    "        \"phase1_run_id\": PHASE1_RUN.name,\n",
    "        \"reranker_model\": reranker.model_name,\n",
    "        \"analysis_top_n\": ANALYSIS_TOP_N,\n",
    "        \"distance_threshold\": DISTANCE_THRESHOLD,\n",
    "        \"mode\": \"reuse_phase1_results\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Phase 2 run: {run_id}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\\n\")\n",
    "\n",
    "all_results: list[dict] = []\n",
    "\n",
    "# Iterate over all results from phase1\n",
    "for i, (test_case_id, phase1_data) in enumerate(sorted(phase1_by_test_case.items()), 1):\n",
    "    print(f\"[{i}/{len(phase1_by_test_case)}] {test_case_id}\")\n",
    "\n",
    "    ground_truth_ids = set(phase1_data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    query_results = phase1_data.get(\"queries\", [])\n",
    "\n",
    "    # Pool and deduplicate across all queries (sorted by distance, best first)\n",
    "    pooled = pool_and_deduplicate_by_distance(query_results)\n",
    "    print(f\"  Deduplicated memories count: {len(pooled)}\")\n",
    "\n",
    "    # --- Baseline: top-N by vector distance (same N as analysis) ---\n",
    "    distance_top_n_metrics = compute_metrics_at_top_n(pooled, ground_truth_ids, ANALYSIS_TOP_N)\n",
    "\n",
    "    # --- Per-query reranking: rerank each query's results independently ---\n",
    "    all_reranked_per_query = []\n",
    "    # Iterate over all queries\n",
    "    for query_result in query_results:\n",
    "        candidates = [\n",
    "            {\n",
    "                \"id\": result[\"id\"],\n",
    "                FIELD_SITUATION: result.get(\"situation\", result.get(FIELD_SITUATION, \"\")),\n",
    "                FIELD_DISTANCE: result.get(\"distance\", 0),\n",
    "                \"is_ground_truth\": result.get(\"is_ground_truth\", result[\"id\"] in ground_truth_ids),\n",
    "            }\n",
    "            # Iterate over all results for this query\n",
    "            for result in query_result.get(\"results\", [])\n",
    "        ]\n",
    "        # Simple reranking, just using \"situation\" description - pass query string of given query result with all its results array\n",
    "        # mapped to candidates\n",
    "        reranked = reranker.rerank(query_result[\"query\"], candidates, top_n=None)\n",
    "        all_reranked_per_query.extend(reranked)\n",
    "\n",
    "    # Deduplicate by best rerank score - drop information about original query string\n",
    "    best_by_memory_id: dict[str, dict] = {}\n",
    "    for result in all_reranked_per_query:\n",
    "        memory_id = result[\"id\"]\n",
    "        if (\n",
    "            memory_id not in best_by_memory_id\n",
    "            or result[FIELD_RERANK_SCORE] > best_by_memory_id[memory_id][FIELD_RERANK_SCORE]\n",
    "        ):\n",
    "            best_by_memory_id[memory_id] = result\n",
    "    all_reranked = sorted(\n",
    "        best_by_memory_id.values(), key=lambda x: x[FIELD_RERANK_SCORE], reverse=True\n",
    "    )\n",
    "\n",
    "    # Compute top-N metrics for display\n",
    "    rerank_top_n_metrics = compute_metrics_at_top_n(all_reranked, ground_truth_ids, ANALYSIS_TOP_N)\n",
    "\n",
    "    f1_delta = rerank_top_n_metrics[\"f1\"] - distance_top_n_metrics[\"f1\"]\n",
    "    marker = \"+\" if f1_delta > 0 else \"\"\n",
    "    print(\n",
    "        f\"  Top-{ANALYSIS_TOP_N} by distance F1={distance_top_n_metrics['f1']:.3f} | Top-{ANALYSIS_TOP_N} by rerank F1={rerank_top_n_metrics['f1']:.3f} ({marker}{f1_delta:.3f})\"\n",
    "    )\n",
    "\n",
    "    # Build result\n",
    "    result = {\n",
    "        \"test_case_id\": test_case_id,\n",
    "        \"source_file\": phase1_data.get(\"source_file\", \"unknown\"),\n",
    "        \"phase1_experiment_id\": phase1_data.get(\"experiment_id\", \"unknown\"),\n",
    "        \"model\": phase1_data.get(\"model\", \"unknown\"),\n",
    "        \"prompt_version\": phase1_data.get(\"prompt_version\", \"unknown\"),\n",
    "        \"reranker_model\": reranker.model_name,\n",
    "        \"rerank_queries\": [query_result[\"query\"] for query_result in query_results],\n",
    "        \"distance_threshold\": DISTANCE_THRESHOLD,\n",
    "        \"ground_truth\": phase1_data.get(\"ground_truth\", {}),\n",
    "        \"queries\": query_results,\n",
    "        \"pooled_candidate_count\": len(pooled),\n",
    "        \"distance_top_n_metrics\": {\n",
    "            \"precision\": distance_top_n_metrics[\"precision\"],\n",
    "            \"recall\": distance_top_n_metrics[\"recall\"],\n",
    "            \"f1\": distance_top_n_metrics[\"f1\"],\n",
    "            \"n\": ANALYSIS_TOP_N,\n",
    "            \"ground_truth_retrieved\": len(\n",
    "                distance_top_n_metrics[\"retrieved_ids\"] & ground_truth_ids\n",
    "            ),\n",
    "        },\n",
    "        \"reranked_results\": [\n",
    "            {\n",
    "                \"id\": result[\"id\"],\n",
    "                \"rerank_score\": result[FIELD_RERANK_SCORE],\n",
    "                \"distance\": result.get(FIELD_DISTANCE, 0),\n",
    "                \"situation\": result.get(FIELD_SITUATION, \"\"),\n",
    "                \"is_ground_truth\": result[\"id\"] in ground_truth_ids,\n",
    "            }\n",
    "            for result in all_reranked  # Store ALL reranked (for sweep analysis)\n",
    "        ],\n",
    "        \"distance_top_n_results\": [\n",
    "            {\n",
    "                \"id\": result[\"id\"],\n",
    "                \"distance\": result.get(\"distance\", 0),\n",
    "                \"situation\": result.get(\"situation\", result.get(FIELD_SITUATION, \"\")),\n",
    "                \"is_ground_truth\": result.get(\"is_ground_truth\", result[\"id\"] in ground_truth_ids),\n",
    "            }\n",
    "            for result in pooled[:ANALYSIS_TOP_N]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    save_json(result, Path(RESULTS_DIR) / f\"rerank_{test_case_id}.json\")\n",
    "    all_results.append(result)\n",
    "\n",
    "# Summary\n",
    "successful = [result for result in all_results if \"reranked_results\" in result]\n",
    "if successful:\n",
    "    avg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in successful) / len(\n",
    "        successful\n",
    "    )\n",
    "\n",
    "    # Compute rerank top-N metrics from stored reranked_results\n",
    "    rerank_f1_scores = []\n",
    "    for result in successful:\n",
    "        ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        rerank_metrics = compute_metrics_at_top_n(\n",
    "            result[\"reranked_results\"], ground_truth_ids, ANALYSIS_TOP_N\n",
    "        )\n",
    "        rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "    avg_rerank_f1 = sum(rerank_f1_scores) / len(rerank_f1_scores)\n",
    "\n",
    "    print(f\"\\n{'=' * 55}\")\n",
    "    print(f\"SUMMARY ({len(successful)} test cases, top-{ANALYSIS_TOP_N})\")\n",
    "    print(f\"{'=' * 55}\")\n",
    "    print(f\"  Avg distance top-{ANALYSIS_TOP_N} F1: {avg_distance_f1:.3f}\")\n",
    "    print(f\"  Avg rerank top-{ANALYSIS_TOP_N} F1:   {avg_rerank_f1:.3f}\")\n",
    "    print(f\"  Delta: {avg_rerank_f1 - avg_distance_f1:+.3f}\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    PHASE2_RUN,\n",
    "    \"experiment\",\n",
    "    {\n",
    "        \"count\": len(successful),\n",
    "        \"avg_f1_distance_top_n\": round(avg_distance_f1, 4) if successful else 0,\n",
    "        \"avg_f1_rerank_top_n\": round(avg_rerank_f1, 4) if successful else 0,\n",
    "        \"analysis_top_n\": ANALYSIS_TOP_N,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3 — Results Summary Table\n",
    "\n",
    "Per-test-case comparison: **top-N by distance** vs **top-N by rerank score** (same N, same candidates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Results Summary Table\n",
    "\n",
    "successful = [result for result in all_results if \"reranked_results\" in result]\n",
    "\n",
    "print(f\"Top-{ANALYSIS_TOP_N} comparison: vector distance vs cross-encoder reranking\\n\")\n",
    "print(\n",
    "    f\"{'Test Case':<25} {'Dist F1':>8} {'Rank F1':>8} {'Delta':>8} {'Dist P':>7} {'Rank P':>7} {'Dist R':>7} {'Rank R':>7}\"\n",
    ")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for result in successful:\n",
    "    name = result.get(\"test_case_id\", \"?\")[:25]\n",
    "    distance_metrics = result[\"distance_top_n_metrics\"]\n",
    "    ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "\n",
    "    # Compute rerank top-N metrics from reranked_results\n",
    "    rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "    rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "\n",
    "    delta = rerank_metrics[\"f1\"] - distance_metrics[\"f1\"]\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "    print(\n",
    "        f\"{name:<25} {distance_metrics['f1']:>8.3f} {rerank_metrics['f1']:>8.3f} {delta:>+7.3f}{marker} {distance_metrics['precision']:>7.3f} {rerank_metrics['precision']:>7.3f} {distance_metrics['recall']:>7.3f} {rerank_metrics['recall']:>7.3f}\"\n",
    "    )\n",
    "\n",
    "if successful:\n",
    "    avg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in successful) / len(\n",
    "        successful\n",
    "    )\n",
    "    avg_distance_precision = sum(\n",
    "        result[\"distance_top_n_metrics\"][\"precision\"] for result in successful\n",
    "    ) / len(successful)\n",
    "    avg_distance_recall = sum(\n",
    "        result[\"distance_top_n_metrics\"][\"recall\"] for result in successful\n",
    "    ) / len(successful)\n",
    "\n",
    "    rerank_f1_scores, rerank_precisions, rerank_recalls = [], [], []\n",
    "    for result in successful:\n",
    "        ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "        rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "        rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "        rerank_precisions.append(rerank_metrics[\"precision\"])\n",
    "        rerank_recalls.append(rerank_metrics[\"recall\"])\n",
    "\n",
    "    avg_rerank_f1 = sum(rerank_f1_scores) / len(rerank_f1_scores)\n",
    "    avg_rerank_precision = sum(rerank_precisions) / len(rerank_precisions)\n",
    "    avg_rerank_recall = sum(rerank_recalls) / len(rerank_recalls)\n",
    "\n",
    "    delta = avg_rerank_f1 - avg_distance_f1\n",
    "    marker = \"+\" if delta > 0 else \"-\" if delta < 0 else \"=\"\n",
    "    print(\"-\" * 85)\n",
    "    print(\n",
    "        f\"{'AVERAGE':<25} {avg_distance_f1:>8.3f} {avg_rerank_f1:>8.3f} {delta:>+7.3f}{marker} {avg_distance_precision:>7.3f} {avg_rerank_precision:>7.3f} {avg_distance_recall:>7.3f} {avg_rerank_recall:>7.3f}\"\n",
    "    )\n",
    "\n",
    "    improved = sum(\n",
    "        1\n",
    "        for result, rerank_f1 in zip(successful, rerank_f1_scores)\n",
    "        if rerank_f1 > result[\"distance_top_n_metrics\"][\"f1\"] + 0.001\n",
    "    )\n",
    "    same = sum(\n",
    "        1\n",
    "        for result, rerank_f1 in zip(successful, rerank_f1_scores)\n",
    "        if abs(rerank_f1 - result[\"distance_top_n_metrics\"][\"f1\"]) <= 0.001\n",
    "    )\n",
    "    worse = sum(\n",
    "        1\n",
    "        for result, rerank_f1 in zip(successful, rerank_f1_scores)\n",
    "        if rerank_f1 < result[\"distance_top_n_metrics\"][\"f1\"] - 0.001\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nReranking helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4 — Top-N Sweep: Distance vs Reranking\n",
    "\n",
    "Compare top-N by vector distance vs top-N by rerank score across different N values.\n",
    "Shows whether reranking improves ranking quality at every cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Top-N Sweep: Distance vs Reranking\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != PHASE2_RUN.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=PHASE2_RUN / \"figures\",\n",
    "        notebook_slug=\"phase1_reranking_comparison\",\n",
    "        context_key=PHASE2_RUN.name,\n",
    "        context={\n",
    "            \"phase\": PHASE2,\n",
    "            \"run_id\": PHASE2_RUN.name,\n",
    "            \"phase1_run_id\": PHASE1_RUN.name,\n",
    "        },\n",
    "    )\n",
    "print(f\"Figure export session: {FIGURE_SESSION.session_dir}\")\n",
    "\n",
    "all_data = all_results\n",
    "num_test_cases = len(all_data)\n",
    "\n",
    "if not all_data:\n",
    "    print(\"No results found. Run Step 2 first.\")\n",
    "else:\n",
    "    max_n = 20\n",
    "    n_values = list(range(1, max_n + 1))\n",
    "\n",
    "    # Build experiment lists for distance and rerank sweeps\n",
    "    distance_experiments = [\n",
    "        {\n",
    "            \"ground_truth_ids\": set(data.get(\"ground_truth\", {}).get(\"memory_ids\", [])),\n",
    "            \"ranked_results\": pool_and_deduplicate_by_distance(data.get(\"queries\", [])),\n",
    "        }\n",
    "        for data in all_data\n",
    "    ]\n",
    "    rerank_experiments = [\n",
    "        {\n",
    "            \"ground_truth_ids\": set(data.get(\"ground_truth\", {}).get(\"memory_ids\", [])),\n",
    "            \"ranked_results\": data.get(\"reranked_results\", []),\n",
    "        }\n",
    "        for data in all_data\n",
    "    ]\n",
    "\n",
    "    distance_sweep = sweep_top_n(distance_experiments, n_values)\n",
    "    rerank_sweep = sweep_top_n(rerank_experiments, n_values)\n",
    "\n",
    "    distance_f1_scores = [entry[\"f1\"] for entry in distance_sweep]\n",
    "    distance_precisions = [entry[\"precision\"] for entry in distance_sweep]\n",
    "    distance_recalls = [entry[\"recall\"] for entry in distance_sweep]\n",
    "    rerank_f1_scores = [entry[\"f1\"] for entry in rerank_sweep]\n",
    "    rerank_precisions = [entry[\"precision\"] for entry in rerank_sweep]\n",
    "    rerank_recalls = [entry[\"recall\"] for entry in rerank_sweep]\n",
    "\n",
    "    # Print table\n",
    "    print(f\"Top-N Sweep: Distance vs Reranking (averaged over {num_test_cases} test cases)\\n\")\n",
    "    print(\n",
    "        f\"{'N':>4} {'Dist P':>8} {'Rank P':>8} {'Dist R':>8} {'Rank R':>8} {'Dist F1':>8} {'Rank F1':>8} {'F1 Delta':>9}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "    for top_n in [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]:\n",
    "        if top_n <= max_n:\n",
    "            index = top_n - 1\n",
    "            delta = rerank_f1_scores[index] - distance_f1_scores[index]\n",
    "            print(\n",
    "                f\"{top_n:>4} {distance_precisions[index]:>8.3f} {rerank_precisions[index]:>8.3f} {distance_recalls[index]:>8.3f} {rerank_recalls[index]:>8.3f} {distance_f1_scores[index]:>8.3f} {rerank_f1_scores[index]:>8.3f} {delta:>+9.3f}\"\n",
    "            )\n",
    "\n",
    "    # Plot: F1 comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(\n",
    "        n_values,\n",
    "        distance_f1_scores,\n",
    "        label=\"F1 (distance)\",\n",
    "        color=\"#3498db\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.plot(\n",
    "        n_values,\n",
    "        rerank_f1_scores,\n",
    "        label=\"F1 (reranked)\",\n",
    "        color=\"#e74c3c\",\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.axvline(\n",
    "        x=ANALYSIS_TOP_N,\n",
    "        color=\"gray\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.5,\n",
    "        label=f\"Default N={ANALYSIS_TOP_N}\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Top-N\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "    ax.set_title(f\"F1: Distance vs Reranking (avg over {num_test_cases} test cases)\")\n",
    "    ax.set_xticks(n_values)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot: F1 delta\n",
    "    ax = axes[1]\n",
    "    f1_deltas = [rerank_f1_scores[i] - distance_f1_scores[i] for i in range(max_n)]\n",
    "    colors = [\"#2ecc71\" if delta > 0 else \"#e74c3c\" for delta in f1_deltas]\n",
    "    ax.bar(n_values, f1_deltas, color=colors, alpha=0.7)\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"Top-N\")\n",
    "    ax.set_ylabel(\"F1 Delta (rerank - distance)\")\n",
    "    ax.set_title(\"Reranking F1 Improvement by N\")\n",
    "    ax.set_xticks(n_values)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    saved_paths = save_figure(\n",
    "        fig,\n",
    "        FIGURE_SESSION,\n",
    "        \"rerank_topn_sweep\",\n",
    "        title=\"Top-N Sweep: Distance vs Reranking\",\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"Saved: {saved_paths['png']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5 — Rerank Score Threshold Analysis\n",
    "\n",
    "Mirrors the Phase 1 distance threshold analysis, but uses **rerank scores** as the filtering signal.\n",
    "Instead of a fixed top-N cutoff, we sweep rerank score thresholds to find the optimal cutoff.\n",
    "\n",
    "All metrics are **macro-averaged**: computed per test case, then averaged.\n",
    "Within each test case, candidates are deduplicated by memory ID (best distance), then reranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Rerank Score Distribution & Threshold Sweep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != PHASE2_RUN.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=PHASE2_RUN / \"figures\",\n",
    "        notebook_slug=\"phase1_reranking_comparison\",\n",
    "        context_key=PHASE2_RUN.name,\n",
    "        context={\n",
    "            \"phase\": PHASE2,\n",
    "            \"run_id\": PHASE2_RUN.name,\n",
    "            \"phase1_run_id\": PHASE1_RUN.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Collect per-observation rerank scores (one entry per test_case x memory pair)\n",
    "ground_truth_scores_all = []  # (score, test_case_id, memory_id)\n",
    "non_ground_truth_scores_all = []  # (score, test_case_id, memory_id)\n",
    "\n",
    "# Build per-experiment deduped rerank data (mirrors phase1 threshold analysis structure)\n",
    "experiments_reranked = []\n",
    "for data in all_data:\n",
    "    test_case_id = data[\"test_case_id\"]\n",
    "    ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    # reranked_results contains ALL candidates sorted by rerank score\n",
    "    reranked = data.get(\"reranked_results\", [])\n",
    "    scores_by_id = {result[\"id\"]: result for result in reranked}\n",
    "\n",
    "    experiments_reranked.append(\n",
    "        {\n",
    "            \"test_case_id\": test_case_id,\n",
    "            \"ground_truth_ids\": ground_truth_ids,\n",
    "            \"scores_by_id\": scores_by_id,\n",
    "            \"reranked\": reranked,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for result in reranked:\n",
    "        entry = (result[\"rerank_score\"], test_case_id, result[\"id\"])\n",
    "        if result.get(\"is_ground_truth\"):\n",
    "            ground_truth_scores_all.append(entry)\n",
    "        else:\n",
    "            non_ground_truth_scores_all.append(entry)\n",
    "\n",
    "ground_truth_scores = np.array([score for score, _, _ in ground_truth_scores_all])\n",
    "non_ground_truth_scores = np.array([score for score, _, _ in non_ground_truth_scores_all])\n",
    "\n",
    "print(f\"Experiments: {len(experiments_reranked)} test cases\")\n",
    "print(\n",
    "    f\"GT observations:     {len(ground_truth_scores)} (unique — each memory is GT in exactly 1 test case)\"\n",
    ")\n",
    "print(f\"Non-GT observations: {len(non_ground_truth_scores)}\")\n",
    "print()\n",
    "print(\n",
    "    f\"GT rerank score range:     [{ground_truth_scores.min():.4f}, {ground_truth_scores.max():.4f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"Non-GT rerank score range: [{non_ground_truth_scores.min():.4f}, {non_ground_truth_scores.max():.4f}]\"\n",
    ")\n",
    "print(f\"GT mean: {ground_truth_scores.mean():.4f}, median: {np.median(ground_truth_scores):.4f}\")\n",
    "print(\n",
    "    f\"Non-GT mean: {non_ground_truth_scores.mean():.4f}, median: {np.median(non_ground_truth_scores):.4f}\"\n",
    ")\n",
    "print(f\"Mean separation: {ground_truth_scores.mean() - non_ground_truth_scores.mean():.4f}\")\n",
    "\n",
    "# --- Figure 1: Score distribution histogram ---\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 5))\n",
    "all_scores_combined = np.concatenate([ground_truth_scores, non_ground_truth_scores])\n",
    "bins = np.linspace(all_scores_combined.min(), all_scores_combined.max(), 40)\n",
    "ax1.hist(\n",
    "    ground_truth_scores,\n",
    "    bins=bins,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=f\"GT (n={len(ground_truth_scores)})\",\n",
    "    color=\"#2ecc71\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax1.hist(\n",
    "    non_ground_truth_scores,\n",
    "    bins=bins,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=f\"Non-GT (n={len(non_ground_truth_scores)})\",\n",
    "    color=\"#e74c3c\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax1.axvline(\n",
    "    np.median(ground_truth_scores),\n",
    "    color=\"#27ae60\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"GT median: {np.median(ground_truth_scores):.4f}\",\n",
    ")\n",
    "ax1.axvline(\n",
    "    np.median(non_ground_truth_scores),\n",
    "    color=\"#c0392b\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Non-GT median: {np.median(non_ground_truth_scores):.4f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Rerank Score\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"Rerank Score Distribution (normalized)\")\n",
    "ax1.legend(fontsize=8)\n",
    "fig1.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig1,\n",
    "    FIGURE_SESSION,\n",
    "    \"rerank_score_distribution\",\n",
    "    title=\"Rerank Score Distribution (normalized)\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "\n",
    "# --- Rerank score vs distance scatter ---\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "for data in all_data:\n",
    "    for result in data.get(\"reranked_results\", []):\n",
    "        color = \"#2ecc71\" if result.get(\"is_ground_truth\") else \"#e74c3c\"\n",
    "        alpha = 0.7 if result.get(\"is_ground_truth\") else 0.3\n",
    "        size = 60 if result.get(\"is_ground_truth\") else 25\n",
    "        ax2.scatter(\n",
    "            result[\"distance\"],\n",
    "            result[\"rerank_score\"],\n",
    "            c=color,\n",
    "            alpha=alpha,\n",
    "            s=size,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"#2ecc71\",\n",
    "        markersize=8,\n",
    "        label=\"Ground Truth\",\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"#e74c3c\", markersize=8, label=\"Non-GT\"\n",
    "    ),\n",
    "]\n",
    "ax2.legend(handles=legend_elements)\n",
    "ax2.set_xlabel(\"Vector Distance\")\n",
    "ax2.set_ylabel(\"Rerank Score\")\n",
    "ax2.set_title(\"Rerank Score vs Vector Distance\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "fig2.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig2,\n",
    "    FIGURE_SESSION,\n",
    "    \"rerank_vs_distance_scatter\",\n",
    "    title=\"Rerank Score vs Vector Distance\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "print(\"\\nPer GT memory details (sorted by rerank score, descending):\")\n",
    "for score, test_case_id, memory_id in sorted(ground_truth_scores_all, key=lambda x: -x[0]):\n",
    "    # Find the distance for this memory\n",
    "    distance = 0\n",
    "    for data in all_data:\n",
    "        if data[\"test_case_id\"] == test_case_id:\n",
    "            for result in data.get(\"reranked_results\", []):\n",
    "                if result[\"id\"] == memory_id:\n",
    "                    distance = result[\"distance\"]\n",
    "                    break\n",
    "    print(f\"  score={score:.4f}  dist={distance:.4f}  {memory_id}  ({test_case_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5b — Rerank Score Threshold Sweep\n",
    "\n",
    "Sweep rerank score thresholds: accept all candidates with score >= threshold.\n",
    "Higher threshold = stricter filtering (higher precision, lower recall).\n",
    "Compare with Phase 1 distance threshold sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Rerank Score Threshold Sweep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != PHASE2_RUN.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=PHASE2_RUN / \"figures\",\n",
    "        notebook_slug=\"phase1_reranking_comparison\",\n",
    "        context_key=PHASE2_RUN.name,\n",
    "        context={\n",
    "            \"phase\": PHASE2,\n",
    "            \"run_id\": PHASE2_RUN.name,\n",
    "            \"phase1_run_id\": PHASE1_RUN.name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Build experiment list for threshold sweep\n",
    "rerank_experiments = []\n",
    "for data in all_data:\n",
    "    ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    reranked = data.get(\"reranked_results\", [])\n",
    "    rerank_experiments.append(\n",
    "        {\n",
    "            \"test_case_id\": data[\"test_case_id\"],\n",
    "            \"ground_truth_ids\": ground_truth_ids,\n",
    "            \"ranked_results\": reranked,\n",
    "            \"reranked\": reranked,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Sweep rerank score thresholds (higher = stricter, opposite direction from distance)\n",
    "all_scores_flat = np.concatenate([ground_truth_scores, non_ground_truth_scores])\n",
    "sweep_thresholds_list = list(np.arange(0.0, max(all_scores_flat) + 0.01, 0.005))\n",
    "\n",
    "rerank_sweep = sweep_threshold(\n",
    "    rerank_experiments,\n",
    "    sweep_thresholds_list,\n",
    "    score_field=\"rerank_score\",\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "sweep_precisions = np.array([entry[\"precision\"] for entry in rerank_sweep])\n",
    "sweep_recalls = np.array([entry[\"recall\"] for entry in rerank_sweep])\n",
    "sweep_f1_scores = np.array([entry[\"f1\"] for entry in rerank_sweep])\n",
    "sweep_mrrs = np.array([entry[\"mrr\"] for entry in rerank_sweep])\n",
    "\n",
    "optimal = find_optimal_threshold(rerank_sweep, metric=\"f1\")\n",
    "best_f1_index = optimal[\"index\"]\n",
    "best_threshold = optimal[\"threshold\"]\n",
    "\n",
    "# --- Figure: P/R/F1/MRR vs rerank score threshold ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(sweep_thresholds_list, sweep_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\n",
    "ax.plot(sweep_thresholds_list, sweep_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\n",
    "ax.plot(sweep_thresholds_list, sweep_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\n",
    "ax.plot(sweep_thresholds_list, sweep_mrrs, label=\"MRR\", color=\"#e67e22\", linewidth=2)\n",
    "ax.axvline(\n",
    "    best_threshold,\n",
    "    color=\"#e74c3c\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Best F1 @ {best_threshold:.3f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Rerank Score Threshold (accept >= threshold)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"P/R/F1/MRR vs Rerank Score Threshold (macro-averaged)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig,\n",
    "    FIGURE_SESSION,\n",
    "    \"rerank_threshold_sweep\",\n",
    "    title=\"P/R/F1/MRR vs Rerank Score Threshold\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "print(f\"\\nOptimal F1 threshold: {best_threshold:.4f}\")\n",
    "print(f\"  F1:        {sweep_f1_scores[best_f1_index]:.3f}\")\n",
    "print(f\"  Precision: {sweep_precisions[best_f1_index]:.3f}\")\n",
    "print(f\"  Recall:    {sweep_recalls[best_f1_index]:.3f}\")\n",
    "print(f\"  MRR:       {sweep_mrrs[best_f1_index]:.3f}\")\n",
    "\n",
    "# Threshold table\n",
    "print(\"\\nThreshold table (macro-averaged):\")\n",
    "print(\n",
    "    f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Pick representative thresholds around the interesting range\n",
    "table_thresholds = sorted(\n",
    "    set(\n",
    "        [\n",
    "            0.001,\n",
    "            0.005,\n",
    "            0.01,\n",
    "            0.02,\n",
    "            0.03,\n",
    "            0.05,\n",
    "            0.08,\n",
    "            0.10,\n",
    "            0.15,\n",
    "            0.20,\n",
    "            0.30,\n",
    "            0.50,\n",
    "            round(best_threshold, 4),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "for threshold in table_thresholds:\n",
    "    index = min(\n",
    "        range(len(sweep_thresholds_list)),\n",
    "        key=lambda idx: abs(sweep_thresholds_list[idx] - threshold),\n",
    "    )\n",
    "    sweep_entry = rerank_sweep[index]\n",
    "    avg_accepted = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                [\n",
    "                    result\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"rerank_score\"] >= threshold\n",
    "                ]\n",
    "            )\n",
    "            for experiment_result in rerank_experiments\n",
    "        ]\n",
    "    )\n",
    "    avg_ground_truth_kept = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                {\n",
    "                    result[\"id\"]\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"rerank_score\"] >= threshold\n",
    "                }\n",
    "                & experiment_result[\"ground_truth_ids\"]\n",
    "            )\n",
    "            for experiment_result in rerank_experiments\n",
    "        ]\n",
    "    )\n",
    "    marker = \" <--\" if abs(threshold - best_threshold) < 0.003 else \"\"\n",
    "    print(\n",
    "        f\"{threshold:>10.4f} {sweep_entry['precision']:>10.3f} {sweep_entry['recall']:>8.3f} {sweep_entry['f1']:>8.3f} {sweep_entry['mrr']:>8.3f} {avg_accepted:>13.1f} {avg_ground_truth_kept:>12.1f}{marker}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 5c — Per-Experiment Impact at Optimal Rerank Threshold\n",
    "\n",
    "Show how the optimal rerank score threshold performs on each test case.\n",
    "Compare with Phase 1 distance threshold results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16 — Per-Experiment Impact at Optimal Rerank Threshold\n",
    "import numpy as np\n",
    "\n",
    "# Phase 1 optimal threshold from threshold analysis notebook\n",
    "PHASE1_OPTIMAL_THRESHOLD = 0.76  # distance threshold\n",
    "\n",
    "print(\"Per-experiment comparison at optimal thresholds:\")\n",
    "print(f\"  Phase 1: distance <= {PHASE1_OPTIMAL_THRESHOLD}\")\n",
    "print(f\"  Rerank:  score >= {best_threshold:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    f\"{'Test Case':<20} {'P1 F1':>7} {'Rk F1':>7} {'Delta':>7} {'P1 P':>6} {'Rk P':>6} {'P1 R':>6} {'Rk R':>6} {'P1 Acc':>7} {'Rk Acc':>7} {'GT':>4}\"\n",
    ")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "phase1_f1_scores, rerank_f1_list = [], []\n",
    "for experiment_result in rerank_experiments:\n",
    "    test_case_id = experiment_result[\"test_case_id\"]\n",
    "    ground_truth_ids = experiment_result[\"ground_truth_ids\"]\n",
    "    ground_truth_count = len(ground_truth_ids)\n",
    "\n",
    "    # Phase 1: distance threshold (pool + dedup from phase1 data)\n",
    "    phase1_data = phase1_by_test_case.get(test_case_id, {})\n",
    "    phase1_pooled = pool_and_deduplicate_by_distance(phase1_data.get(\"queries\", []))\n",
    "    phase1_metrics = compute_metrics_at_threshold(\n",
    "        phase1_pooled,\n",
    "        ground_truth_ids,\n",
    "        PHASE1_OPTIMAL_THRESHOLD,\n",
    "        score_field=\"distance\",\n",
    "        higher_is_better=False,\n",
    "    )\n",
    "\n",
    "    # Rerank: score threshold\n",
    "    rerank_metrics = compute_metrics_at_threshold(\n",
    "        experiment_result[\"reranked\"],\n",
    "        ground_truth_ids,\n",
    "        best_threshold,\n",
    "        score_field=\"rerank_score\",\n",
    "        higher_is_better=True,\n",
    "    )\n",
    "\n",
    "    delta = rerank_metrics[\"f1\"] - phase1_metrics[\"f1\"]\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "\n",
    "    phase1_f1_scores.append(phase1_metrics[\"f1\"])\n",
    "    rerank_f1_list.append(rerank_metrics[\"f1\"])\n",
    "\n",
    "    print(\n",
    "        f\"{test_case_id:<20} {phase1_metrics['f1']:>7.3f} {rerank_metrics['f1']:>7.3f} {delta:>+6.3f}{marker} {phase1_metrics['precision']:>6.1%} {rerank_metrics['precision']:>6.1%} {phase1_metrics['recall']:>6.1%} {rerank_metrics['recall']:>6.1%} {phase1_metrics['accepted_count']:>7} {rerank_metrics['accepted_count']:>7} {ground_truth_count:>4}\"\n",
    "    )\n",
    "\n",
    "    # Show missed GT for rerank\n",
    "    rerank_missed = ground_truth_ids - rerank_metrics[\"retrieved_ids\"]\n",
    "    if rerank_missed:\n",
    "        for memory_id in sorted(rerank_missed):\n",
    "            # Find rerank score for this missed memory\n",
    "            score = next(\n",
    "                (\n",
    "                    result[\"rerank_score\"]\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"id\"] == memory_id\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            score_str = f\"score={score:.4f}\" if score is not None else \"NOT IN POOL\"\n",
    "            print(f\"  {'':20} Missed: {memory_id} ({score_str})\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "avg_phase1 = np.mean(phase1_f1_scores)\n",
    "avg_rerank = np.mean(rerank_f1_list)\n",
    "delta = avg_rerank - avg_phase1\n",
    "print(\n",
    "    f\"{'AVERAGE':<20} {avg_phase1:>7.3f} {avg_rerank:>7.3f} {delta:>+6.3f}{'+' if delta > 0 else '-' if delta < 0 else '='}\"\n",
    ")\n",
    "\n",
    "improved = sum(\n",
    "    1\n",
    "    for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_list)\n",
    "    if rerank_f1 > phase1_f1 + 0.001\n",
    ")\n",
    "same = sum(\n",
    "    1\n",
    "    for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_list)\n",
    "    if abs(rerank_f1 - phase1_f1) <= 0.001\n",
    ")\n",
    "worse = sum(\n",
    "    1\n",
    "    for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_list)\n",
    "    if rerank_f1 < phase1_f1 - 0.001\n",
    ")\n",
    "print(\n",
    "    f\"\\nReranking helped: {improved}/{len(phase1_f1_scores)} | Same: {same}/{len(phase1_f1_scores)} | Hurt: {worse}/{len(phase1_f1_scores)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17 — Final Summary\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RERANKING COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nPhase 1 run: {PHASE1_RUN.name}\")\n",
    "print(f\"Test cases: {len(experiments_reranked)}\")\n",
    "print(f\"Reranker: {reranker.model_name}\")\n",
    "\n",
    "# Top-N comparison\n",
    "print(f\"\\n--- Top-{ANALYSIS_TOP_N} Comparison (distance vs rerank) ---\")\n",
    "avg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in all_results) / len(\n",
    "    all_results\n",
    ")\n",
    "rerank_f1_scores = []\n",
    "for result in all_results:\n",
    "    ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "    rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "    rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "avg_rerank_f1 = sum(rerank_f1_scores) / len(rerank_f1_scores)\n",
    "\n",
    "print(f\"  Distance top-{ANALYSIS_TOP_N} avg F1: {avg_distance_f1:.3f}\")\n",
    "print(f\"  Rerank top-{ANALYSIS_TOP_N} avg F1:   {avg_rerank_f1:.3f}\")\n",
    "print(f\"  Delta: {avg_rerank_f1 - avg_distance_f1:+.3f}\")\n",
    "\n",
    "# Threshold comparison\n",
    "print(\"\\n--- Threshold Comparison (Phase 1 distance vs rerank score) ---\")\n",
    "print(f\"  Phase 1 distance threshold {PHASE1_OPTIMAL_THRESHOLD}: avg F1 = {avg_phase1:.3f}\")\n",
    "print(f\"  Rerank score threshold {best_threshold:.4f}: avg F1 = {avg_rerank:.3f}\")\n",
    "print(\n",
    "    f\"  Delta: {avg_rerank - avg_phase1:+.3f} ({(avg_rerank - avg_phase1) / avg_phase1 * 100:+.1f}%)\"\n",
    ")\n",
    "\n",
    "# Optimal rerank threshold details\n",
    "print(\"\\n--- Optimal Rerank Score Threshold ---\")\n",
    "print(f\"  Threshold: {best_threshold:.4f}\")\n",
    "print(f\"  F1:        {sweep_f1_scores[best_f1_index]:.3f}\")\n",
    "print(f\"  Precision: {sweep_precisions[best_f1_index]:.3f}\")\n",
    "print(f\"  Recall:    {sweep_recalls[best_f1_index]:.3f}\")\n",
    "print(f\"  MRR:       {sweep_mrrs[best_f1_index]:.3f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
