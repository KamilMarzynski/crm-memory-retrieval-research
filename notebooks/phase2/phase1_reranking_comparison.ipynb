{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 1 Reranking Comparison\n",
    "\n",
    "Applies cross-encoder reranking on top of **existing Phase 1 experiment results**.\n",
    "Reuses the exact same queries, vector search candidates, and ground truth from the Phase 1 run.\n",
    "\n",
    "This notebook does **not** generate new queries or run new searches — it loads the saved\n",
    "Phase 1 results and only adds the reranking step to measure its isolated impact on retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from memory_retrieval.experiments.metrics import compute_metrics\n",
    "from memory_retrieval.infra.io import load_json, save_json\n",
    "from memory_retrieval.infra.runs import (\n",
    "    PHASE1,\n",
    "    PHASE2,\n",
    "    create_run,\n",
    "    get_latest_run,\n",
    "    update_run_status,\n",
    ")\n",
    "from memory_retrieval.memories.schema import FIELD_DISTANCE, FIELD_RERANK_SCORE, FIELD_SITUATION\n",
    "from memory_retrieval.search.reranker import Reranker\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration\n",
    "\n",
    "# Analysis parameter for top-N comparison tables\n",
    "ANALYSIS_TOP_N = 4\n",
    "DISTANCE_THRESHOLD = 1.1  # For pre-rerank metrics (must match Phase 1)\n",
    "\n",
    "# Phase 1 run selection (source of results, DB, and test cases)\n",
    "# To see available runs: print(list_runs(PHASE1))\n",
    "# To select specific run: PHASE1_RUN = get_run(PHASE1, \"run_20260208_143022\")\n",
    "PHASE1_RUN = get_latest_run(PHASE1)\n",
    "\n",
    "# Derived paths from Phase 1\n",
    "PHASE1_RESULTS_DIR = PHASE1_RUN / \"results\"\n",
    "phase1_result_files = sorted(PHASE1_RESULTS_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Phase 1 run: {PHASE1_RUN.name}\")\n",
    "print(f\"  Phase 1 results dir: {PHASE1_RESULTS_DIR}\")\n",
    "print(f\"  Phase 1 result files: {len(phase1_result_files)}\")\n",
    "print(f\"  Analysis top-n: {ANALYSIS_TOP_N}\")\n",
    "print(f\"  Distance threshold: {DISTANCE_THRESHOLD}\")\n",
    "\n",
    "if not phase1_result_files:\n",
    "    print(\"\\nERROR: No Phase 1 result files found. Run phase1.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1 — Load Phase 1 Results & Initialize Reranker\n",
    "\n",
    "Load existing Phase 1 experiment results (queries + vector search candidates).\n",
    "Initialize the cross-encoder reranker (`bge-reranker-v2-m3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Load Phase 1 Results & Initialize Reranker\n",
    "\n",
    "# Load all Phase 1 experiment results\n",
    "# When multiple results exist per test case, keep the latest experiment\n",
    "phase1_by_test_case: dict[str, dict] = {}\n",
    "for result_file in phase1_result_files:\n",
    "    data = load_json(str(result_file))\n",
    "    test_case_id = data.get(\"test_case_id\", \"?\")\n",
    "    if test_case_id not in phase1_by_test_case or data.get(\n",
    "        \"experiment_id\", \"\"\n",
    "    ) > phase1_by_test_case[test_case_id].get(\"experiment_id\", \"\"):\n",
    "        phase1_by_test_case[test_case_id] = data\n",
    "\n",
    "print(f\"Loaded {len(phase1_by_test_case)} unique test case results from Phase 1\\n\")\n",
    "for test_case_id, data in sorted(phase1_by_test_case.items()):\n",
    "    ground_truth_count = data.get(\"ground_truth\", {}).get(\"count\", 0)\n",
    "    metrics = data.get(\"metrics\", {})\n",
    "    num_queries = len(data.get(\"queries\", []))\n",
    "    print(\n",
    "        f\"  {test_case_id}: {num_queries} queries, {ground_truth_count} GT memories, F1={metrics.get('f1', 0):.3f}\"\n",
    "    )\n",
    "\n",
    "# Initialize reranker\n",
    "reranker = Reranker()\n",
    "\n",
    "# Quick test to load the model\n",
    "print(\"\\nLoading reranker model (first use triggers download)...\")\n",
    "_ = reranker.score_pairs(\"test\", [\"test document\"])\n",
    "print(\"Reranker ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2 — Apply Reranking to Phase 1 Results\n",
    "\n",
    "For each Phase 1 test case result:\n",
    "1. Pool and deduplicate vector search candidates from all queries\n",
    "2. Compute **top-N by vector distance** baseline (apples-to-apples with reranking)\n",
    "3. Rerank candidates with cross-encoder, take top-N\n",
    "4. Compare both at the same N — the only difference is ranking method\n",
    "\n",
    "No new LLM calls or vector searches — everything is reused from Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Apply Reranking to Phase 1 Results\n",
    "\n",
    "\n",
    "def pool_and_deduplicate(query_results: list[dict]) -> list[dict]:\n",
    "    \"\"\"Pool results from all queries and deduplicate by memory ID (keep best distance).\"\"\"\n",
    "    best_by_memory_id: dict[str, dict] = {}\n",
    "    for query_result in query_results:\n",
    "        for result in query_result.get(\"results\", []):\n",
    "            memory_id = result[\"id\"]\n",
    "            distance = result.get(\"distance\", float(\"inf\"))\n",
    "            if memory_id not in best_by_memory_id or distance < best_by_memory_id[memory_id].get(\n",
    "                \"distance\", float(\"inf\")\n",
    "            ):\n",
    "                best_by_memory_id[memory_id] = result\n",
    "    return sorted(best_by_memory_id.values(), key=lambda x: x.get(\"distance\", 0))\n",
    "\n",
    "\n",
    "# Create Phase 2 run for results\n",
    "run_id, PHASE2_RUN = create_run(\n",
    "    PHASE2,\n",
    "    description=f\"Reranking on phase1 results ({PHASE1_RUN.name})\",\n",
    ")\n",
    "RESULTS_DIR = str(PHASE2_RUN / \"results\")\n",
    "\n",
    "update_run_status(\n",
    "    PHASE2_RUN,\n",
    "    \"config\",\n",
    "    {\n",
    "        \"phase1_run_id\": PHASE1_RUN.name,\n",
    "        \"reranker_model\": reranker.model_name,\n",
    "        \"analysis_top_n\": ANALYSIS_TOP_N,\n",
    "        \"distance_threshold\": DISTANCE_THRESHOLD,\n",
    "        \"mode\": \"reuse_phase1_results\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Phase 2 run: {run_id}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\\n\")\n",
    "\n",
    "all_results: list[dict] = []\n",
    "\n",
    "# Iterate over all results from phase1\n",
    "for i, (test_case_id, phase1_data) in enumerate(sorted(phase1_by_test_case.items()), 1):\n",
    "    print(f\"[{i}/{len(phase1_by_test_case)}] {test_case_id}\")\n",
    "\n",
    "    ground_truth_ids = set(phase1_data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    query_results = phase1_data.get(\"queries\", [])\n",
    "\n",
    "    # Pool and deduplicate across all queries (sorted by distance, best first)\n",
    "    pooled = pool_and_deduplicate(query_results)\n",
    "    print(f\"  Deduplicated memories count: {len(pooled)}\")\n",
    "\n",
    "    # --- Baseline: top-N by vector distance (same N as analysis) ---\n",
    "    top_n_by_distance = pooled[:ANALYSIS_TOP_N]\n",
    "    distance_top_n_ids = {result[\"id\"] for result in top_n_by_distance}\n",
    "    distance_top_n_metrics = compute_metrics(distance_top_n_ids, ground_truth_ids)\n",
    "\n",
    "    # --- Per-query reranking: rerank each query's results independently ---\n",
    "    all_reranked_per_query = []\n",
    "    # Iterate over all queries\n",
    "    for query_result in query_results:\n",
    "        candidates = [\n",
    "            {\n",
    "                \"id\": result[\"id\"],\n",
    "                FIELD_SITUATION: result.get(\"situation\", result.get(FIELD_SITUATION, \"\")),\n",
    "                FIELD_DISTANCE: result.get(\"distance\", 0),\n",
    "                \"is_ground_truth\": result.get(\"is_ground_truth\", result[\"id\"] in ground_truth_ids),\n",
    "            }\n",
    "            # Iterate over all results for this query\n",
    "            for result in query_result.get(\"results\", [])\n",
    "        ]\n",
    "        # Simple reranking, just using \"situation\" description - pass query string of given query result with all its results array\n",
    "        # mapped to candidates\n",
    "        reranked = reranker.rerank(query_result[\"query\"], candidates, top_n=None)\n",
    "        all_reranked_per_query.extend(reranked)\n",
    "\n",
    "    # Deduplicate by best rerank score - drop information about original query string\n",
    "    best_by_memory_id: dict[str, dict] = {}\n",
    "    for result in all_reranked_per_query:\n",
    "        memory_id = result[\"id\"]\n",
    "        if (\n",
    "            memory_id not in best_by_memory_id\n",
    "            or result[FIELD_RERANK_SCORE] > best_by_memory_id[memory_id][FIELD_RERANK_SCORE]\n",
    "        ):\n",
    "            best_by_memory_id[memory_id] = result\n",
    "    all_reranked = sorted(\n",
    "        best_by_memory_id.values(), key=lambda x: x[FIELD_RERANK_SCORE], reverse=True\n",
    "    )\n",
    "\n",
    "    # Compute top-N metrics for display\n",
    "    rerank_top_n_ids = {result[\"id\"] for result in all_reranked[:ANALYSIS_TOP_N]}\n",
    "    rerank_top_n_metrics = compute_metrics(rerank_top_n_ids, ground_truth_ids)\n",
    "\n",
    "    f1_delta = rerank_top_n_metrics[\"f1\"] - distance_top_n_metrics[\"f1\"]\n",
    "    marker = \"+\" if f1_delta > 0 else \"\"\n",
    "    print(\n",
    "        f\"  Top-{ANALYSIS_TOP_N} by distance F1={distance_top_n_metrics['f1']:.3f} | Top-{ANALYSIS_TOP_N} by rerank F1={rerank_top_n_metrics['f1']:.3f} ({marker}{f1_delta:.3f})\"\n",
    "    )\n",
    "\n",
    "    # Build result\n",
    "    result = {\n",
    "        \"test_case_id\": test_case_id,\n",
    "        \"source_file\": phase1_data.get(\"source_file\", \"unknown\"),\n",
    "        \"phase1_experiment_id\": phase1_data.get(\"experiment_id\", \"unknown\"),\n",
    "        \"model\": phase1_data.get(\"model\", \"unknown\"),\n",
    "        \"prompt_version\": phase1_data.get(\"prompt_version\", \"unknown\"),\n",
    "        \"reranker_model\": reranker.model_name,\n",
    "        \"rerank_queries\": [query_result[\"query\"] for query_result in query_results],\n",
    "        \"distance_threshold\": DISTANCE_THRESHOLD,\n",
    "        \"ground_truth\": phase1_data.get(\"ground_truth\", {}),\n",
    "        \"queries\": query_results,\n",
    "        \"pooled_candidate_count\": len(pooled),\n",
    "        \"distance_top_n_metrics\": {\n",
    "            **distance_top_n_metrics,\n",
    "            \"n\": ANALYSIS_TOP_N,\n",
    "            \"ground_truth_retrieved\": len(distance_top_n_ids & ground_truth_ids),\n",
    "        },\n",
    "        \"reranked_results\": [\n",
    "            {\n",
    "                \"id\": result[\"id\"],\n",
    "                \"rerank_score\": result[FIELD_RERANK_SCORE],\n",
    "                \"distance\": result.get(FIELD_DISTANCE, 0),\n",
    "                \"situation\": result.get(FIELD_SITUATION, \"\"),\n",
    "                \"is_ground_truth\": result[\"id\"] in ground_truth_ids,\n",
    "            }\n",
    "            for result in all_reranked  # Store ALL reranked (for sweep analysis)\n",
    "        ],\n",
    "        \"distance_top_n_results\": [\n",
    "            {\n",
    "                \"id\": result[\"id\"],\n",
    "                \"distance\": result.get(\"distance\", 0),\n",
    "                \"situation\": result.get(\"situation\", result.get(FIELD_SITUATION, \"\")),\n",
    "                \"is_ground_truth\": result.get(\"is_ground_truth\", result[\"id\"] in ground_truth_ids),\n",
    "            }\n",
    "            for result in top_n_by_distance\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    save_json(result, Path(RESULTS_DIR) / f\"rerank_{test_case_id}.json\")\n",
    "    all_results.append(result)\n",
    "\n",
    "# Summary\n",
    "successful = [result for result in all_results if \"reranked_results\" in result]\n",
    "if successful:\n",
    "    avg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in successful) / len(\n",
    "        successful\n",
    "    )\n",
    "\n",
    "    # Compute rerank top-N metrics from stored reranked_results\n",
    "    rerank_f1_scores = []\n",
    "    for result in successful:\n",
    "        ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "        rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "        rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "    avg_rerank_f1 = sum(rerank_f1_scores) / len(rerank_f1_scores)\n",
    "\n",
    "    print(f\"\\n{'=' * 55}\")\n",
    "    print(f\"SUMMARY ({len(successful)} test cases, top-{ANALYSIS_TOP_N})\")\n",
    "    print(f\"{'=' * 55}\")\n",
    "    print(f\"  Avg distance top-{ANALYSIS_TOP_N} F1: {avg_distance_f1:.3f}\")\n",
    "    print(f\"  Avg rerank top-{ANALYSIS_TOP_N} F1:   {avg_rerank_f1:.3f}\")\n",
    "    print(f\"  Delta: {avg_rerank_f1 - avg_distance_f1:+.3f}\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    PHASE2_RUN,\n",
    "    \"experiment\",\n",
    "    {\n",
    "        \"count\": len(successful),\n",
    "        \"avg_f1_distance_top_n\": round(avg_distance_f1, 4) if successful else 0,\n",
    "        \"avg_f1_rerank_top_n\": round(avg_rerank_f1, 4) if successful else 0,\n",
    "        \"analysis_top_n\": ANALYSIS_TOP_N,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3 — Results Summary Table\n",
    "\n",
    "Per-test-case comparison: **top-N by distance** vs **top-N by rerank score** (same N, same candidates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Results Summary Table\n",
    "\n",
    "successful = [result for result in all_results if \"reranked_results\" in result]\n",
    "\n",
    "print(f\"Top-{ANALYSIS_TOP_N} comparison: vector distance vs cross-encoder reranking\\n\")\n",
    "print(\n",
    "    f\"{'Test Case':<25} {'Dist F1':>8} {'Rank F1':>8} {'Delta':>8} {'Dist P':>7} {'Rank P':>7} {'Dist R':>7} {'Rank R':>7}\"\n",
    ")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for result in successful:\n",
    "    name = result.get(\"test_case_id\", \"?\")[:25]\n",
    "    distance_metrics = result[\"distance_top_n_metrics\"]\n",
    "    ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "\n",
    "    # Compute rerank top-N metrics from reranked_results\n",
    "    rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "    rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "\n",
    "    delta = rerank_metrics[\"f1\"] - distance_metrics[\"f1\"]\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "    print(\n",
    "        f\"{name:<25} {distance_metrics['f1']:>8.3f} {rerank_metrics['f1']:>8.3f} {delta:>+7.3f}{marker} {distance_metrics['precision']:>7.3f} {rerank_metrics['precision']:>7.3f} {distance_metrics['recall']:>7.3f} {rerank_metrics['recall']:>7.3f}\"\n",
    "    )\n",
    "\n",
    "if successful:\n",
    "    avg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in successful) / len(\n",
    "        successful\n",
    "    )\n",
    "    avg_distance_precision = sum(\n",
    "        result[\"distance_top_n_metrics\"][\"precision\"] for result in successful\n",
    "    ) / len(successful)\n",
    "    avg_distance_recall = sum(\n",
    "        result[\"distance_top_n_metrics\"][\"recall\"] for result in successful\n",
    "    ) / len(successful)\n",
    "\n",
    "    rerank_f1_scores, rerank_precisions, rerank_recalls = [], [], []\n",
    "    for result in successful:\n",
    "        ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "        rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "        rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "        rerank_precisions.append(rerank_metrics[\"precision\"])\n",
    "        rerank_recalls.append(rerank_metrics[\"recall\"])\n",
    "\n",
    "    avg_rerank_f1 = sum(rerank_f1_scores) / len(rerank_f1_scores)\n",
    "    avg_rerank_precision = sum(rerank_precisions) / len(rerank_precisions)\n",
    "    avg_rerank_recall = sum(rerank_recalls) / len(rerank_recalls)\n",
    "\n",
    "    delta = avg_rerank_f1 - avg_distance_f1\n",
    "    marker = \"+\" if delta > 0 else \"-\" if delta < 0 else \"=\"\n",
    "    print(\"-\" * 85)\n",
    "    print(\n",
    "        f\"{'AVERAGE':<25} {avg_distance_f1:>8.3f} {avg_rerank_f1:>8.3f} {delta:>+7.3f}{marker} {avg_distance_precision:>7.3f} {avg_rerank_precision:>7.3f} {avg_distance_recall:>7.3f} {avg_rerank_recall:>7.3f}\"\n",
    "    )\n",
    "\n",
    "    improved = sum(\n",
    "        1\n",
    "        for result, rerank_f1 in zip(successful, rerank_f1_scores)\n",
    "        if rerank_f1 > result[\"distance_top_n_metrics\"][\"f1\"] + 0.001\n",
    "    )\n",
    "    same = sum(\n",
    "        1\n",
    "        for result, rerank_f1 in zip(successful, rerank_f1_scores)\n",
    "        if abs(rerank_f1 - result[\"distance_top_n_metrics\"][\"f1\"]) <= 0.001\n",
    "    )\n",
    "    worse = sum(\n",
    "        1\n",
    "        for result, rerank_f1 in zip(successful, rerank_f1_scores)\n",
    "        if rerank_f1 < result[\"distance_top_n_metrics\"][\"f1\"] - 0.001\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nReranking helped: {improved}/{len(successful)} | Same: {same}/{len(successful)} | Hurt: {worse}/{len(successful)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4 — Top-N Sweep: Distance vs Reranking\n",
    "\n",
    "Compare top-N by vector distance vs top-N by rerank score across different N values.\n",
    "Shows whether reranking improves ranking quality at every cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Top-N Sweep: Distance vs Reranking\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FIGURES_DIR = PHASE2_RUN / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_data = all_results\n",
    "num_test_cases = len(all_data)\n",
    "\n",
    "if not all_data:\n",
    "    print(\"No results found. Run Step 2 first.\")\n",
    "else:\n",
    "    max_n = 20\n",
    "    n_values = list(range(1, max_n + 1))\n",
    "\n",
    "    # Sweep for both distance-based and rerank-based top-N\n",
    "    distance_f1_scores, rerank_f1_scores = [], []\n",
    "    distance_precisions, rerank_precisions = [], []\n",
    "    distance_recalls, rerank_recalls = [], []\n",
    "\n",
    "    for top_n in n_values:\n",
    "        distance_f1_list, distance_precision_list, distance_recall_list = [], [], []\n",
    "        rerank_f1_list, rerank_precision_list, rerank_recall_list = [], [], []\n",
    "\n",
    "        for data in all_data:\n",
    "            ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "            ground_truth_count = len(ground_truth_ids)\n",
    "\n",
    "            # Distance-based top-N (pooled is sorted by distance in queries)\n",
    "            pooled = pool_and_deduplicate(data.get(\"queries\", []))\n",
    "            distance_top_n_ids = {result[\"id\"] for result in pooled[:top_n]}\n",
    "            distance_hits = len(distance_top_n_ids & ground_truth_ids)\n",
    "            distance_count = len(distance_top_n_ids)\n",
    "\n",
    "            # Rerank-based top-N (reranked_results sorted by rerank score)\n",
    "            reranked = data.get(\"reranked_results\", [])\n",
    "            rerank_top_n_ids = {result[\"id\"] for result in reranked[:top_n]}\n",
    "            rerank_hits = len(rerank_top_n_ids & ground_truth_ids)\n",
    "            rerank_count = len(rerank_top_n_ids)\n",
    "\n",
    "            # Distance metrics\n",
    "            precision = distance_hits / distance_count if distance_count > 0 else 0.0\n",
    "            recall = distance_hits / ground_truth_count if ground_truth_count > 0 else 0.0\n",
    "            f1_score = (\n",
    "                2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            )\n",
    "            distance_precision_list.append(precision)\n",
    "            distance_recall_list.append(recall)\n",
    "            distance_f1_list.append(f1_score)\n",
    "\n",
    "            # Rerank metrics\n",
    "            precision = rerank_hits / rerank_count if rerank_count > 0 else 0.0\n",
    "            recall = rerank_hits / ground_truth_count if ground_truth_count > 0 else 0.0\n",
    "            f1_score = (\n",
    "                2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            )\n",
    "            rerank_precision_list.append(precision)\n",
    "            rerank_recall_list.append(recall)\n",
    "            rerank_f1_list.append(f1_score)\n",
    "\n",
    "        distance_f1_scores.append(np.mean(distance_f1_list))\n",
    "        distance_precisions.append(np.mean(distance_precision_list))\n",
    "        distance_recalls.append(np.mean(distance_recall_list))\n",
    "        rerank_f1_scores.append(np.mean(rerank_f1_list))\n",
    "        rerank_precisions.append(np.mean(rerank_precision_list))\n",
    "        rerank_recalls.append(np.mean(rerank_recall_list))\n",
    "\n",
    "    # Print table\n",
    "    print(f\"Top-N Sweep: Distance vs Reranking (averaged over {num_test_cases} test cases)\\n\")\n",
    "    print(\n",
    "        f\"{'N':>4} {'Dist P':>8} {'Rank P':>8} {'Dist R':>8} {'Rank R':>8} {'Dist F1':>8} {'Rank F1':>8} {'F1 Delta':>9}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "    for top_n in [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]:\n",
    "        if top_n <= max_n:\n",
    "            index = top_n - 1\n",
    "            delta = rerank_f1_scores[index] - distance_f1_scores[index]\n",
    "            print(\n",
    "                f\"{top_n:>4} {distance_precisions[index]:>8.3f} {rerank_precisions[index]:>8.3f} {distance_recalls[index]:>8.3f} {rerank_recalls[index]:>8.3f} {distance_f1_scores[index]:>8.3f} {rerank_f1_scores[index]:>8.3f} {delta:>+9.3f}\"\n",
    "            )\n",
    "\n",
    "    # Plot: F1 comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(\n",
    "        n_values,\n",
    "        distance_f1_scores,\n",
    "        label=\"F1 (distance)\",\n",
    "        color=\"#3498db\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.plot(\n",
    "        n_values,\n",
    "        rerank_f1_scores,\n",
    "        label=\"F1 (reranked)\",\n",
    "        color=\"#e74c3c\",\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.axvline(\n",
    "        x=ANALYSIS_TOP_N,\n",
    "        color=\"gray\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.5,\n",
    "        label=f\"Default N={ANALYSIS_TOP_N}\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Top-N\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "    ax.set_title(f\"F1: Distance vs Reranking (avg over {num_test_cases} test cases)\")\n",
    "    ax.set_xticks(n_values)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot: F1 delta\n",
    "    ax = axes[1]\n",
    "    f1_deltas = [rerank_f1_scores[i] - distance_f1_scores[i] for i in range(max_n)]\n",
    "    colors = [\"#2ecc71\" if delta > 0 else \"#e74c3c\" for delta in f1_deltas]\n",
    "    ax.bar(n_values, f1_deltas, color=colors, alpha=0.7)\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"Top-N\")\n",
    "    ax.set_ylabel(\"F1 Delta (rerank - distance)\")\n",
    "    ax.set_title(\"Reranking F1 Improvement by N\")\n",
    "    ax.set_xticks(n_values)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / \"rerank_topn_sweep.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'rerank_topn_sweep.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5 — Rerank Score Threshold Analysis\n",
    "\n",
    "Mirrors the Phase 1 distance threshold analysis, but uses **rerank scores** as the filtering signal.\n",
    "Instead of a fixed top-N cutoff, we sweep rerank score thresholds to find the optimal cutoff.\n",
    "\n",
    "All metrics are **macro-averaged**: computed per test case, then averaged.\n",
    "Within each test case, candidates are deduplicated by memory ID (best distance), then reranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Rerank Score Distribution & Threshold Sweep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FIGURES_DIR = PHASE2_RUN / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect per-observation rerank scores (one entry per test_case x memory pair)\n",
    "ground_truth_scores_all = []  # (score, test_case_id, memory_id)\n",
    "non_ground_truth_scores_all = []  # (score, test_case_id, memory_id)\n",
    "\n",
    "# Build per-experiment deduped rerank data (mirrors phase1 threshold analysis structure)\n",
    "experiments_reranked = []\n",
    "for data in all_data:\n",
    "    test_case_id = data[\"test_case_id\"]\n",
    "    ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    # reranked_results contains ALL candidates sorted by rerank score\n",
    "    reranked = data.get(\"reranked_results\", [])\n",
    "    scores_by_id = {result[\"id\"]: result for result in reranked}\n",
    "\n",
    "    experiments_reranked.append(\n",
    "        {\n",
    "            \"test_case_id\": test_case_id,\n",
    "            \"ground_truth_ids\": ground_truth_ids,\n",
    "            \"scores_by_id\": scores_by_id,\n",
    "            \"reranked\": reranked,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for result in reranked:\n",
    "        entry = (result[\"rerank_score\"], test_case_id, result[\"id\"])\n",
    "        if result.get(\"is_ground_truth\"):\n",
    "            ground_truth_scores_all.append(entry)\n",
    "        else:\n",
    "            non_ground_truth_scores_all.append(entry)\n",
    "\n",
    "ground_truth_scores = np.array([score for score, _, _ in ground_truth_scores_all])\n",
    "non_ground_truth_scores = np.array([score for score, _, _ in non_ground_truth_scores_all])\n",
    "\n",
    "print(f\"Experiments: {len(experiments_reranked)} test cases\")\n",
    "print(\n",
    "    f\"GT observations:     {len(ground_truth_scores)} (unique — each memory is GT in exactly 1 test case)\"\n",
    ")\n",
    "print(f\"Non-GT observations: {len(non_ground_truth_scores)}\")\n",
    "print()\n",
    "print(\n",
    "    f\"GT rerank score range:     [{ground_truth_scores.min():.4f}, {ground_truth_scores.max():.4f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"Non-GT rerank score range: [{non_ground_truth_scores.min():.4f}, {non_ground_truth_scores.max():.4f}]\"\n",
    ")\n",
    "print(f\"GT mean: {ground_truth_scores.mean():.4f}, median: {np.median(ground_truth_scores):.4f}\")\n",
    "print(\n",
    "    f\"Non-GT mean: {non_ground_truth_scores.mean():.4f}, median: {np.median(non_ground_truth_scores):.4f}\"\n",
    ")\n",
    "print(f\"Mean separation: {ground_truth_scores.mean() - non_ground_truth_scores.mean():.4f}\")\n",
    "\n",
    "# --- Figure 1: Score distribution histogram ---\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 5))\n",
    "all_scores_combined = np.concatenate([ground_truth_scores, non_ground_truth_scores])\n",
    "bins = np.linspace(all_scores_combined.min(), all_scores_combined.max(), 40)\n",
    "ax1.hist(\n",
    "    ground_truth_scores,\n",
    "    bins=bins,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=f\"GT (n={len(ground_truth_scores)})\",\n",
    "    color=\"#2ecc71\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax1.hist(\n",
    "    non_ground_truth_scores,\n",
    "    bins=bins,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=f\"Non-GT (n={len(non_ground_truth_scores)})\",\n",
    "    color=\"#e74c3c\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax1.axvline(\n",
    "    np.median(ground_truth_scores),\n",
    "    color=\"#27ae60\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"GT median: {np.median(ground_truth_scores):.4f}\",\n",
    ")\n",
    "ax1.axvline(\n",
    "    np.median(non_ground_truth_scores),\n",
    "    color=\"#c0392b\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Non-GT median: {np.median(non_ground_truth_scores):.4f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Rerank Score\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"Rerank Score Distribution (normalized)\")\n",
    "ax1.legend(fontsize=8)\n",
    "fig1.tight_layout()\n",
    "fig1.savefig(FIGURES_DIR / \"rerank_score_distribution.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'rerank_score_distribution.png'}\")\n",
    "\n",
    "# --- Rerank score vs distance scatter ---\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "for data in all_data:\n",
    "    for result in data.get(\"reranked_results\", []):\n",
    "        color = \"#2ecc71\" if result.get(\"is_ground_truth\") else \"#e74c3c\"\n",
    "        alpha = 0.7 if result.get(\"is_ground_truth\") else 0.3\n",
    "        size = 60 if result.get(\"is_ground_truth\") else 25\n",
    "        ax2.scatter(\n",
    "            result[\"distance\"],\n",
    "            result[\"rerank_score\"],\n",
    "            c=color,\n",
    "            alpha=alpha,\n",
    "            s=size,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"#2ecc71\",\n",
    "        markersize=8,\n",
    "        label=\"Ground Truth\",\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"#e74c3c\", markersize=8, label=\"Non-GT\"\n",
    "    ),\n",
    "]\n",
    "ax2.legend(handles=legend_elements)\n",
    "ax2.set_xlabel(\"Vector Distance\")\n",
    "ax2.set_ylabel(\"Rerank Score\")\n",
    "ax2.set_title(\"Rerank Score vs Vector Distance\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "fig2.tight_layout()\n",
    "fig2.savefig(FIGURES_DIR / \"rerank_vs_distance_scatter.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'rerank_vs_distance_scatter.png'}\")\n",
    "\n",
    "print(\"\\nPer GT memory details (sorted by rerank score, descending):\")\n",
    "for score, test_case_id, memory_id in sorted(ground_truth_scores_all, key=lambda x: -x[0]):\n",
    "    # Find the distance for this memory\n",
    "    distance = 0\n",
    "    for data in all_data:\n",
    "        if data[\"test_case_id\"] == test_case_id:\n",
    "            for result in data.get(\"reranked_results\", []):\n",
    "                if result[\"id\"] == memory_id:\n",
    "                    distance = result[\"distance\"]\n",
    "                    break\n",
    "    print(f\"  score={score:.4f}  dist={distance:.4f}  {memory_id}  ({test_case_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5b — Rerank Score Threshold Sweep\n",
    "\n",
    "Sweep rerank score thresholds: accept all candidates with score >= threshold.\n",
    "Higher threshold = stricter filtering (higher precision, lower recall).\n",
    "Compare with Phase 1 distance threshold sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Rerank Score Threshold Sweep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sweep rerank score thresholds (higher = stricter, opposite direction from distance)\n",
    "all_scores_flat = np.concatenate([ground_truth_scores, non_ground_truth_scores])\n",
    "sweep_thresholds = np.arange(0.0, max(all_scores_flat) + 0.01, 0.005)\n",
    "\n",
    "sweep_precisions, sweep_recalls, sweep_f1_scores, sweep_mrrs = [], [], [], []\n",
    "\n",
    "for threshold in sweep_thresholds:\n",
    "    precisions, recalls, f1_scores, reciprocal_ranks = [], [], [], []\n",
    "    for experiment_result in experiments_reranked:\n",
    "        # Accept candidates with rerank score >= threshold\n",
    "        accepted = {\n",
    "            result[\"id\"]\n",
    "            for result in experiment_result[\"reranked\"]\n",
    "            if result[\"rerank_score\"] >= threshold\n",
    "        }\n",
    "        ground_truth_accepted = len(accepted & experiment_result[\"ground_truth_ids\"])\n",
    "        num_accepted = len(accepted)\n",
    "        ground_truth_count = len(experiment_result[\"ground_truth_ids\"])\n",
    "\n",
    "        precision = ground_truth_accepted / num_accepted if num_accepted > 0 else 0.0\n",
    "        recall = ground_truth_accepted / ground_truth_count if ground_truth_count > 0 else 0.0\n",
    "        f1_score = (\n",
    "            2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        )\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        # MRR: reciprocal rank of first GT hit in accepted results (sorted by rerank score desc)\n",
    "        accepted_sorted = [\n",
    "            result\n",
    "            for result in experiment_result[\"reranked\"]\n",
    "            if result[\"rerank_score\"] >= threshold\n",
    "        ]\n",
    "        reciprocal_rank = 0.0\n",
    "        for rank, result in enumerate(accepted_sorted, 1):\n",
    "            if result[\"id\"] in experiment_result[\"ground_truth_ids\"]:\n",
    "                reciprocal_rank = 1.0 / rank\n",
    "                break\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "    sweep_precisions.append(np.mean(precisions))\n",
    "    sweep_recalls.append(np.mean(recalls))\n",
    "    sweep_f1_scores.append(np.mean(f1_scores))\n",
    "    sweep_mrrs.append(np.mean(reciprocal_ranks))\n",
    "\n",
    "sweep_precisions = np.array(sweep_precisions)\n",
    "sweep_recalls = np.array(sweep_recalls)\n",
    "sweep_f1_scores = np.array(sweep_f1_scores)\n",
    "sweep_mrrs = np.array(sweep_mrrs)\n",
    "\n",
    "best_f1_index = np.argmax(sweep_f1_scores)\n",
    "best_threshold = sweep_thresholds[best_f1_index]\n",
    "\n",
    "# --- Figure: P/R/F1/MRR vs rerank score threshold ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(sweep_thresholds, sweep_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, sweep_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, sweep_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\n",
    "ax.plot(sweep_thresholds, sweep_mrrs, label=\"MRR\", color=\"#e67e22\", linewidth=2)\n",
    "ax.axvline(\n",
    "    best_threshold,\n",
    "    color=\"#e74c3c\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Best F1 @ {best_threshold:.3f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Rerank Score Threshold (accept >= threshold)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"P/R/F1/MRR vs Rerank Score Threshold (macro-averaged)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"rerank_threshold_sweep.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'rerank_threshold_sweep.png'}\")\n",
    "\n",
    "print(f\"\\nOptimal F1 threshold: {best_threshold:.4f}\")\n",
    "print(f\"  F1:        {sweep_f1_scores[best_f1_index]:.3f}\")\n",
    "print(f\"  Precision: {sweep_precisions[best_f1_index]:.3f}\")\n",
    "print(f\"  Recall:    {sweep_recalls[best_f1_index]:.3f}\")\n",
    "print(f\"  MRR:       {sweep_mrrs[best_f1_index]:.3f}\")\n",
    "\n",
    "# Threshold table\n",
    "print(\"\\nThreshold table (macro-averaged):\")\n",
    "print(\n",
    "    f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Pick representative thresholds around the interesting range\n",
    "table_thresholds = sorted(\n",
    "    set(\n",
    "        [\n",
    "            0.001,\n",
    "            0.005,\n",
    "            0.01,\n",
    "            0.02,\n",
    "            0.03,\n",
    "            0.05,\n",
    "            0.08,\n",
    "            0.10,\n",
    "            0.15,\n",
    "            0.20,\n",
    "            0.30,\n",
    "            0.50,\n",
    "            round(best_threshold, 4),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "for threshold in table_thresholds:\n",
    "    index = np.argmin(np.abs(sweep_thresholds - threshold))\n",
    "    avg_accepted = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                [\n",
    "                    result\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"rerank_score\"] >= threshold\n",
    "                ]\n",
    "            )\n",
    "            for experiment_result in experiments_reranked\n",
    "        ]\n",
    "    )\n",
    "    avg_ground_truth_kept = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                {\n",
    "                    result[\"id\"]\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"rerank_score\"] >= threshold\n",
    "                }\n",
    "                & experiment_result[\"ground_truth_ids\"]\n",
    "            )\n",
    "            for experiment_result in experiments_reranked\n",
    "        ]\n",
    "    )\n",
    "    marker = \" <--\" if abs(threshold - best_threshold) < 0.003 else \"\"\n",
    "    print(\n",
    "        f\"{threshold:>10.4f} {sweep_precisions[index]:>10.3f} {sweep_recalls[index]:>8.3f} {sweep_f1_scores[index]:>8.3f} {sweep_mrrs[index]:>8.3f} {avg_accepted:>13.1f} {avg_ground_truth_kept:>12.1f}{marker}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 5c — Per-Experiment Impact at Optimal Rerank Threshold\n",
    "\n",
    "Show how the optimal rerank score threshold performs on each test case.\n",
    "Compare with Phase 1 distance threshold results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16 — Per-Experiment Impact at Optimal Rerank Threshold\n",
    "import numpy as np\n",
    "\n",
    "# Phase 1 optimal threshold from threshold analysis notebook\n",
    "PHASE1_OPTIMAL_THRESHOLD = 0.76  # distance threshold\n",
    "\n",
    "print(\"Per-experiment comparison at optimal thresholds:\")\n",
    "print(f\"  Phase 1: distance <= {PHASE1_OPTIMAL_THRESHOLD}\")\n",
    "print(f\"  Rerank:  score >= {best_threshold:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    f\"{'Test Case':<20} {'P1 F1':>7} {'Rk F1':>7} {'Delta':>7} {'P1 P':>6} {'Rk P':>6} {'P1 R':>6} {'Rk R':>6} {'P1 Acc':>7} {'Rk Acc':>7} {'GT':>4}\"\n",
    ")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "phase1_f1_scores, rerank_f1_scores = [], []\n",
    "for experiment_result in experiments_reranked:\n",
    "    test_case_id = experiment_result[\"test_case_id\"]\n",
    "    ground_truth_ids = experiment_result[\"ground_truth_ids\"]\n",
    "    ground_truth_count = len(ground_truth_ids)\n",
    "\n",
    "    # Phase 1: distance threshold\n",
    "    phase1_data = phase1_by_test_case.get(test_case_id, {})\n",
    "    phase1_best_distances = {}\n",
    "    for query_result in phase1_data.get(\"queries\", []):\n",
    "        for result in query_result.get(\"results\", []):\n",
    "            memory_id = result[\"id\"]\n",
    "            distance = result.get(\"distance\", float(\"inf\"))\n",
    "            if (\n",
    "                memory_id not in phase1_best_distances\n",
    "                or distance < phase1_best_distances[memory_id]\n",
    "            ):\n",
    "                phase1_best_distances[memory_id] = distance\n",
    "    phase1_accepted = {\n",
    "        memory_id\n",
    "        for memory_id, distance in phase1_best_distances.items()\n",
    "        if distance <= PHASE1_OPTIMAL_THRESHOLD\n",
    "    }\n",
    "    phase1_ground_truth_hits = len(phase1_accepted & ground_truth_ids)\n",
    "    phase1_accepted_count = len(phase1_accepted)\n",
    "    phase1_precision = (\n",
    "        phase1_ground_truth_hits / phase1_accepted_count if phase1_accepted_count > 0 else 0\n",
    "    )\n",
    "    phase1_recall = phase1_ground_truth_hits / ground_truth_count if ground_truth_count > 0 else 0\n",
    "    phase1_f1_score = (\n",
    "        2 * phase1_precision * phase1_recall / (phase1_precision + phase1_recall)\n",
    "        if (phase1_precision + phase1_recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # Rerank: score threshold\n",
    "    rerank_accepted_list = [\n",
    "        result\n",
    "        for result in experiment_result[\"reranked\"]\n",
    "        if result[\"rerank_score\"] >= best_threshold\n",
    "    ]\n",
    "    rerank_accepted_ids = {result[\"id\"] for result in rerank_accepted_list}\n",
    "    rerank_ground_truth_hits = len(rerank_accepted_ids & ground_truth_ids)\n",
    "    rerank_accepted_count = len(rerank_accepted_ids)\n",
    "    rerank_precision = (\n",
    "        rerank_ground_truth_hits / rerank_accepted_count if rerank_accepted_count > 0 else 0\n",
    "    )\n",
    "    rerank_recall = rerank_ground_truth_hits / ground_truth_count if ground_truth_count > 0 else 0\n",
    "    rerank_f1_score = (\n",
    "        2 * rerank_precision * rerank_recall / (rerank_precision + rerank_recall)\n",
    "        if (rerank_precision + rerank_recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    delta = rerank_f1_score - phase1_f1_score\n",
    "    marker = \"+\" if delta > 0.001 else \"-\" if delta < -0.001 else \"=\"\n",
    "\n",
    "    phase1_f1_scores.append(phase1_f1_score)\n",
    "    rerank_f1_scores.append(rerank_f1_score)\n",
    "\n",
    "    print(\n",
    "        f\"{test_case_id:<20} {phase1_f1_score:>7.3f} {rerank_f1_score:>7.3f} {delta:>+6.3f}{marker} {phase1_precision:>6.1%} {rerank_precision:>6.1%} {phase1_recall:>6.1%} {rerank_recall:>6.1%} {phase1_accepted_count:>7} {rerank_accepted_count:>7} {ground_truth_count:>4}\"\n",
    "    )\n",
    "\n",
    "    # Show missed GT for rerank\n",
    "    rerank_missed = ground_truth_ids - rerank_accepted_ids\n",
    "    if rerank_missed:\n",
    "        for memory_id in sorted(rerank_missed):\n",
    "            # Find rerank score for this missed memory\n",
    "            score = next(\n",
    "                (\n",
    "                    result[\"rerank_score\"]\n",
    "                    for result in experiment_result[\"reranked\"]\n",
    "                    if result[\"id\"] == memory_id\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            score_str = f\"score={score:.4f}\" if score is not None else \"NOT IN POOL\"\n",
    "            print(f\"  {'':20} Missed: {memory_id} ({score_str})\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "avg_phase1 = np.mean(phase1_f1_scores)\n",
    "avg_rerank = np.mean(rerank_f1_scores)\n",
    "delta = avg_rerank - avg_phase1\n",
    "print(\n",
    "    f\"{'AVERAGE':<20} {avg_phase1:>7.3f} {avg_rerank:>7.3f} {delta:>+6.3f}{'+' if delta > 0 else '-' if delta < 0 else '='}\"\n",
    ")\n",
    "\n",
    "improved = sum(\n",
    "    1\n",
    "    for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_scores)\n",
    "    if rerank_f1 > phase1_f1 + 0.001\n",
    ")\n",
    "same = sum(\n",
    "    1\n",
    "    for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_scores)\n",
    "    if abs(rerank_f1 - phase1_f1) <= 0.001\n",
    ")\n",
    "worse = sum(\n",
    "    1\n",
    "    for phase1_f1, rerank_f1 in zip(phase1_f1_scores, rerank_f1_scores)\n",
    "    if rerank_f1 < phase1_f1 - 0.001\n",
    ")\n",
    "print(\n",
    "    f\"\\nReranking helped: {improved}/{len(phase1_f1_scores)} | Same: {same}/{len(phase1_f1_scores)} | Hurt: {worse}/{len(phase1_f1_scores)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17 — Final Summary\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RERANKING COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nPhase 1 run: {PHASE1_RUN.name}\")\n",
    "print(f\"Test cases: {len(experiments_reranked)}\")\n",
    "print(f\"Reranker: {reranker.model_name}\")\n",
    "\n",
    "# Top-N comparison\n",
    "print(f\"\\n--- Top-{ANALYSIS_TOP_N} Comparison (distance vs rerank) ---\")\n",
    "avg_distance_f1 = sum(result[\"distance_top_n_metrics\"][\"f1\"] for result in all_results) / len(\n",
    "    all_results\n",
    ")\n",
    "rerank_f1_scores = []\n",
    "for result in all_results:\n",
    "    ground_truth_ids = set(result.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    rerank_top_ids = {entry[\"id\"] for entry in result[\"reranked_results\"][:ANALYSIS_TOP_N]}\n",
    "    rerank_metrics = compute_metrics(rerank_top_ids, ground_truth_ids)\n",
    "    rerank_f1_scores.append(rerank_metrics[\"f1\"])\n",
    "avg_rerank_f1 = sum(rerank_f1_scores) / len(rerank_f1_scores)\n",
    "\n",
    "print(f\"  Distance top-{ANALYSIS_TOP_N} avg F1: {avg_distance_f1:.3f}\")\n",
    "print(f\"  Rerank top-{ANALYSIS_TOP_N} avg F1:   {avg_rerank_f1:.3f}\")\n",
    "print(f\"  Delta: {avg_rerank_f1 - avg_distance_f1:+.3f}\")\n",
    "\n",
    "# Threshold comparison\n",
    "print(\"\\n--- Threshold Comparison (Phase 1 distance vs rerank score) ---\")\n",
    "print(f\"  Phase 1 distance threshold {PHASE1_OPTIMAL_THRESHOLD}: avg F1 = {avg_phase1:.3f}\")\n",
    "print(f\"  Rerank score threshold {best_threshold:.4f}: avg F1 = {avg_rerank:.3f}\")\n",
    "print(\n",
    "    f\"  Delta: {avg_rerank - avg_phase1:+.3f} ({(avg_rerank - avg_phase1) / avg_phase1 * 100:+.1f}%)\"\n",
    ")\n",
    "\n",
    "# Optimal rerank threshold details\n",
    "print(\"\\n--- Optimal Rerank Score Threshold ---\")\n",
    "print(f\"  Threshold: {best_threshold:.4f}\")\n",
    "print(f\"  F1:        {sweep_f1_scores[best_f1_index]:.3f}\")\n",
    "print(f\"  Precision: {sweep_precisions[best_f1_index]:.3f}\")\n",
    "print(f\"  Recall:    {sweep_recalls[best_f1_index]:.3f}\")\n",
    "print(f\"  MRR:       {sweep_mrrs[best_f1_index]:.3f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}