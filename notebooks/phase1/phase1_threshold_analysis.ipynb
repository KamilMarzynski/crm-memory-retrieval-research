{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 1: Distance Threshold Analysis\n",
    "\n",
    "Analyzes experiment results to determine an optimal cosine distance cutoff for filtering\n",
    "retrieved memories. The goal is to increase **precision** (reduce false positives) while\n",
    "maintaining acceptable **recall** (not losing ground truth memories).\n",
    "\n",
    "All metrics are **macro-averaged**: computed per experiment, then averaged across experiments.\n",
    "This gives each test case equal weight. Within each experiment, results are deduplicated by\n",
    "memory ID using the *best* (minimum) distance across all queries.\n",
    "\n",
    "**Approach:**\n",
    "1. Collect best-distance data per unique memory per experiment\n",
    "2. Visualize distance distributions (ground truth vs non-ground-truth)\n",
    "3. Sweep thresholds and compute precision/recall/F1 (macro-averaged)\n",
    "4. Identify the optimal cutoff and its tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from memory_retrieval.infra.figures import create_figure_session, save_figure\n",
    "from memory_retrieval.infra.runs import get_latest_run\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Run selection: use latest run or select a specific one\n",
    "# To see available runs: print(list_runs(\"phase1\"))\n",
    "# To select specific run: RUN_DIR = get_run(\"phase1\", \"run_20260208_143022\")\n",
    "\n",
    "RUN_DIR = get_latest_run(\"phase1\")\n",
    "RESULTS_DIR = RUN_DIR / \"results\"\n",
    "\n",
    "# Load all experiment result files\n",
    "result_files = sorted(RESULTS_DIR.glob(\"results_*.json\"))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Using run: {RUN_DIR.name}\")\n",
    "print(f\"Found {len(result_files)} result files:\")\n",
    "for result_file in result_files:\n",
    "    print(f\"  {result_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-experiment deduped data: best (minimum) distance for each memory across all queries\n",
    "\n",
    "from retrieval_metrics.compute import compute_threshold_metrics\n",
    "from retrieval_metrics.sweeps import find_optimal_entry\n",
    "\n",
    "from memory_retrieval.experiments.metrics import pool_and_deduplicate_by_distance\n",
    "from memory_retrieval.experiments.metrics_adapter import (\n",
    "    restriction_evaluation_to_dict,\n",
    "    threshold_sweep_from_experiments as sweep_threshold,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics_at_threshold(\n",
    "    ranked_results,\n",
    "    ground_truth_ids,\n",
    "    threshold,\n",
    "    score_field,\n",
    "    higher_is_better,\n",
    "    id_field=\"id\",\n",
    "):\n",
    "    evaluation = compute_threshold_metrics(\n",
    "        ranked_results,\n",
    "        ground_truth_ids,\n",
    "        threshold,\n",
    "        score_key=score_field,\n",
    "        higher_is_better=higher_is_better,\n",
    "        id_key=id_field,\n",
    "    )\n",
    "    return restriction_evaluation_to_dict(evaluation, include_accepted_count=True)\n",
    "\n",
    "\n",
    "def find_optimal_threshold(sweep_results, metric=\"f1\"):\n",
    "    return find_optimal_entry(sweep_results, metric_key=metric)\n",
    "\n",
    "experiments = []\n",
    "for result_file in result_files:\n",
    "    with open(result_file) as fh:\n",
    "        experiments.append(json.load(fh))\n",
    "\n",
    "# Precompute per-experiment: deduplicated best distances + GT IDs (reused by all cells below)\n",
    "experiments_deduped = []\n",
    "for experiment in experiments:\n",
    "    test_case_id = experiment.get(\"test_case_id\", \"unknown\")\n",
    "    ground_truth_ids = set(experiment.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "    pooled = pool_and_deduplicate_by_distance(experiment.get(\"queries\", []))\n",
    "    best_distances = {result[\"id\"]: result[\"distance\"] for result in pooled}\n",
    "    experiments_deduped.append(\n",
    "        {\n",
    "            \"test_case_id\": test_case_id,\n",
    "            \"ground_truth_ids\": ground_truth_ids,\n",
    "            \"best_distances\": best_distances,\n",
    "            \"ranked_results\": pooled,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Collect per-observation distances (one entry per experiment×memory pair) — used for histograms\n",
    "ground_truth_observations = []  # GT observations (each memory appears exactly once since GT sets are disjoint)\n",
    "non_ground_truth_observations = []  # non-GT observations (same memory appears in ~10 experiments)\n",
    "ground_truth_memory_details = []  # (memory_id, best_distance, test_case_id)\n",
    "\n",
    "for experiment_data in experiments_deduped:\n",
    "    for memory_id, distance in experiment_data[\"best_distances\"].items():\n",
    "        if memory_id in experiment_data[\"ground_truth_ids\"]:\n",
    "            ground_truth_observations.append(distance)\n",
    "            ground_truth_memory_details.append(\n",
    "                (memory_id, distance, experiment_data[\"test_case_id\"])\n",
    "            )\n",
    "        else:\n",
    "            non_ground_truth_observations.append(distance)\n",
    "\n",
    "ground_truth_obs = np.array(ground_truth_observations)\n",
    "non_ground_truth_obs = np.array(non_ground_truth_observations)\n",
    "\n",
    "# True unique counts\n",
    "total_ground_truth_memories = sum(\n",
    "    len(experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped\n",
    ")\n",
    "all_unique_ids = set()\n",
    "for experiment_data in experiments_deduped:\n",
    "    all_unique_ids.update(experiment_data[\"best_distances\"].keys())\n",
    "num_unique_memories = len(all_unique_ids)\n",
    "\n",
    "print(f\"Database: {num_unique_memories} unique memories\")\n",
    "print(\n",
    "    f\"Experiments: {len(experiments_deduped)} test cases, GT sets are disjoint (each memory GT in exactly 1)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Total GT assignments: {total_ground_truth_memories} (= {num_unique_memories} unique memories)\"\n",
    ")\n",
    "print(\"\")\n",
    "print(\"Observations (experiment × memory pairs, used for histograms):\")\n",
    "print(f\"  GT observations:     {len(ground_truth_obs)} (unique — each memory is GT once)\")\n",
    "print(\n",
    "    f\"  Non-GT observations: {len(non_ground_truth_obs)} (~{num_unique_memories} memories × ~{len(experiments_deduped) - 1} experiments each)\"\n",
    ")\n",
    "print(\"\")\n",
    "print(f\"GT distance range:     [{ground_truth_obs.min():.4f}, {ground_truth_obs.max():.4f}]\")\n",
    "print(\n",
    "    f\"Non-GT distance range: [{non_ground_truth_obs.min():.4f}, {non_ground_truth_obs.max():.4f}]\"\n",
    ")\n",
    "print(f\"GT mean: {ground_truth_obs.mean():.4f}, median: {np.median(ground_truth_obs):.4f}\")\n",
    "print(\n",
    "    f\"Non-GT mean: {non_ground_truth_obs.mean():.4f}, median: {np.median(non_ground_truth_obs):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"\\nPer GT memory details (sorted by distance):\")\n",
    "for memory_id, distance, test_case_id in sorted(ground_truth_memory_details, key=lambda x: x[1]):\n",
    "    print(f\"  {distance:.4f}  {memory_id}  ({test_case_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density histogram and threshold sweep (macro-averaged with MRR)\n",
    "# Exported as separate figures for blog post\n",
    "\n",
    "if \"FIGURE_SESSION\" not in globals() or FIGURE_SESSION.context.get(\"run_id\") != RUN_DIR.name:\n",
    "    FIGURE_SESSION = create_figure_session(\n",
    "        root_dir=RUN_DIR / \"figures\",\n",
    "        notebook_slug=\"phase1_threshold_analysis\",\n",
    "        context_key=RUN_DIR.name,\n",
    "        context={\"phase\": \"phase1\", \"run_id\": RUN_DIR.name},\n",
    "    )\n",
    "print(f\"Figure export session: {FIGURE_SESSION.session_dir}\")\n",
    "\n",
    "bins = np.arange(0.1, 1.15, 0.025)\n",
    "\n",
    "# --- Figure 1: Normalized density histogram ---\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax1.hist(\n",
    "    ground_truth_obs,\n",
    "    bins=bins,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"GT\",\n",
    "    color=\"#2ecc71\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax1.hist(\n",
    "    non_ground_truth_obs,\n",
    "    bins=bins,\n",
    "    alpha=0.6,\n",
    "    density=True,\n",
    "    label=\"Non-GT\",\n",
    "    color=\"#e74c3c\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax1.axvline(\n",
    "    np.median(ground_truth_obs),\n",
    "    color=\"#27ae60\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"GT median: {np.median(ground_truth_obs):.3f}\",\n",
    ")\n",
    "ax1.axvline(\n",
    "    np.median(non_ground_truth_obs),\n",
    "    color=\"#c0392b\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Non-GT median: {np.median(non_ground_truth_obs):.3f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Best Cosine Distance\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"Distance Distribution (normalized)\")\n",
    "ax1.legend(fontsize=8)\n",
    "fig1.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig1,\n",
    "    FIGURE_SESSION,\n",
    "    \"distance_distribution_histogram\",\n",
    "    title=\"Distance Distribution (normalized)\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "\n",
    "# --- Compute threshold sweep data using centralized metrics ---\n",
    "experiment_thresholds = list(np.arange(0.10, 1.15, 0.01))\n",
    "distance_sweep = sweep_threshold(\n",
    "    experiments_deduped, experiment_thresholds, score_field=\"distance\", higher_is_better=False\n",
    ")\n",
    "\n",
    "experiment_precisions = np.array([entry[\"precision\"] for entry in distance_sweep])\n",
    "experiment_recalls = np.array([entry[\"recall\"] for entry in distance_sweep])\n",
    "experiment_f1_scores = np.array([entry[\"f1\"] for entry in distance_sweep])\n",
    "experiment_mrr_scores = np.array([entry[\"mrr\"] for entry in distance_sweep])\n",
    "experiment_thresholds = np.array(experiment_thresholds)\n",
    "\n",
    "optimal = find_optimal_threshold(distance_sweep, metric=\"f1\")\n",
    "experiment_best_f1_index = optimal[\"index\"]\n",
    "experiment_best_threshold = optimal[\"threshold\"]\n",
    "\n",
    "# --- Figure 2: P/R/F1/MRR vs threshold ---\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "ax2.plot(\n",
    "    experiment_thresholds, experiment_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2\n",
    ")\n",
    "ax2.plot(experiment_thresholds, experiment_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\n",
    "ax2.plot(experiment_thresholds, experiment_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\n",
    "ax2.plot(experiment_thresholds, experiment_mrr_scores, label=\"MRR\", color=\"#e67e22\", linewidth=2)\n",
    "ax2.axvline(\n",
    "    experiment_best_threshold,\n",
    "    color=\"#e74c3c\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Best F1 @ {experiment_best_threshold:.2f}\",\n",
    ")\n",
    "ax2.set_xlabel(\"Distance Threshold\")\n",
    "ax2.set_ylabel(\"Score\")\n",
    "ax2.set_title(\"P/R/F1/MRR vs Threshold\")\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "fig2.tight_layout()\n",
    "saved_paths = save_figure(\n",
    "    fig2,\n",
    "    FIGURE_SESSION,\n",
    "    \"threshold_sweep_metrics\",\n",
    "    title=\"P/R/F1/MRR vs Threshold\",\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Saved: {saved_paths['png']}\")\n",
    "print(f\"\\nOptimal F1 threshold: {experiment_best_threshold:.2f}\")\n",
    "print(f\"  F1:        {experiment_f1_scores[experiment_best_f1_index]:.3f}\")\n",
    "print(f\"  Precision: {experiment_precisions[experiment_best_f1_index]:.3f}\")\n",
    "print(f\"  Recall:    {experiment_recalls[experiment_best_f1_index]:.3f}\")\n",
    "print(f\"  MRR:       {experiment_mrr_scores[experiment_best_f1_index]:.3f}\")\n",
    "print(\"  (all metrics macro-averaged: per-experiment, then mean)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold table (macro-averaged per experiment, with MRR)\n",
    "print(\"Threshold table (macro-averaged: per-experiment P/R/F1/MRR, then mean):\")\n",
    "print(f\"Database has {num_unique_memories} unique memories, {len(experiments_deduped)} test cases\")\n",
    "print()\n",
    "print(\n",
    "    f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12} {'Avg GT Lost':>12}\"\n",
    ")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "table_thresholds = [0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.05]\n",
    "avg_ground_truth_count = np.mean(\n",
    "    [len(experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped]\n",
    ")\n",
    "\n",
    "for threshold in table_thresholds:\n",
    "    per_case_metrics = []\n",
    "    num_accepted_list, ground_truth_kept_list = [], []\n",
    "\n",
    "    for experiment_data in experiments_deduped:\n",
    "        metrics = compute_metrics_at_threshold(\n",
    "            experiment_data[\"ranked_results\"],\n",
    "            experiment_data[\"ground_truth_ids\"],\n",
    "            threshold,\n",
    "            score_field=\"distance\",\n",
    "            higher_is_better=False,\n",
    "        )\n",
    "        per_case_metrics.append(metrics)\n",
    "        num_accepted_list.append(metrics[\"accepted_count\"])\n",
    "        ground_truth_kept_list.append(\n",
    "            len(metrics[\"retrieved_ids\"] & experiment_data[\"ground_truth_ids\"])\n",
    "        )\n",
    "\n",
    "    avg_precision = np.mean([metrics[\"precision\"] for metrics in per_case_metrics])\n",
    "    avg_recall = np.mean([metrics[\"recall\"] for metrics in per_case_metrics])\n",
    "    avg_f1_score = np.mean([metrics[\"f1\"] for metrics in per_case_metrics])\n",
    "    avg_mrr = np.mean([metrics[\"mrr\"] for metrics in per_case_metrics])\n",
    "    avg_num_accepted = np.mean(num_accepted_list)\n",
    "    avg_ground_truth_kept = np.mean(ground_truth_kept_list)\n",
    "    avg_ground_truth_lost = avg_ground_truth_count - avg_ground_truth_kept\n",
    "\n",
    "    marker = \" <--\" if abs(threshold - experiment_best_threshold) < 0.015 else \"\"\n",
    "    print(\n",
    "        f\"{threshold:>10.2f} {avg_precision:>10.3f} {avg_recall:>8.3f} {avg_f1_score:>8.3f} {avg_mrr:>8.3f} {avg_num_accepted:>13.1f} {avg_ground_truth_kept:>12.1f} {avg_ground_truth_lost:>12.1f}{marker}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-experiment impact of applying the best threshold (with MRR)\n",
    "\n",
    "print(f\"Impact of threshold={experiment_best_threshold:.2f} per experiment:\\n\")\n",
    "print(\n",
    "    f\"{'Test Case':<25} {'Recall':>7} {'Prec':>7} {'F1':>7} {'MRR':>7} {'GT':>4} {'Kept':>5} {'Lost':>5} {'Accepted':>9}\"\n",
    ")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "per_experiment_mrrs = []\n",
    "for experiment_data in experiments_deduped:\n",
    "    metrics = compute_metrics_at_threshold(\n",
    "        experiment_data[\"ranked_results\"],\n",
    "        experiment_data[\"ground_truth_ids\"],\n",
    "        experiment_best_threshold,\n",
    "        score_field=\"distance\",\n",
    "        higher_is_better=False,\n",
    "    )\n",
    "\n",
    "    accepted_ids = metrics[\"retrieved_ids\"]\n",
    "    ground_truth_accepted = accepted_ids & experiment_data[\"ground_truth_ids\"]\n",
    "    ground_truth_lost = experiment_data[\"ground_truth_ids\"] - accepted_ids\n",
    "\n",
    "    per_experiment_mrrs.append(metrics[\"mrr\"])\n",
    "\n",
    "    print(\n",
    "        f\"{experiment_data['test_case_id']:<25} {metrics['recall']:>7.1%} {metrics['precision']:>7.1%} {metrics['f1']:>7.3f} {metrics['mrr']:>7.3f} {len(experiment_data['ground_truth_ids']):>4} {len(ground_truth_accepted):>5} {len(ground_truth_lost):>5} {metrics['accepted_count']:>9}\"\n",
    "    )\n",
    "\n",
    "    if ground_truth_lost:\n",
    "        for memory_id in sorted(ground_truth_lost):\n",
    "            distance = experiment_data[\"best_distances\"].get(memory_id)\n",
    "            distance_str = f\"{distance:.4f}\" if distance is not None else \"NOT RETRIEVED\"\n",
    "            print(f\"  {'':25} Lost: {memory_id} (best d={distance_str})\")\n",
    "\n",
    "print(\"-\" * 85)\n",
    "avg_num_accepted = np.mean(\n",
    "    [\n",
    "        len(\n",
    "            {\n",
    "                memory_id\n",
    "                for memory_id, distance in experiment_data[\"best_distances\"].items()\n",
    "                if distance <= experiment_best_threshold\n",
    "            }\n",
    "        )\n",
    "        for experiment_data in experiments_deduped\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    f\"{'AVERAGE':<25} {np.mean([len({memory_id for memory_id, distance in experiment_data['best_distances'].items() if distance <= experiment_best_threshold} & experiment_data['ground_truth_ids']) / len(experiment_data['ground_truth_ids']) for experiment_data in experiments_deduped]):>7.1%} {'':>7} {'':>7} {np.mean(per_experiment_mrrs):>7.3f} {'':>4} {'':>5} {'':>5} {avg_num_accepted:>9.1f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: deduplicated — one point per unique memory per experiment (best distance)\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "experiment_names = [\n",
    "    experiment_data[\"test_case_id\"].replace(\"tc_\", \"\")[:30]\n",
    "    for experiment_data in experiments_deduped\n",
    "]\n",
    "x_positions = []\n",
    "colors = []\n",
    "distances_to_plot = []\n",
    "\n",
    "for experiment_index, experiment_data in enumerate(experiments_deduped):\n",
    "    for memory_id, distance in experiment_data[\"best_distances\"].items():\n",
    "        x_positions.append(experiment_index + np.random.uniform(-0.3, 0.3))\n",
    "        distances_to_plot.append(distance)\n",
    "        colors.append(\"#2ecc71\" if memory_id in experiment_data[\"ground_truth_ids\"] else \"#e74c3c\")\n",
    "\n",
    "ax.scatter(\n",
    "    x_positions, distances_to_plot, c=colors, alpha=0.5, s=25, edgecolors=\"white\", linewidth=0.3\n",
    ")\n",
    "ax.axhline(\n",
    "    experiment_best_threshold,\n",
    "    color=\"#3498db\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Optimal threshold: {experiment_best_threshold:.2f}\",\n",
    ")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"#2ecc71\",\n",
    "        markersize=8,\n",
    "        label=\"Ground Truth\",\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"#e74c3c\", markersize=8, label=\"Non-GT\"\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        color=\"#3498db\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Threshold: {experiment_best_threshold:.2f}\",\n",
    "    ),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"upper left\")\n",
    "\n",
    "ax.set_xticks(range(len(experiment_names)))\n",
    "ax.set_xticklabels(experiment_names, rotation=30, ha=\"right\", fontsize=8)\n",
    "ax.set_ylabel(\"Cosine Distance\")\n",
    "ax.set_title(\"Unique Memories by Experiment — best distance (below threshold = accepted)\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print counts\n",
    "total_raw = sum(\n",
    "    len(result)\n",
    "    for experiment in experiments\n",
    "    for query_result in experiment.get(\"queries\", [])\n",
    "    for result in [query_result.get(\"results\", [])]\n",
    ")\n",
    "num_ground_truth_plotted = colors.count(\"#2ecc71\")\n",
    "num_non_ground_truth_plotted = colors.count(\"#e74c3c\")\n",
    "print(f\"\\nRaw result rows (before dedup): {total_raw}\")\n",
    "print(\n",
    "    f\"Unique memories plotted:        {len(distances_to_plot)} ({len(distances_to_plot) // len(experiments_deduped):.0f} avg/experiment)\"\n",
    ")\n",
    "print(\n",
    "    f\"  GT:     {num_ground_truth_plotted} ({num_ground_truth_plotted // len(experiments_deduped):.0f} avg/experiment)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Non-GT: {num_non_ground_truth_plotted} ({num_non_ground_truth_plotted // len(experiments_deduped):.0f} avg/experiment)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the current confidence buckets from db.py vs data-driven buckets\n",
    "# Uses per-observation data (GT observations are unique; non-GT repeated across experiments)\n",
    "current_buckets = [\n",
    "    (\"high\", 0.0, 0.5),\n",
    "    (\"medium\", 0.5, 0.8),\n",
    "    (\"low\", 0.8, 1.2),\n",
    "    (\"very_low\", 1.2, 2.0),\n",
    "]\n",
    "\n",
    "print(\"Current confidence buckets (db.py) vs actual data:\")\n",
    "print(\n",
    "    f\"NOTE: GT observations are unique (n={len(ground_truth_obs)}), non-GT are repeated across experiments (n={len(non_ground_truth_obs)} from {num_unique_memories} memories)\"\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    f\"{'Bucket':<10} {'Range':<15} {'GT Obs':>10} {'Non-GT Obs':>12} {'GT %':>8} {'Obs Precision':>14}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for bucket_name, range_low, range_high in current_buckets:\n",
    "    ground_truth_in_bucket = int(\n",
    "        np.sum((ground_truth_obs >= range_low) & (ground_truth_obs < range_high))\n",
    "    )\n",
    "    non_ground_truth_in_bucket = int(\n",
    "        np.sum((non_ground_truth_obs >= range_low) & (non_ground_truth_obs < range_high))\n",
    "    )\n",
    "    total_in_bucket = ground_truth_in_bucket + non_ground_truth_in_bucket\n",
    "    ground_truth_percentage = (\n",
    "        ground_truth_in_bucket / len(ground_truth_obs) * 100 if len(ground_truth_obs) > 0 else 0\n",
    "    )\n",
    "    observation_precision = ground_truth_in_bucket / total_in_bucket if total_in_bucket > 0 else 0\n",
    "    print(\n",
    "        f\"{bucket_name:<10} [{range_low:.1f}, {range_high:.1f}){'':<5} {ground_truth_in_bucket:>10} {non_ground_truth_in_bucket:>12} {ground_truth_percentage:>7.1f}% {observation_precision:>14.1%}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nRecommendation: Update confidence buckets based on data distribution.\")\n",
    "print(\n",
    "    f\"The optimal F1 threshold ({experiment_best_threshold:.2f}) should be the cutoff between 'accepted' and 'rejected'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and recommendation (macro-averaged, with MRR)\n",
    "print(\"=\" * 70)\n",
    "print(\"THRESHOLD ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDatabase: {num_unique_memories} unique memories\")\n",
    "print(\n",
    "    f\"Test cases: {len(experiments_deduped)} (GT sets are disjoint, {total_ground_truth_memories} total GT assignments)\"\n",
    ")\n",
    "avg_retrieved = np.mean(\n",
    "    [len(experiment_data[\"best_distances\"]) for experiment_data in experiments_deduped]\n",
    ")\n",
    "print(f\"Avg memories retrieved per experiment: {avg_retrieved:.1f} / {num_unique_memories}\")\n",
    "\n",
    "print(f\"\\nOptimal F1 threshold: {experiment_best_threshold:.2f}\")\n",
    "print(\n",
    "    f\"  P={experiment_precisions[experiment_best_f1_index]:.1%}, R={experiment_recalls[experiment_best_f1_index]:.1%}, \"\n",
    "    f\"F1={experiment_f1_scores[experiment_best_f1_index]:.3f}, MRR={experiment_mrr_scores[experiment_best_f1_index]:.3f}\"\n",
    ")\n",
    "print(\"  (all metrics macro-averaged: per-experiment, then mean)\")\n",
    "\n",
    "# Retrieval counts at optimal threshold\n",
    "avg_num_accepted = np.mean(\n",
    "    [\n",
    "        len(\n",
    "            {\n",
    "                memory_id\n",
    "                for memory_id, distance in experiment_data[\"best_distances\"].items()\n",
    "                if distance <= experiment_best_threshold\n",
    "            }\n",
    "        )\n",
    "        for experiment_data in experiments_deduped\n",
    "    ]\n",
    ")\n",
    "avg_ground_truth_kept = np.mean(\n",
    "    [\n",
    "        len(\n",
    "            {\n",
    "                memory_id\n",
    "                for memory_id, distance in experiment_data[\"best_distances\"].items()\n",
    "                if distance <= experiment_best_threshold\n",
    "            }\n",
    "            & experiment_data[\"ground_truth_ids\"]\n",
    "        )\n",
    "        for experiment_data in experiments_deduped\n",
    "    ]\n",
    ")\n",
    "avg_ground_truth_count = np.mean(\n",
    "    [len(experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped]\n",
    ")\n",
    "print(\n",
    "    f\"\\n--- Memories retrieved at threshold={experiment_best_threshold:.2f} (avg per experiment) ---\"\n",
    ")\n",
    "print(f\"  {avg_num_accepted:.1f} / {avg_retrieved:.1f} retrieved memories accepted\")\n",
    "print(f\"    GT kept:  {avg_ground_truth_kept:.1f} / {avg_ground_truth_count:.1f}\")\n",
    "print(f\"    Non-GT:   {avg_num_accepted - avg_ground_truth_kept:.1f}\")\n",
    "\n",
    "# Data-driven tradeoff points\n",
    "print(\"\\n--- Key Tradeoff Points (data-driven) ---\")\n",
    "tradeoff_points = [(\"Best F1\", experiment_best_threshold)]\n",
    "\n",
    "# Find lowest threshold where macro-avg recall >= 0.9\n",
    "recall_90_candidates = experiment_thresholds[experiment_recalls >= 0.9]\n",
    "if len(recall_90_candidates) > 0:\n",
    "    threshold_recall_90 = recall_90_candidates[0]\n",
    "    tradeoff_points.append((\"Recall >= 90%\", threshold_recall_90))\n",
    "\n",
    "# Find lowest threshold where macro-avg recall >= 0.95\n",
    "recall_95_candidates = experiment_thresholds[experiment_recalls >= 0.95]\n",
    "if len(recall_95_candidates) > 0:\n",
    "    threshold_recall_95 = recall_95_candidates[0]\n",
    "    tradeoff_points.append((\"Recall >= 95%\", threshold_recall_95))\n",
    "\n",
    "# Find highest threshold where macro-avg precision >= 0.9\n",
    "precision_90_candidates = experiment_thresholds[experiment_precisions >= 0.9]\n",
    "if len(precision_90_candidates) > 0:\n",
    "    threshold_precision_90 = precision_90_candidates[-1]\n",
    "    tradeoff_points.append((\"Precision >= 90%\", threshold_precision_90))\n",
    "\n",
    "# Sort by threshold for readability\n",
    "tradeoff_points.sort(key=lambda x: x[1])\n",
    "\n",
    "for label, threshold in tradeoff_points:\n",
    "    index = np.argmin(np.abs(experiment_thresholds - threshold))\n",
    "    avg_accepted_at_threshold = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                {\n",
    "                    memory_id\n",
    "                    for memory_id, distance in experiment_data[\"best_distances\"].items()\n",
    "                    if distance <= threshold\n",
    "                }\n",
    "            )\n",
    "            for experiment_data in experiments_deduped\n",
    "        ]\n",
    "    )\n",
    "    avg_ground_truth_at_threshold = np.mean(\n",
    "        [\n",
    "            len(\n",
    "                {\n",
    "                    memory_id\n",
    "                    for memory_id, distance in experiment_data[\"best_distances\"].items()\n",
    "                    if distance <= threshold\n",
    "                }\n",
    "                & experiment_data[\"ground_truth_ids\"]\n",
    "            )\n",
    "            for experiment_data in experiments_deduped\n",
    "        ]\n",
    "    )\n",
    "    print(\n",
    "        f\"  t={threshold:.2f} ({label:>17}): P={experiment_precisions[index]:.1%}, R={experiment_recalls[index]:.1%}, \"\n",
    "        f\"F1={experiment_f1_scores[index]:.3f}, MRR={experiment_mrr_scores[index]:.3f}, \"\n",
    "        f\"avg accepted={avg_accepted_at_threshold:.1f} ({avg_ground_truth_at_threshold:.1f} GT + {avg_accepted_at_threshold - avg_ground_truth_at_threshold:.1f} non-GT)\"\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Recommendation ---\")\n",
    "print(\n",
    "    f\"The GT and non-GT distributions overlap significantly (GT median={np.median(ground_truth_obs):.3f}, \"\n",
    "    f\"non-GT median={np.median(non_ground_truth_obs):.3f}).\"\n",
    ")\n",
    "print(\"A single distance threshold cannot cleanly separate them.\")\n",
    "print(\"\")\n",
    "print(\"Practical options:\")\n",
    "print(\n",
    "    f\"  1. Use threshold={experiment_best_threshold:.2f} as DEFAULT_DISTANCE_THRESHOLD for best F1\"\n",
    ")\n",
    "print(\"  2. Use a higher threshold to preserve recall, accepting lower precision\")\n",
    "print(\"  3. Combine threshold with additional signals (RRF scoring, re-ranking)\")\n",
    "print(\"     to improve separation beyond what distance alone provides\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
