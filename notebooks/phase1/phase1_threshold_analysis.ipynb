{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 1: Distance Threshold Analysis\n",
    "\n",
    "Analyzes experiment results to determine an optimal cosine distance cutoff for filtering\n",
    "retrieved memories. The goal is to increase **precision** (reduce false positives) while\n",
    "maintaining acceptable **recall** (not losing ground truth memories).\n",
    "\n",
    "All metrics are **macro-averaged**: computed per experiment, then averaged across experiments.\n",
    "This gives each test case equal weight. Within each experiment, results are deduplicated by\n",
    "memory ID using the *best* (minimum) distance across all queries.\n",
    "\n",
    "**Approach:**\n",
    "1. Collect best-distance data per unique memory per experiment\n",
    "2. Visualize distance distributions (ground truth vs non-ground-truth)\n",
    "3. Sweep thresholds and compute precision/recall/F1 (macro-averaged)\n",
    "4. Identify the optimal cutoff and its tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom memory_retrieval.infra.runs import get_latest_run, get_run, list_runs\n\n# Find project root by walking up to pyproject.toml\nPROJECT_ROOT = Path.cwd()\nwhile not (PROJECT_ROOT / \"pyproject.toml\").exists():\n    if PROJECT_ROOT == PROJECT_ROOT.parent:\n        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n    PROJECT_ROOT = PROJECT_ROOT.parent\nos.chdir(PROJECT_ROOT)\n\n# Run selection: use latest run or select a specific one\n# To see available runs: print(list_runs(\"phase1\"))\n# To select specific run: RUN_DIR = get_run(\"phase1\", \"run_20260208_143022\")\n\nRUN_DIR = get_latest_run(\"phase1\")\nRESULTS_DIR = RUN_DIR / \"results\"\n\n# Load all experiment result files\nresult_files = sorted(RESULTS_DIR.glob(\"results_*.json\"))\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Using run: {RUN_DIR.name}\")\nprint(f\"Found {len(result_files)} result files:\")\nfor f in result_files:\n    print(f\"  {f.name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8r49ituha2d",
   "metadata": {},
   "outputs": [],
   "source": "# Build per-experiment deduped data: best (minimum) distance for each memory across all queries\nexperiments = []\nfor result_file in result_files:\n    with open(result_file) as fh:\n        experiments.append(json.load(fh))\n\n# Precompute per-experiment: deduplicated best distances + GT IDs (reused by all cells below)\nexperiments_deduped = []\nfor experiment in experiments:\n    test_case_id = experiment.get(\"test_case_id\", \"unknown\")\n    ground_truth_ids = set(experiment.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n    best_distances = {}\n    for query_result in experiment.get(\"queries\", []):\n        for result in query_result.get(\"results\", []):\n            memory_id = result[\"id\"]\n            distance = result[\"distance\"]\n            if memory_id not in best_distances or distance < best_distances[memory_id]:\n                best_distances[memory_id] = distance\n    experiments_deduped.append({\n        \"test_case_id\": test_case_id,\n        \"ground_truth_ids\": ground_truth_ids,\n        \"best_distances\": best_distances,\n    })\n\n# Collect per-observation distances (one entry per experiment×memory pair) — used for histograms\nground_truth_observations = []      # GT observations (each memory appears exactly once since GT sets are disjoint)\nnon_ground_truth_observations = []  # non-GT observations (same memory appears in ~10 experiments)\nground_truth_memory_details = []    # (memory_id, best_distance, test_case_id)\n\nfor experiment_data in experiments_deduped:\n    for memory_id, distance in experiment_data[\"best_distances\"].items():\n        if memory_id in experiment_data[\"ground_truth_ids\"]:\n            ground_truth_observations.append(distance)\n            ground_truth_memory_details.append((memory_id, distance, experiment_data[\"test_case_id\"]))\n        else:\n            non_ground_truth_observations.append(distance)\n\nground_truth_obs = np.array(ground_truth_observations)\nnon_ground_truth_obs = np.array(non_ground_truth_observations)\n\n# True unique counts\ntotal_ground_truth_memories = sum(len(experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped)\nall_unique_ids = set()\nfor experiment_data in experiments_deduped:\n    all_unique_ids.update(experiment_data[\"best_distances\"].keys())\nnum_unique_memories = len(all_unique_ids)\n\nprint(f\"Database: {num_unique_memories} unique memories\")\nprint(f\"Experiments: {len(experiments_deduped)} test cases, GT sets are disjoint (each memory GT in exactly 1)\")\nprint(f\"  Total GT assignments: {total_ground_truth_memories} (= {num_unique_memories} unique memories)\")\nprint(f\"\")\nprint(f\"Observations (experiment × memory pairs, used for histograms):\")\nprint(f\"  GT observations:     {len(ground_truth_obs)} (unique — each memory is GT once)\")\nprint(f\"  Non-GT observations: {len(non_ground_truth_obs)} (~{num_unique_memories} memories × ~{len(experiments_deduped)-1} experiments each)\")\nprint(f\"\")\nprint(f\"GT distance range:     [{ground_truth_obs.min():.4f}, {ground_truth_obs.max():.4f}]\")\nprint(f\"Non-GT distance range: [{non_ground_truth_obs.min():.4f}, {non_ground_truth_obs.max():.4f}]\")\nprint(f\"GT mean: {ground_truth_obs.mean():.4f}, median: {np.median(ground_truth_obs):.4f}\")\nprint(f\"Non-GT mean: {non_ground_truth_obs.mean():.4f}, median: {np.median(non_ground_truth_obs):.4f}\")\n\nprint(f\"\\nPer GT memory details (sorted by distance):\")\nfor memory_id, distance, test_case_id in sorted(ground_truth_memory_details, key=lambda x: x[1]):\n    print(f\"  {distance:.4f}  {memory_id}  ({test_case_id})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0r2ga7nyzl",
   "metadata": {},
   "outputs": [],
   "source": "# Density histogram and threshold sweep (macro-averaged with MRR)\n# Exported as separate figures for blog post\n\nFIGURES_DIR = Path(\"notebooks/phase1/figures\")\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\nbins = np.arange(0.1, 1.15, 0.025)\n\n# --- Figure 1: Normalized density histogram ---\nfig1, ax1 = plt.subplots(figsize=(10, 5))\nax1.hist(ground_truth_obs, bins=bins, alpha=0.6, density=True,\n         label=\"GT\", color=\"#2ecc71\", edgecolor=\"white\", linewidth=0.5)\nax1.hist(non_ground_truth_obs, bins=bins, alpha=0.6, density=True,\n         label=\"Non-GT\", color=\"#e74c3c\", edgecolor=\"white\", linewidth=0.5)\nax1.axvline(np.median(ground_truth_obs), color=\"#27ae60\", linestyle=\"--\", linewidth=1.5,\n            label=f\"GT median: {np.median(ground_truth_obs):.3f}\")\nax1.axvline(np.median(non_ground_truth_obs), color=\"#c0392b\", linestyle=\"--\", linewidth=1.5,\n            label=f\"Non-GT median: {np.median(non_ground_truth_obs):.3f}\")\nax1.set_xlabel(\"Best Cosine Distance\")\nax1.set_ylabel(\"Density\")\nax1.set_title(\"Distance Distribution (normalized)\")\nax1.legend(fontsize=8)\nfig1.tight_layout()\nfig1.savefig(FIGURES_DIR / \"distance_distribution_histogram.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Saved: {FIGURES_DIR / 'distance_distribution_histogram.png'}\")\n\n# --- Compute threshold sweep data ---\nexperiment_thresholds = np.arange(0.10, 1.15, 0.01)\nexperiment_precisions = []\nexperiment_recalls = []\nexperiment_f1_scores = []\nexperiment_mrr_scores = []\n\nfor threshold in experiment_thresholds:\n    precisions, recalls, f1_scores, reciprocal_ranks = [], [], [], []\n    for experiment_data in experiments_deduped:\n        accepted = {memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= threshold}\n        ground_truth_accepted = len(accepted & experiment_data[\"ground_truth_ids\"])\n        num_accepted = len(accepted)\n        ground_truth_count = len(experiment_data[\"ground_truth_ids\"])\n\n        precision = ground_truth_accepted / num_accepted if num_accepted > 0 else 0.0\n        recall = ground_truth_accepted / ground_truth_count if ground_truth_count > 0 else 0.0\n        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1_score)\n\n        accepted_sorted = sorted(\n            [(memory_id, distance) for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= threshold],\n            key=lambda x: x[1]\n        )\n        reciprocal_rank = 0.0\n        for rank, (memory_id, _) in enumerate(accepted_sorted, 1):\n            if memory_id in experiment_data[\"ground_truth_ids\"]:\n                reciprocal_rank = 1.0 / rank\n                break\n        reciprocal_ranks.append(reciprocal_rank)\n\n    experiment_precisions.append(np.mean(precisions))\n    experiment_recalls.append(np.mean(recalls))\n    experiment_f1_scores.append(np.mean(f1_scores))\n    experiment_mrr_scores.append(np.mean(reciprocal_ranks))\n\nexperiment_precisions = np.array(experiment_precisions)\nexperiment_recalls = np.array(experiment_recalls)\nexperiment_f1_scores = np.array(experiment_f1_scores)\nexperiment_mrr_scores = np.array(experiment_mrr_scores)\n\nexperiment_best_f1_index = np.argmax(experiment_f1_scores)\nexperiment_best_threshold = experiment_thresholds[experiment_best_f1_index]\n\n# --- Figure 2: P/R/F1/MRR vs threshold ---\nfig2, ax2 = plt.subplots(figsize=(10, 5))\nax2.plot(experiment_thresholds, experiment_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2)\nax2.plot(experiment_thresholds, experiment_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2)\nax2.plot(experiment_thresholds, experiment_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2)\nax2.plot(experiment_thresholds, experiment_mrr_scores, label=\"MRR\", color=\"#e67e22\", linewidth=2)\nax2.axvline(experiment_best_threshold, color=\"#e74c3c\", linestyle=\"--\", linewidth=1.5,\n            label=f\"Best F1 @ {experiment_best_threshold:.2f}\")\nax2.set_xlabel(\"Distance Threshold\")\nax2.set_ylabel(\"Score\")\nax2.set_title(\"P/R/F1/MRR vs Threshold\")\nax2.legend(fontsize=8)\nax2.set_ylim(0, 1.05)\nax2.grid(True, alpha=0.3)\nfig2.tight_layout()\nfig2.savefig(FIGURES_DIR / \"threshold_sweep_metrics.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Saved: {FIGURES_DIR / 'threshold_sweep_metrics.png'}\")\n\nprint(f\"\\nOptimal F1 threshold: {experiment_best_threshold:.2f}\")\nprint(f\"  F1:        {experiment_f1_scores[experiment_best_f1_index]:.3f}\")\nprint(f\"  Precision: {experiment_precisions[experiment_best_f1_index]:.3f}\")\nprint(f\"  Recall:    {experiment_recalls[experiment_best_f1_index]:.3f}\")\nprint(f\"  MRR:       {experiment_mrr_scores[experiment_best_f1_index]:.3f}\")\nprint(f\"  (all metrics macro-averaged: per-experiment, then mean)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ttksiuwzcbl",
   "metadata": {},
   "outputs": [],
   "source": "# Threshold table (macro-averaged per experiment, with MRR)\nprint(f\"Threshold table (macro-averaged: per-experiment P/R/F1/MRR, then mean):\")\nprint(f\"Database has {num_unique_memories} unique memories, {len(experiments_deduped)} test cases\")\nprint()\nprint(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8} {'Avg Accepted':>13} {'Avg GT Kept':>12} {'Avg GT Lost':>12}\")\nprint(\"-\" * 95)\n\nfor threshold in [0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.05]:\n    precisions, recalls, f1_scores, reciprocal_ranks = [], [], [], []\n    num_accepted_list, ground_truth_kept_list = [], []\n    \n    for experiment_data in experiments_deduped:\n        accepted = {memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= threshold}\n        ground_truth_accepted = len(accepted & experiment_data[\"ground_truth_ids\"])\n        num_accepted = len(accepted)\n        ground_truth_count = len(experiment_data[\"ground_truth_ids\"])\n\n        precision = ground_truth_accepted / num_accepted if num_accepted > 0 else 0.0\n        recall = ground_truth_accepted / ground_truth_count if ground_truth_count > 0 else 0.0\n        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1_score)\n        num_accepted_list.append(num_accepted)\n        ground_truth_kept_list.append(ground_truth_accepted)\n\n        accepted_sorted = sorted(\n            [(memory_id, distance) for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= threshold],\n            key=lambda x: x[1]\n        )\n        reciprocal_rank = 0.0\n        for rank, (memory_id, _) in enumerate(accepted_sorted, 1):\n            if memory_id in experiment_data[\"ground_truth_ids\"]:\n                reciprocal_rank = 1.0 / rank\n                break\n        reciprocal_ranks.append(reciprocal_rank)\n\n    avg_precision, avg_recall = np.mean(precisions), np.mean(recalls)\n    avg_f1_score, avg_mrr = np.mean(f1_scores), np.mean(reciprocal_ranks)\n    avg_num_accepted = np.mean(num_accepted_list)\n    avg_ground_truth_kept = np.mean(ground_truth_kept_list)\n    avg_ground_truth_count = np.mean([len(experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped])\n    avg_ground_truth_lost = avg_ground_truth_count - avg_ground_truth_kept\n\n    marker = \" <--\" if abs(threshold - experiment_best_threshold) < 0.015 else \"\"\n    print(f\"{threshold:>10.2f} {avg_precision:>10.3f} {avg_recall:>8.3f} {avg_f1_score:>8.3f} {avg_mrr:>8.3f} {avg_num_accepted:>13.1f} {avg_ground_truth_kept:>12.1f} {avg_ground_truth_lost:>12.1f}{marker}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-experiment",
   "metadata": {},
   "outputs": [],
   "source": "# Per-experiment impact of applying the best threshold (with MRR)\nprint(f\"Impact of threshold={experiment_best_threshold:.2f} per experiment:\\n\")\nprint(f\"{'Test Case':<25} {'Recall':>7} {'Prec':>7} {'F1':>7} {'MRR':>7} {'GT':>4} {'Kept':>5} {'Lost':>5} {'Accepted':>9}\")\nprint(\"-\" * 85)\n\nper_experiment_mrrs = []\nfor experiment_data in experiments_deduped:\n    # Apply threshold and sort by distance\n    accepted_sorted = sorted(\n        [(memory_id, distance) for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= experiment_best_threshold],\n        key=lambda x: x[1]\n    )\n    accepted_ids = {memory_id for memory_id, _ in accepted_sorted}\n    ground_truth_accepted = accepted_ids & experiment_data[\"ground_truth_ids\"]\n    ground_truth_lost = experiment_data[\"ground_truth_ids\"] - accepted_ids\n\n    recall = len(ground_truth_accepted) / len(experiment_data[\"ground_truth_ids\"]) if experiment_data[\"ground_truth_ids\"] else 0\n    precision = len(ground_truth_accepted) / len(accepted_ids) if accepted_ids else 0\n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n\n    # MRR: reciprocal rank of first GT hit in accepted results (sorted by distance)\n    reciprocal_rank = 0.0\n    for rank, (memory_id, _) in enumerate(accepted_sorted, 1):\n        if memory_id in experiment_data[\"ground_truth_ids\"]:\n            reciprocal_rank = 1.0 / rank\n            break\n    per_experiment_mrrs.append(reciprocal_rank)\n\n    print(f\"{experiment_data['test_case_id']:<25} {recall:>7.1%} {precision:>7.1%} {f1_score:>7.3f} {reciprocal_rank:>7.3f} {len(experiment_data['ground_truth_ids']):>4} {len(ground_truth_accepted):>5} {len(ground_truth_lost):>5} {len(accepted_ids):>9}\")\n\n    if ground_truth_lost:\n        for memory_id in sorted(ground_truth_lost):\n            distance = experiment_data[\"best_distances\"].get(memory_id)\n            distance_str = f\"{distance:.4f}\" if distance is not None else \"NOT RETRIEVED\"\n            print(f\"  {'':25} Lost: {memory_id} (best d={distance_str})\")\n\nprint(\"-\" * 85)\navg_num_accepted = np.mean([len({memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= experiment_best_threshold}) for experiment_data in experiments_deduped])\nprint(f\"{'AVERAGE':<25} {np.mean([len({memory_id for memory_id, distance in experiment_data['best_distances'].items() if distance <= experiment_best_threshold} & experiment_data['ground_truth_ids']) / len(experiment_data['ground_truth_ids']) for experiment_data in experiments_deduped]):>7.1%} {'':>7} {'':>7} {np.mean(per_experiment_mrrs):>7.3f} {'':>4} {'':>5} {'':>5} {avg_num_accepted:>9.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": "# Scatter plot: deduplicated — one point per unique memory per experiment (best distance)\nfig, ax = plt.subplots(figsize=(14, 6))\n\nexperiment_names = [experiment_data[\"test_case_id\"].replace(\"tc_\", \"\")[:30] for experiment_data in experiments_deduped]\nx_positions = []\ncolors = []\ndistances_to_plot = []\n\nfor experiment_index, experiment_data in enumerate(experiments_deduped):\n    for memory_id, distance in experiment_data[\"best_distances\"].items():\n        x_positions.append(experiment_index + np.random.uniform(-0.3, 0.3))\n        distances_to_plot.append(distance)\n        colors.append(\"#2ecc71\" if memory_id in experiment_data[\"ground_truth_ids\"] else \"#e74c3c\")\n\nax.scatter(x_positions, distances_to_plot, c=colors, alpha=0.5, s=25, edgecolors=\"white\", linewidth=0.3)\nax.axhline(experiment_best_threshold, color=\"#3498db\", linestyle=\"--\", linewidth=2,\n           label=f\"Optimal threshold: {experiment_best_threshold:.2f}\")\n\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0], [0], marker='o', color='w', markerfacecolor='#2ecc71', markersize=8, label='Ground Truth'),\n    Line2D([0], [0], marker='o', color='w', markerfacecolor='#e74c3c', markersize=8, label='Non-GT'),\n    Line2D([0], [0], color='#3498db', linestyle='--', linewidth=2, label=f'Threshold: {experiment_best_threshold:.2f}'),\n]\nax.legend(handles=legend_elements, loc=\"upper left\")\n\nax.set_xticks(range(len(experiment_names)))\nax.set_xticklabels(experiment_names, rotation=30, ha=\"right\", fontsize=8)\nax.set_ylabel(\"Cosine Distance\")\nax.set_title(\"Unique Memories by Experiment — best distance (below threshold = accepted)\")\nax.grid(True, alpha=0.3, axis=\"y\")\n\nplt.tight_layout()\nplt.show()\n\n# Print counts\ntotal_raw = sum(\n    len(result) for experiment in experiments for query_result in experiment.get(\"queries\", []) for result in [query_result.get(\"results\", [])]\n)\nnum_ground_truth_plotted = colors.count('#2ecc71')\nnum_non_ground_truth_plotted = colors.count('#e74c3c')\nprint(f\"\\nRaw result rows (before dedup): {total_raw}\")\nprint(f\"Unique memories plotted:        {len(distances_to_plot)} ({len(distances_to_plot)//len(experiments_deduped):.0f} avg/experiment)\")\nprint(f\"  GT:     {num_ground_truth_plotted} ({num_ground_truth_plotted//len(experiments_deduped):.0f} avg/experiment)\")\nprint(f\"  Non-GT: {num_non_ground_truth_plotted} ({num_non_ground_truth_plotted//len(experiments_deduped):.0f} avg/experiment)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence-buckets",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze the current confidence buckets from db.py vs data-driven buckets\n# Uses per-observation data (GT observations are unique; non-GT repeated across experiments)\ncurrent_buckets = [\n    (\"high\",     0.0,  0.5),\n    (\"medium\",   0.5,  0.8),\n    (\"low\",      0.8,  1.2),\n    (\"very_low\", 1.2,  2.0),\n]\n\nprint(f\"Current confidence buckets (db.py) vs actual data:\")\nprint(f\"NOTE: GT observations are unique (n={len(ground_truth_obs)}), non-GT are repeated across experiments (n={len(non_ground_truth_obs)} from {num_unique_memories} memories)\")\nprint()\nprint(f\"{'Bucket':<10} {'Range':<15} {'GT Obs':>10} {'Non-GT Obs':>12} {'GT %':>8} {'Obs Precision':>14}\")\nprint(\"-\" * 75)\n\nfor bucket_name, range_low, range_high in current_buckets:\n    ground_truth_in_bucket = int(np.sum((ground_truth_obs >= range_low) & (ground_truth_obs < range_high)))\n    non_ground_truth_in_bucket = int(np.sum((non_ground_truth_obs >= range_low) & (non_ground_truth_obs < range_high)))\n    total_in_bucket = ground_truth_in_bucket + non_ground_truth_in_bucket\n    ground_truth_percentage = ground_truth_in_bucket / len(ground_truth_obs) * 100 if len(ground_truth_obs) > 0 else 0\n    observation_precision = ground_truth_in_bucket / total_in_bucket if total_in_bucket > 0 else 0\n    print(f\"{bucket_name:<10} [{range_low:.1f}, {range_high:.1f}){'':<5} {ground_truth_in_bucket:>10} {non_ground_truth_in_bucket:>12} {ground_truth_percentage:>7.1f}% {observation_precision:>14.1%}\")\n\nprint(f\"\\nRecommendation: Update confidence buckets based on data distribution.\")\nprint(f\"The optimal F1 threshold ({experiment_best_threshold:.2f}) should be the cutoff between 'accepted' and 'rejected'.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "# Final summary and recommendation (macro-averaged, with MRR)\nprint(\"=\" * 70)\nprint(\"THRESHOLD ANALYSIS SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\nDatabase: {num_unique_memories} unique memories\")\nprint(f\"Test cases: {len(experiments_deduped)} (GT sets are disjoint, {total_ground_truth_memories} total GT assignments)\")\navg_retrieved = np.mean([len(experiment_data[\"best_distances\"]) for experiment_data in experiments_deduped])\nprint(f\"Avg memories retrieved per experiment: {avg_retrieved:.1f} / {num_unique_memories}\")\n\nprint(f\"\\nOptimal F1 threshold: {experiment_best_threshold:.2f}\")\nprint(f\"  P={experiment_precisions[experiment_best_f1_index]:.1%}, R={experiment_recalls[experiment_best_f1_index]:.1%}, \"\n      f\"F1={experiment_f1_scores[experiment_best_f1_index]:.3f}, MRR={experiment_mrr_scores[experiment_best_f1_index]:.3f}\")\nprint(f\"  (all metrics macro-averaged: per-experiment, then mean)\")\n\n# Retrieval counts at optimal threshold\navg_num_accepted = np.mean([len({memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= experiment_best_threshold}) for experiment_data in experiments_deduped])\navg_ground_truth_kept = np.mean([len({memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= experiment_best_threshold} & experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped])\navg_ground_truth_count = np.mean([len(experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped])\nprint(f\"\\n--- Memories retrieved at threshold={experiment_best_threshold:.2f} (avg per experiment) ---\")\nprint(f\"  {avg_num_accepted:.1f} / {avg_retrieved:.1f} retrieved memories accepted\")\nprint(f\"    GT kept:  {avg_ground_truth_kept:.1f} / {avg_ground_truth_count:.1f}\")\nprint(f\"    Non-GT:   {avg_num_accepted - avg_ground_truth_kept:.1f}\")\n\n# Data-driven tradeoff points\nprint(f\"\\n--- Key Tradeoff Points (data-driven) ---\")\ntradeoff_points = [(\"Best F1\", experiment_best_threshold)]\n\n# Find lowest threshold where macro-avg recall >= 0.9\nrecall_90_candidates = experiment_thresholds[experiment_recalls >= 0.9]\nif len(recall_90_candidates) > 0:\n    threshold_recall_90 = recall_90_candidates[0]\n    tradeoff_points.append((\"Recall >= 90%\", threshold_recall_90))\n\n# Find lowest threshold where macro-avg recall >= 0.95\nrecall_95_candidates = experiment_thresholds[experiment_recalls >= 0.95]\nif len(recall_95_candidates) > 0:\n    threshold_recall_95 = recall_95_candidates[0]\n    tradeoff_points.append((\"Recall >= 95%\", threshold_recall_95))\n\n# Find highest threshold where macro-avg precision >= 0.9\nprecision_90_candidates = experiment_thresholds[experiment_precisions >= 0.9]\nif len(precision_90_candidates) > 0:\n    threshold_precision_90 = precision_90_candidates[-1]\n    tradeoff_points.append((\"Precision >= 90%\", threshold_precision_90))\n\n# Sort by threshold for readability\ntradeoff_points.sort(key=lambda x: x[1])\n\nfor label, threshold in tradeoff_points:\n    index = np.argmin(np.abs(experiment_thresholds - threshold))\n    avg_accepted_at_threshold = np.mean([len({memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= threshold}) for experiment_data in experiments_deduped])\n    avg_ground_truth_at_threshold = np.mean([len({memory_id for memory_id, distance in experiment_data[\"best_distances\"].items() if distance <= threshold} & experiment_data[\"ground_truth_ids\"]) for experiment_data in experiments_deduped])\n    print(f\"  t={threshold:.2f} ({label:>17}): P={experiment_precisions[index]:.1%}, R={experiment_recalls[index]:.1%}, \"\n          f\"F1={experiment_f1_scores[index]:.3f}, MRR={experiment_mrr_scores[index]:.3f}, \"\n          f\"avg accepted={avg_accepted_at_threshold:.1f} ({avg_ground_truth_at_threshold:.1f} GT + {avg_accepted_at_threshold-avg_ground_truth_at_threshold:.1f} non-GT)\")\n\nprint(f\"\\n--- Recommendation ---\")\nprint(f\"The GT and non-GT distributions overlap significantly (GT median={np.median(ground_truth_obs):.3f}, \"\n      f\"non-GT median={np.median(non_ground_truth_obs):.3f}).\")\nprint(f\"A single distance threshold cannot cleanly separate them.\")\nprint(f\"\")\nprint(f\"Practical options:\")\nprint(f\"  1. Use threshold={experiment_best_threshold:.2f} as DEFAULT_DISTANCE_THRESHOLD for best F1\")\nprint(f\"  2. Use a higher threshold to preserve recall, accepting lower precision\")\nprint(f\"  3. Combine threshold with additional signals (RRF scoring, re-ranking)\")\nprint(f\"     to improve separation beyond what distance alone provides\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1da9d-4744-4631-8b71-5b27b3558911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}