{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1 — Setup & Imports\nimport os\nfrom pathlib import Path\n\nfrom memory_retrieval.memories.extractor import extract_memories, ExtractionConfig, SituationFormat\nfrom memory_retrieval.search.vector import VectorBackend\nfrom memory_retrieval.experiments.runner import run_experiment, run_all_experiments, ExperimentConfig\nfrom memory_retrieval.experiments.query_generation import generate_all_queries, QueryGenerationConfig\nfrom memory_retrieval.experiments.test_cases import build_test_cases\nfrom memory_retrieval.infra.io import load_json\n\n# Find project root by walking up to pyproject.toml\nPROJECT_ROOT = Path.cwd()\nwhile not (PROJECT_ROOT / \"pyproject.toml\").exists():\n    if PROJECT_ROOT == PROJECT_ROOT.parent:\n        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n    PROJECT_ROOT = PROJECT_ROOT.parent\nos.chdir(PROJECT_ROOT)\n\n# Verify API key\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and query generation will fail.\")\nelse:\n    print(\"OPENROUTER_API_KEY is set.\")\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(\"Imports OK.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2 — Configuration\nfrom memory_retrieval.infra.runs import create_run, get_latest_run, get_run, list_runs, update_run_status\n\nPROMPT_VERSION = \"2.0.0\"                        # semantic versioning\nMODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"   # LLM for memory extraction\nMODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\" # LLM for query generation\n\nRAW_DATA_DIR = \"data/review_data\"\n\n# Run selection: use latest run or select a specific one\n# To see available runs: print(list_runs(\"phase1\"))\n# To select specific run: RUN_DIR = get_run(\"phase1\", \"run_20260208_143022\")\nRUN_DIR = get_latest_run(\"phase1\")\n\n# Derived paths (automatic from run directory)\nMEMORIES_DIR = str(RUN_DIR / \"memories\")\nDB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\nTEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\nQUERIES_DIR = str(RUN_DIR / \"queries\")\nRESULTS_DIR = str(RUN_DIR / \"results\")\n\n# Initialize backends\nvector_backend = VectorBackend()\n\nprint(\"Configuration:\")\nprint(f\"  Using run: {RUN_DIR.name}\")\nprint(f\"  Prompt version: {PROMPT_VERSION}\")\nprint(f\"  Model (memories): {MODEL_MEMORIES}\")\nprint(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\nprint(f\"  Raw data dir: {RAW_DATA_DIR}\")\nprint(f\"  Memories dir: {MEMORIES_DIR}\")\nprint(f\"  DB path: {DB_PATH}\")\nprint(f\"  Test cases dir: {TEST_CASES_DIR}\")\nprint(f\"  Queries dir: {QUERIES_DIR}\")\nprint(f\"  Results dir: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words describing the code pattern/issue) and an **actionable lesson** (imperative guidance, max 160 chars).\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-single",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4 — Build Memories: Single File\n# Creates a new run if none exists\n\nif RUN_DIR is None:\n    run_id, RUN_DIR = create_run(\"phase1\")\n    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n    QUERIES_DIR = str(RUN_DIR / \"queries\")\n    RESULTS_DIR = str(RUN_DIR / \"results\")\n    print(f\"Created new run: {run_id}\")\n\nraw_data_path = Path(RAW_DATA_DIR)\nraw_files = sorted(raw_data_path.glob(\"*.json\"))\n\nprint(f\"Found {len(raw_files)} raw data files:\")\nfor i, f in enumerate(raw_files):\n    print(f\"  [{i}] {f.name}\")\n\nif raw_files:\n    # Process the first file (change index to pick a different one)\n    target_file = raw_files[0]\n    print(f\"\\nProcessing: {target_file.name}\")\n    extraction_config = ExtractionConfig(\n        situation_format=SituationFormat.SINGLE,\n        prompts_dir=\"data/prompts/phase1\",\n        prompt_version=PROMPT_VERSION,\n        model=MODEL_MEMORIES,\n    )\n    output_path = extract_memories(\n        raw_path=str(target_file),\n        out_dir=MEMORIES_DIR,\n        config=extraction_config,\n    )\n    print(f\"Output saved to: {output_path}\")\nelse:\n    print(\"No raw data files found.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-all",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5 — Build Memories: All Files\n# Creates a new run if none exists\n\nif RUN_DIR is None:\n    run_id, RUN_DIR = create_run(\"phase1\")\n    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n    QUERIES_DIR = str(RUN_DIR / \"queries\")\n    RESULTS_DIR = str(RUN_DIR / \"results\")\n    print(f\"Created new run: {run_id}\")\n\nraw_data_path = Path(RAW_DATA_DIR)\nraw_files = sorted(raw_data_path.glob(\"*.json\"))\n\nprint(f\"Processing all {len(raw_files)} raw data files...\\n\")\n\nextraction_config = ExtractionConfig(\n    situation_format=SituationFormat.SINGLE,\n    prompts_dir=\"data/prompts/phase1\",\n    prompt_version=PROMPT_VERSION,\n    model=MODEL_MEMORIES,\n)\n\nresults = []\nfor f in raw_files:\n    print(f\"Processing: {f.name}\")\n    try:\n        output_path = extract_memories(\n            raw_path=str(f),\n            out_dir=MEMORIES_DIR,\n            config=extraction_config,\n        )\n        results.append({\"file\": f.name, \"output\": output_path, \"status\": \"ok\"})\n    except Exception as e:\n        results.append({\"file\": f.name, \"output\": None, \"status\": str(e)})\n        print(f\"  ERROR: {e}\")\n\nsuccess_count = sum(1 for r in results if r['status'] == 'ok')\nprint(f\"\\nSummary: {success_count}/{len(results)} files processed successfully.\")\n\n# Update run status\nupdate_run_status(RUN_DIR, \"build_memories\", {\n    \"count\": success_count,\n    \"failed\": len(results) - success_count,\n    \"prompt_version\": PROMPT_VERSION,\n})"
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rebuild-database",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7 — Rebuild Database\nprint(f\"Rebuilding database for run: {RUN_DIR.name}...\")\nvector_backend.rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n\ncount = vector_backend.get_memory_count(DB_PATH)\nprint(f\"Database rebuilt. Total memories indexed: {count}\")\n\n# Update run status\nupdate_run_status(RUN_DIR, \"db\", {\"memory_count\": count})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-database",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8 — Verify Database: Sample Search\nfrom memory_retrieval.search.vector import get_confidence_from_distance\n\nsample_query = \"error handling in async functions\"\nprint(f\"Sample search: \\\"{sample_query}\\\"\\n\")\n\nresults = vector_backend.search(db_path=DB_PATH, query=sample_query, limit=5)\n\nif results:\n    for i, result in enumerate(results):\n        print(f\"--- Result {i + 1} (distance: {result.raw_score:.4f}) ---\")\n        print(f\"  ID: {result.id}\")\n        print(f\"  Situation: {result.situation}\")\n        print(f\"  Lesson: {result.lesson}\")\n        print()\nelse:\n    print(\"No results found. Check that the database is populated and Ollama is running.\")"
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-test-cases",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10 — Build Test Cases\nprint(f\"Building test cases for run: {RUN_DIR.name}...\\n\")\nbuild_test_cases(\n    raw_dir=RAW_DATA_DIR,\n    memories_dir=MEMORIES_DIR,\n    output_dir=TEST_CASES_DIR,\n)\n\ntest_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\nprint(f\"\\nGenerated {len(test_case_files)} test cases:\")\nfor test_case_file in test_case_files:\n    test_case = load_json(str(test_case_file))\n    ground_truth_count = test_case.get(\"ground_truth_count\", len(test_case.get(\"ground_truth_memory_ids\", [])))\n    print(f\"  {test_case_file.name} — {ground_truth_count} ground truth memories\")\n\n# Update run status\nupdate_run_status(RUN_DIR, \"test_cases\", {\"count\": len(test_case_files)})"
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": "## Step 4 — Generate Queries\n\nGenerates search queries from each test case's PR context and diff via LLM.\nQueries are saved as separate JSON files in the `queries/` directory so they can be\nreused across multiple experiment runs without re-calling the API.\n\nRequires `OPENROUTER_API_KEY`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-single-experiment",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12 — Generate Queries for All Test Cases\nprint(f\"Generating queries for run: {RUN_DIR.name}...\\n\")\n\nquery_config = QueryGenerationConfig(\n    prompts_dir=\"data/prompts/phase1\",\n    prompt_version=PROMPT_VERSION,\n    model=MODEL_EXPERIMENT,\n)\nall_query_data = generate_all_queries(\n    test_cases_dir=TEST_CASES_DIR,\n    queries_dir=QUERIES_DIR,\n    config=query_config,\n    db_path=DB_PATH,\n    search_backend=vector_backend,\n)\n\nsuccessful_queries = [data for data in all_query_data if \"queries\" in data]\ntotal_queries = sum(len(data[\"queries\"]) for data in successful_queries)\nprint(f\"\\nGenerated queries for {len(successful_queries)} test cases ({total_queries} total queries)\")\n\n# Update run status\nupdate_run_status(RUN_DIR, \"query_generation\", {\n    \"count\": len(successful_queries),\n    \"total_queries\": total_queries,\n    \"model\": MODEL_EXPERIMENT,\n    \"prompt_version\": PROMPT_VERSION,\n})"
  },
  {
   "cell_type": "markdown",
   "id": "swtq98gsvza",
   "source": "## Step 5 — Run Experiments\n\nFor each test case, the experiment:\n1. Loads pre-generated search queries from the `queries/` directory\n2. Runs vector similarity search against the database\n3. Computes **recall**, **precision**, and **F1** against the ground truth\n\nRequires Ollama with `mxbai-embed-large`. Does NOT require `OPENROUTER_API_KEY`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 13 — Run All Experiments\nprint(f\"Running all experiments for run: {RUN_DIR.name}...\\n\")\n\nconfig = ExperimentConfig(\n    search_backend=vector_backend,\n    search_limit=20,\n    distance_threshold=1.1,\n)\nall_results = run_all_experiments(\n    test_cases_dir=TEST_CASES_DIR,\n    queries_dir=QUERIES_DIR,\n    db_path=DB_PATH,\n    results_dir=RESULTS_DIR,\n    config=config,\n)\n\nprint(f\"\\nCompleted {len(all_results)} experiments.\\n\")\nprint(f\"{'Test Case':<40} {'Recall':>8} {'Precision':>10} {'F1':>8}\")\nprint(\"-\" * 70)\nfor result in all_results:\n    test_case_name = result.get(\"test_case_id\", \"?\")[:40]\n    metrics = result.get(\"metrics\", {})\n    print(f\"{test_case_name:<40} {metrics.get('recall', 0):>8.3f} {metrics.get('precision', 0):>10.3f} {metrics.get('f1', 0):>8.3f}\")\n\nif all_results:\n    metrics_list = [result.get(\"metrics\", {}) for result in all_results]\n    avg_recall = sum(metrics.get(\"recall\", 0) for metrics in metrics_list) / len(metrics_list)\n    avg_precision = sum(metrics.get(\"precision\", 0) for metrics in metrics_list) / len(metrics_list)\n    avg_f1_score = sum(metrics.get(\"f1\", 0) for metrics in metrics_list) / len(metrics_list)\n    print(\"-\" * 70)\n    print(f\"{'AVERAGE':<40} {avg_recall:>8.3f} {avg_precision:>10.3f} {avg_f1_score:>8.3f}\")\n\n# Update run status\nsuccessful = [result for result in all_results if \"metrics\" in result]\nupdate_run_status(RUN_DIR, \"experiment\", {\n    \"count\": len(successful),\n    \"failed\": len(all_results) - len(successful),\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-summary",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 14 — Results Analysis: Top@K & Distance Threshold Sweep (with MRR)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# === Configuration ===\nDISTANCE_THRESHOLDS = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.10]\nMAX_K = 20\n\nFIGURES_DIR = Path(\"notebooks/phase1/figures\")\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\nresults_path = Path(RESULTS_DIR)\nresult_files = sorted(results_path.glob(\"*.json\"))\n\nif not result_files:\n    print(\"No result files found. Run experiments first.\")\nelse:\n    all_data = [load_json(str(file_path)) for file_path in result_files]\n    num_test_cases = len(all_data)\n\n    # --- Helper: deduplicate results per test case (best distance per memory) ---\n    def get_best_distances(data):\n        best_by_memory_id = {}\n        for query_result in data.get(\"queries\", []):\n            for result in query_result.get(\"results\", []):\n                memory_id = result[\"id\"]\n                distance = result[\"distance\"]\n                if memory_id not in best_by_memory_id or distance < best_by_memory_id[memory_id]:\n                    best_by_memory_id[memory_id] = distance\n        return sorted(best_by_memory_id.items(), key=lambda x: x[1])\n\n    # --- Helper: compute reciprocal rank for a ranked list ---\n    def reciprocal_rank(sorted_memory_ids, ground_truth_ids):\n        \"\"\"Return 1/rank of the first GT hit, or 0 if none found.\"\"\"\n        for rank, memory_id in enumerate(sorted_memory_ids, 1):\n            if memory_id in ground_truth_ids:\n                return 1.0 / rank\n        return 0.0\n\n    # =========================================================================\n    # Top@K Analysis (per-test-case: pool all queries, deduplicate, take top k)\n    # =========================================================================\n    k_values = list(range(1, MAX_K + 1))\n    all_precisions_by_k = {k: [] for k in k_values}\n    all_recalls_by_k = {k: [] for k in k_values}\n    all_f1_scores_by_k = {k: [] for k in k_values}\n    all_reciprocal_ranks_by_k = {k: [] for k in k_values}  # reciprocal ranks for MRR@k\n\n    for data in all_data:\n        ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n        ground_truth_count = len(ground_truth_ids)\n        sorted_memories = get_best_distances(data)\n\n        for k in k_values:\n            top_k = sorted_memories[:k]\n            top_k_ids = {memory_id for memory_id, _ in top_k}\n            ground_truth_in_top_k = len(top_k_ids & ground_truth_ids)\n            actual_k = min(k, len(sorted_memories))\n\n            precision = ground_truth_in_top_k / actual_k if actual_k > 0 else 0.0\n            recall = ground_truth_in_top_k / ground_truth_count if ground_truth_count > 0 else 0.0\n            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n            reciprocal_rank_value = reciprocal_rank([memory_id for memory_id, _ in top_k], ground_truth_ids)\n\n            all_precisions_by_k[k].append(precision)\n            all_recalls_by_k[k].append(recall)\n            all_f1_scores_by_k[k].append(f1_score)\n            all_reciprocal_ranks_by_k[k].append(reciprocal_rank_value)\n\n    avg_precisions = [np.mean(all_precisions_by_k[k]) for k in k_values]\n    avg_recalls = [np.mean(all_recalls_by_k[k]) for k in k_values]\n    avg_f1_scores = [np.mean(all_f1_scores_by_k[k]) for k in k_values]\n    avg_mrr_scores = [np.mean(all_reciprocal_ranks_by_k[k]) for k in k_values]\n\n    # Top@K table\n    print(f\"Top@K Results (averaged over {num_test_cases} test cases)\\n\")\n    print(f\"{'K':>4} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8}\")\n    print(\"-\" * 42)\n    for k in [1, 2, 3, 4, 5, 10, 15, 20]:\n        if k <= MAX_K:\n            index = k - 1\n            print(f\"{k:>4} {avg_precisions[index]:>10.3f} {avg_recalls[index]:>8.3f} {avg_f1_scores[index]:>8.3f} {avg_mrr_scores[index]:>8.3f}\")\n\n    # Top@K plot — single axis\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(k_values, avg_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=4)\n    ax.plot(k_values, avg_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2, marker=\"s\", markersize=4)\n    ax.plot(k_values, avg_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=4)\n    ax.plot(k_values, avg_mrr_scores, label=\"MRR@K\", color=\"#e67e22\", linewidth=2, marker=\"D\", markersize=5)\n    ax.set_xlabel(\"K (top results kept per test case)\")\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"Precision / Recall / F1 / MRR vs Top@K (avg over {num_test_cases} test cases)\")\n    ax.set_xticks(k_values)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    for k in [1, 5, 10]:\n        index = k - 1\n        ax.annotate(f\"MRR={avg_mrr_scores[index]:.3f}\", (k, avg_mrr_scores[index]),\n                     textcoords=\"offset points\", xytext=(8, 8), fontsize=9,\n                     arrowprops=dict(arrowstyle=\"->\", color=\"#e67e22\", lw=0.8))\n    plt.tight_layout()\n    fig.savefig(FIGURES_DIR / \"topk_metrics.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved: {FIGURES_DIR / 'topk_metrics.png'}\")\n\n    # =========================================================================\n    # Distance Threshold Sweep (no top@k limit, uses all deduplicated results)\n    # =========================================================================\n    print(f\"\\nDistance Threshold Results (averaged over {num_test_cases} test cases)\\n\")\n    print(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8}\")\n    print(\"-\" * 48)\n\n    sweep_precisions, sweep_recalls, sweep_f1_scores, sweep_mrr_scores = [], [], [], []\n    for threshold in DISTANCE_THRESHOLDS:\n        threshold_precisions, threshold_recalls, threshold_f1_scores, threshold_reciprocal_ranks = [], [], [], []\n        for data in all_data:\n            ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n            ground_truth_count = len(ground_truth_ids)\n            sorted_memories = get_best_distances(data)\n\n            accepted = [(memory_id, distance) for memory_id, distance in sorted_memories if distance <= threshold]\n            accepted_ids = {memory_id for memory_id, _ in accepted}\n            ground_truth_accepted = len(accepted_ids & ground_truth_ids)\n            num_accepted = len(accepted_ids)\n\n            precision = ground_truth_accepted / num_accepted if num_accepted > 0 else 0.0\n            recall = ground_truth_accepted / ground_truth_count if ground_truth_count > 0 else 0.0\n            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n            reciprocal_rank_value = reciprocal_rank([memory_id for memory_id, _ in accepted], ground_truth_ids)\n\n            threshold_precisions.append(precision)\n            threshold_recalls.append(recall)\n            threshold_f1_scores.append(f1_score)\n            threshold_reciprocal_ranks.append(reciprocal_rank_value)\n\n        sweep_precisions.append(np.mean(threshold_precisions))\n        sweep_recalls.append(np.mean(threshold_recalls))\n        sweep_f1_scores.append(np.mean(threshold_f1_scores))\n        sweep_mrr_scores.append(np.mean(threshold_reciprocal_ranks))\n        print(f\"{threshold:>10.2f} {sweep_precisions[-1]:>10.3f} {sweep_recalls[-1]:>8.3f} {sweep_f1_scores[-1]:>8.3f} {sweep_mrr_scores[-1]:>8.3f}\")\n\n    # Threshold sweep plot — single axis\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(DISTANCE_THRESHOLDS, sweep_precisions, label=\"Precision\", color=\"#3498db\", linewidth=2, marker=\"o\", markersize=5)\n    ax.plot(DISTANCE_THRESHOLDS, sweep_recalls, label=\"Recall\", color=\"#2ecc71\", linewidth=2, marker=\"s\", markersize=5)\n    ax.plot(DISTANCE_THRESHOLDS, sweep_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=5)\n    ax.plot(DISTANCE_THRESHOLDS, sweep_mrr_scores, label=\"MRR\", color=\"#e67e22\", linewidth=2, marker=\"D\", markersize=5)\n    ax.set_xlabel(\"Cosine Distance Threshold\")\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"Precision / Recall / F1 / MRR vs Distance Threshold (avg over {num_test_cases} test cases)\")\n    ax.set_xticks(DISTANCE_THRESHOLDS)\n    ax.set_ylim(0, 1.05)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9fa28-8211-467f-8675-2a83a63ceb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}