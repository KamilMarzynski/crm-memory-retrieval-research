{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from memory_retrieval.experiments.query_generation import (\n",
    "    QueryGenerationConfig,\n",
    "    generate_all_queries,\n",
    ")\n",
    "from memory_retrieval.experiments.runner import (\n",
    "    ExperimentConfig,\n",
    "    run_all_experiments,\n",
    ")\n",
    "from memory_retrieval.experiments.test_cases import build_test_cases\n",
    "from memory_retrieval.infra.io import load_json\n",
    "from memory_retrieval.memories.extractor import ExtractionConfig, SituationFormat, extract_memories\n",
    "from memory_retrieval.search.vector import VectorBackend\n",
    "\n",
    "# Find project root by walking up to pyproject.toml\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find project root (pyproject.toml)\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Verify API key\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"WARNING: OPENROUTER_API_KEY is not set. Memory building and query generation will fail.\")\n",
    "else:\n",
    "    print(\"OPENROUTER_API_KEY is set.\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Configuration\n",
    "from memory_retrieval.infra.runs import create_run, get_latest_run, update_run_status\n",
    "\n",
    "PROMPT_VERSION = \"2.0.0\"  # semantic versioning\n",
    "MODEL_MEMORIES = \"anthropic/claude-haiku-4.5\"  # LLM for memory extraction\n",
    "MODEL_EXPERIMENT = \"anthropic/claude-sonnet-4.5\"  # LLM for query generation\n",
    "\n",
    "RAW_DATA_DIR = \"data/review_data\"\n",
    "\n",
    "# Run selection: use latest run or select a specific one\n",
    "# To see available runs: print(list_runs(\"phase1\"))\n",
    "# To select specific run: RUN_DIR = get_run(\"phase1\", \"run_20260208_143022\")\n",
    "RUN_DIR = get_latest_run(\"phase1\")\n",
    "\n",
    "# Derived paths (automatic from run directory)\n",
    "MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "\n",
    "# Initialize backends\n",
    "vector_backend = VectorBackend()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Using run: {RUN_DIR.name}\")\n",
    "print(f\"  Prompt version: {PROMPT_VERSION}\")\n",
    "print(f\"  Model (memories): {MODEL_MEMORIES}\")\n",
    "print(f\"  Model (experiment): {MODEL_EXPERIMENT}\")\n",
    "print(f\"  Raw data dir: {RAW_DATA_DIR}\")\n",
    "print(f\"  Memories dir: {MEMORIES_DIR}\")\n",
    "print(f\"  DB path: {DB_PATH}\")\n",
    "print(f\"  Test cases dir: {TEST_CASES_DIR}\")\n",
    "print(f\"  Queries dir: {QUERIES_DIR}\")\n",
    "print(f\"  Results dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Build Memories\n",
    "\n",
    "Extracts structured memories from raw code review data via LLM.\n",
    "Each memory contains a **situation description** (25-60 words describing the code pattern/issue) and an **actionable lesson** (imperative guidance, max 160 chars).\n",
    "\n",
    "Requires `OPENROUTER_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Build Memories: Single File\n",
    "# Creates a new run if none exists\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(\"phase1\")\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} raw data files:\")\n",
    "for i, f in enumerate(raw_files):\n",
    "    print(f\"  [{i}] {f.name}\")\n",
    "\n",
    "if raw_files:\n",
    "    # Process the first file (change index to pick a different one)\n",
    "    target_file = raw_files[0]\n",
    "    print(f\"\\nProcessing: {target_file.name}\")\n",
    "    extraction_config = ExtractionConfig(\n",
    "        situation_format=SituationFormat.SINGLE,\n",
    "        prompts_dir=\"data/prompts/phase1\",\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "        model=MODEL_MEMORIES,\n",
    "    )\n",
    "    output_path = extract_memories(\n",
    "        raw_path=str(target_file),\n",
    "        out_dir=MEMORIES_DIR,\n",
    "        config=extraction_config,\n",
    "    )\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No raw data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-memories-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Build Memories: All Files\n",
    "# Creates a new run if none exists\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    run_id, RUN_DIR = create_run(\"phase1\")\n",
    "    MEMORIES_DIR = str(RUN_DIR / \"memories\")\n",
    "    DB_PATH = str(RUN_DIR / \"memories\" / \"memories.db\")\n",
    "    TEST_CASES_DIR = str(RUN_DIR / \"test_cases\")\n",
    "    QUERIES_DIR = str(RUN_DIR / \"queries\")\n",
    "    RESULTS_DIR = str(RUN_DIR / \"results\")\n",
    "    print(f\"Created new run: {run_id}\")\n",
    "\n",
    "raw_data_path = Path(RAW_DATA_DIR)\n",
    "raw_files = sorted(raw_data_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing all {len(raw_files)} raw data files...\\n\")\n",
    "\n",
    "extraction_config = ExtractionConfig(\n",
    "    situation_format=SituationFormat.SINGLE,\n",
    "    prompts_dir=\"data/prompts/phase1\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_MEMORIES,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for f in raw_files:\n",
    "    print(f\"Processing: {f.name}\")\n",
    "    try:\n",
    "        output_path = extract_memories(\n",
    "            raw_path=str(f),\n",
    "            out_dir=MEMORIES_DIR,\n",
    "            config=extraction_config,\n",
    "        )\n",
    "        results.append({\"file\": f.name, \"output\": output_path, \"status\": \"ok\"})\n",
    "    except Exception as e:\n",
    "        results.append({\"file\": f.name, \"output\": None, \"status\": str(e)})\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "success_count = sum(1 for r in results if r[\"status\"] == \"ok\")\n",
    "print(f\"\\nSummary: {success_count}/{len(results)} files processed successfully.\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"build_memories\",\n",
    "    {\n",
    "        \"count\": success_count,\n",
    "        \"failed\": len(results) - success_count,\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Create Database\n",
    "\n",
    "Builds a SQLite database with **sqlite-vec** for vector similarity search.\n",
    "Loads all accepted memories from JSONL files and indexes their situation descriptions as 1024-dimensional embeddings (via Ollama `mxbai-embed-large`).\n",
    "\n",
    "Requires Ollama running locally with the `mxbai-embed-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rebuild-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Rebuild Database\n",
    "print(f\"Rebuilding database for run: {RUN_DIR.name}...\")\n",
    "vector_backend.rebuild_database(db_path=DB_PATH, memories_dir=MEMORIES_DIR)\n",
    "\n",
    "count = vector_backend.get_memory_count(DB_PATH)\n",
    "print(f\"Database rebuilt. Total memories indexed: {count}\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"db\", {\"memory_count\": count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Verify Database: Sample Search\n",
    "\n",
    "sample_query = \"error handling in async functions\"\n",
    "print(f'Sample search: \"{sample_query}\"\\n')\n",
    "\n",
    "results = vector_backend.search(db_path=DB_PATH, query=sample_query, limit=5)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"--- Result {i + 1} (distance: {result.raw_score:.4f}) ---\")\n",
    "        print(f\"  ID: {result.id}\")\n",
    "        print(f\"  Situation: {result.situation}\")\n",
    "        print(f\"  Lesson: {result.lesson}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found. Check that the database is populated and Ollama is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Create Test Cases\n",
    "\n",
    "Matches raw PR data to extracted memories to build **ground truth** test cases.\n",
    "Each test case contains the filtered diff, PR context, and the set of memory IDs that should be retrieved.\n",
    "PRs with no matching memories are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-test-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Build Test Cases\n",
    "print(f\"Building test cases for run: {RUN_DIR.name}...\\n\")\n",
    "build_test_cases(\n",
    "    raw_dir=RAW_DATA_DIR,\n",
    "    memories_dir=MEMORIES_DIR,\n",
    "    output_dir=TEST_CASES_DIR,\n",
    ")\n",
    "\n",
    "test_case_files = sorted(Path(TEST_CASES_DIR).glob(\"*.json\"))\n",
    "print(f\"\\nGenerated {len(test_case_files)} test cases:\")\n",
    "for test_case_file in test_case_files:\n",
    "    test_case = load_json(str(test_case_file))\n",
    "    ground_truth_count = test_case.get(\n",
    "        \"ground_truth_count\", len(test_case.get(\"ground_truth_memory_ids\", []))\n",
    "    )\n",
    "    print(f\"  {test_case_file.name} — {ground_truth_count} ground truth memories\")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(RUN_DIR, \"test_cases\", {\"count\": len(test_case_files)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": "## Step 4 — Generate Queries\n\nGenerates search queries from each test case's PR context and diff via LLM.\nQueries are saved as separate JSON files in the `queries/` directory so they can be\nreused across multiple experiment runs without re-calling the API.\n\nRequires `OPENROUTER_API_KEY`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-single-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Generate Queries for All Test Cases\n",
    "print(f\"Generating queries for run: {RUN_DIR.name}...\\n\")\n",
    "\n",
    "query_config = QueryGenerationConfig(\n",
    "    prompts_dir=\"data/prompts/phase1\",\n",
    "    prompt_version=PROMPT_VERSION,\n",
    "    model=MODEL_EXPERIMENT,\n",
    ")\n",
    "all_query_data = generate_all_queries(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    queries_dir=QUERIES_DIR,\n",
    "    config=query_config,\n",
    "    db_path=DB_PATH,\n",
    "    search_backend=vector_backend,\n",
    ")\n",
    "\n",
    "successful_queries = [data for data in all_query_data if \"queries\" in data]\n",
    "total_queries = sum(len(data[\"queries\"]) for data in successful_queries)\n",
    "print(\n",
    "    f\"\\nGenerated queries for {len(successful_queries)} test cases ({total_queries} total queries)\"\n",
    ")\n",
    "\n",
    "# Update run status\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"query_generation\",\n",
    "    {\n",
    "        \"count\": len(successful_queries),\n",
    "        \"total_queries\": total_queries,\n",
    "        \"model\": MODEL_EXPERIMENT,\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swtq98gsvza",
   "metadata": {},
   "source": "## Step 5 — Run Experiments\n\nFor each test case, the experiment:\n1. Loads pre-generated search queries from the `queries/` directory\n2. Runs vector similarity search against the database\n3. Computes **recall**, **precision**, and **F1** against the ground truth\n\nRequires Ollama with `mxbai-embed-large`. Does NOT require `OPENROUTER_API_KEY`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Run All Experiments\n",
    "print(f\"Running all experiments for run: {RUN_DIR.name}...\\n\")\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    search_backend=vector_backend,\n",
    "    search_limit=20,\n",
    "    distance_threshold=1.1,\n",
    ")\n",
    "all_results = run_all_experiments(\n",
    "    test_cases_dir=TEST_CASES_DIR,\n",
    "    queries_dir=QUERIES_DIR,\n",
    "    db_path=DB_PATH,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} experiments.\\n\")\n",
    "print(f\"{'Test Case':<40} {'Recall':>8} {'Precision':>10} {'F1':>8}\")\n",
    "print(\"-\" * 70)\n",
    "for result in all_results:\n",
    "    test_case_name = result.get(\"test_case_id\", \"?\")[:40]\n",
    "    metrics = result.get(\"metrics\", {})\n",
    "    print(\n",
    "        f\"{test_case_name:<40} {metrics.get('recall', 0):>8.3f} {metrics.get('precision', 0):>10.3f} {metrics.get('f1', 0):>8.3f}\"\n",
    "    )\n",
    "\n",
    "if all_results:\n",
    "    metrics_list = [result.get(\"metrics\", {}) for result in all_results]\n",
    "    avg_recall = sum(metrics.get(\"recall\", 0) for metrics in metrics_list) / len(metrics_list)\n",
    "    avg_precision = sum(metrics.get(\"precision\", 0) for metrics in metrics_list) / len(metrics_list)\n",
    "    avg_f1_score = sum(metrics.get(\"f1\", 0) for metrics in metrics_list) / len(metrics_list)\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'AVERAGE':<40} {avg_recall:>8.3f} {avg_precision:>10.3f} {avg_f1_score:>8.3f}\")\n",
    "\n",
    "# Update run status\n",
    "successful = [result for result in all_results if \"metrics\" in result]\n",
    "update_run_status(\n",
    "    RUN_DIR,\n",
    "    \"experiment\",\n",
    "    {\n",
    "        \"count\": len(successful),\n",
    "        \"failed\": len(all_results) - len(successful),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Results Analysis: Top@K & Distance Threshold Sweep (with MRR)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Configuration ===\n",
    "DISTANCE_THRESHOLDS = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 1.10]\n",
    "MAX_K = 20\n",
    "\n",
    "FIGURES_DIR = RUN_DIR / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_path = Path(RESULTS_DIR)\n",
    "result_files = sorted(results_path.glob(\"*.json\"))\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No result files found. Run experiments first.\")\n",
    "else:\n",
    "    all_data = [load_json(str(file_path)) for file_path in result_files]\n",
    "    num_test_cases = len(all_data)\n",
    "\n",
    "    # --- Helper: deduplicate results per test case (best distance per memory) ---\n",
    "    def get_best_distances(data):\n",
    "        best_by_memory_id = {}\n",
    "        for query_result in data.get(\"queries\", []):\n",
    "            for result in query_result.get(\"results\", []):\n",
    "                memory_id = result[\"id\"]\n",
    "                distance = result[\"distance\"]\n",
    "                if memory_id not in best_by_memory_id or distance < best_by_memory_id[memory_id]:\n",
    "                    best_by_memory_id[memory_id] = distance\n",
    "        return sorted(best_by_memory_id.items(), key=lambda x: x[1])\n",
    "\n",
    "    # --- Helper: compute reciprocal rank for a ranked list ---\n",
    "    def reciprocal_rank(sorted_memory_ids, ground_truth_ids):\n",
    "        \"\"\"Return 1/rank of the first GT hit, or 0 if none found.\"\"\"\n",
    "        for rank, memory_id in enumerate(sorted_memory_ids, 1):\n",
    "            if memory_id in ground_truth_ids:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "\n",
    "    # =========================================================================\n",
    "    # Top@K Analysis (per-test-case: pool all queries, deduplicate, take top k)\n",
    "    # =========================================================================\n",
    "    k_values = list(range(1, MAX_K + 1))\n",
    "    all_precisions_by_k = {k: [] for k in k_values}\n",
    "    all_recalls_by_k = {k: [] for k in k_values}\n",
    "    all_f1_scores_by_k = {k: [] for k in k_values}\n",
    "    all_reciprocal_ranks_by_k = {k: [] for k in k_values}  # reciprocal ranks for MRR@k\n",
    "\n",
    "    for data in all_data:\n",
    "        ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "        ground_truth_count = len(ground_truth_ids)\n",
    "        sorted_memories = get_best_distances(data)\n",
    "\n",
    "        for k in k_values:\n",
    "            top_k = sorted_memories[:k]\n",
    "            top_k_ids = {memory_id for memory_id, _ in top_k}\n",
    "            ground_truth_in_top_k = len(top_k_ids & ground_truth_ids)\n",
    "            actual_k = min(k, len(sorted_memories))\n",
    "\n",
    "            precision = ground_truth_in_top_k / actual_k if actual_k > 0 else 0.0\n",
    "            recall = ground_truth_in_top_k / ground_truth_count if ground_truth_count > 0 else 0.0\n",
    "            f1_score = (\n",
    "                2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            )\n",
    "            reciprocal_rank_value = reciprocal_rank(\n",
    "                [memory_id for memory_id, _ in top_k], ground_truth_ids\n",
    "            )\n",
    "\n",
    "            all_precisions_by_k[k].append(precision)\n",
    "            all_recalls_by_k[k].append(recall)\n",
    "            all_f1_scores_by_k[k].append(f1_score)\n",
    "            all_reciprocal_ranks_by_k[k].append(reciprocal_rank_value)\n",
    "\n",
    "    avg_precisions = [np.mean(all_precisions_by_k[k]) for k in k_values]\n",
    "    avg_recalls = [np.mean(all_recalls_by_k[k]) for k in k_values]\n",
    "    avg_f1_scores = [np.mean(all_f1_scores_by_k[k]) for k in k_values]\n",
    "    avg_mrr_scores = [np.mean(all_reciprocal_ranks_by_k[k]) for k in k_values]\n",
    "\n",
    "    # Top@K table\n",
    "    print(f\"Top@K Results (averaged over {num_test_cases} test cases)\\n\")\n",
    "    print(f\"{'K':>4} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8}\")\n",
    "    print(\"-\" * 42)\n",
    "    for k in [1, 2, 3, 4, 5, 10, 15, 20]:\n",
    "        if k <= MAX_K:\n",
    "            index = k - 1\n",
    "            print(\n",
    "                f\"{k:>4} {avg_precisions[index]:>10.3f} {avg_recalls[index]:>8.3f} {avg_f1_scores[index]:>8.3f} {avg_mrr_scores[index]:>8.3f}\"\n",
    "            )\n",
    "\n",
    "    # Top@K plot — single axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(\n",
    "        k_values,\n",
    "        avg_precisions,\n",
    "        label=\"Precision\",\n",
    "        color=\"#3498db\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.plot(\n",
    "        k_values,\n",
    "        avg_recalls,\n",
    "        label=\"Recall\",\n",
    "        color=\"#2ecc71\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.plot(\n",
    "        k_values, avg_f1_scores, label=\"F1\", color=\"#9b59b6\", linewidth=2, marker=\"^\", markersize=4\n",
    "    )\n",
    "    ax.plot(\n",
    "        k_values,\n",
    "        avg_mrr_scores,\n",
    "        label=\"MRR@K\",\n",
    "        color=\"#e67e22\",\n",
    "        linewidth=2,\n",
    "        marker=\"D\",\n",
    "        markersize=5,\n",
    "    )\n",
    "    ax.set_xlabel(\"K (top results kept per test case)\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(f\"Precision / Recall / F1 / MRR vs Top@K (avg over {num_test_cases} test cases)\")\n",
    "    ax.set_xticks(k_values)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for k in [1, 5, 10]:\n",
    "        index = k - 1\n",
    "        ax.annotate(\n",
    "            f\"MRR={avg_mrr_scores[index]:.3f}\",\n",
    "            (k, avg_mrr_scores[index]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(8, 8),\n",
    "            fontsize=9,\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"#e67e22\", lw=0.8),\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / \"topk_metrics.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'topk_metrics.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Distance Threshold Sweep (no top@k limit, uses all deduplicated results)\n",
    "    # =========================================================================\n",
    "    print(f\"\\nDistance Threshold Results (averaged over {num_test_cases} test cases)\\n\")\n",
    "    print(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>8} {'F1':>8} {'MRR':>8}\")\n",
    "    print(\"-\" * 48)\n",
    "\n",
    "    sweep_precisions, sweep_recalls, sweep_f1_scores, sweep_mrr_scores = [], [], [], []\n",
    "    for threshold in DISTANCE_THRESHOLDS:\n",
    "        threshold_precisions, threshold_recalls, threshold_f1_scores, threshold_reciprocal_ranks = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        for data in all_data:\n",
    "            ground_truth_ids = set(data.get(\"ground_truth\", {}).get(\"memory_ids\", []))\n",
    "            ground_truth_count = len(ground_truth_ids)\n",
    "            sorted_memories = get_best_distances(data)\n",
    "\n",
    "            accepted = [\n",
    "                (memory_id, distance)\n",
    "                for memory_id, distance in sorted_memories\n",
    "                if distance <= threshold\n",
    "            ]\n",
    "            accepted_ids = {memory_id for memory_id, _ in accepted}\n",
    "            ground_truth_accepted = len(accepted_ids & ground_truth_ids)\n",
    "            num_accepted = len(accepted_ids)\n",
    "\n",
    "            precision = ground_truth_accepted / num_accepted if num_accepted > 0 else 0.0\n",
    "            recall = ground_truth_accepted / ground_truth_count if ground_truth_count > 0 else 0.0\n",
    "            f1_score = (\n",
    "                2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            )\n",
    "            reciprocal_rank_value = reciprocal_rank(\n",
    "                [memory_id for memory_id, _ in accepted], ground_truth_ids\n",
    "            )\n",
    "\n",
    "            threshold_precisions.append(precision)\n",
    "            threshold_recalls.append(recall)\n",
    "            threshold_f1_scores.append(f1_score)\n",
    "            threshold_reciprocal_ranks.append(reciprocal_rank_value)\n",
    "\n",
    "        sweep_precisions.append(np.mean(threshold_precisions))\n",
    "        sweep_recalls.append(np.mean(threshold_recalls))\n",
    "        sweep_f1_scores.append(np.mean(threshold_f1_scores))\n",
    "        sweep_mrr_scores.append(np.mean(threshold_reciprocal_ranks))\n",
    "        print(\n",
    "            f\"{threshold:>10.2f} {sweep_precisions[-1]:>10.3f} {sweep_recalls[-1]:>8.3f} {sweep_f1_scores[-1]:>8.3f} {sweep_mrr_scores[-1]:>8.3f}\"\n",
    "        )\n",
    "\n",
    "    # Threshold sweep plot — single axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(\n",
    "        DISTANCE_THRESHOLDS,\n",
    "        sweep_precisions,\n",
    "        label=\"Precision\",\n",
    "        color=\"#3498db\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=5,\n",
    "    )\n",
    "    ax.plot(\n",
    "        DISTANCE_THRESHOLDS,\n",
    "        sweep_recalls,\n",
    "        label=\"Recall\",\n",
    "        color=\"#2ecc71\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "        markersize=5,\n",
    "    )\n",
    "    ax.plot(\n",
    "        DISTANCE_THRESHOLDS,\n",
    "        sweep_f1_scores,\n",
    "        label=\"F1\",\n",
    "        color=\"#9b59b6\",\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        markersize=5,\n",
    "    )\n",
    "    ax.plot(\n",
    "        DISTANCE_THRESHOLDS,\n",
    "        sweep_mrr_scores,\n",
    "        label=\"MRR\",\n",
    "        color=\"#e67e22\",\n",
    "        linewidth=2,\n",
    "        marker=\"D\",\n",
    "        markersize=5,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cosine Distance Threshold\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\n",
    "        f\"Precision / Recall / F1 / MRR vs Distance Threshold (avg over {num_test_cases} test cases)\"\n",
    "    )\n",
    "    ax.set_xticks(DISTANCE_THRESHOLDS)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9fa28-8211-467f-8675-2a83a63ceb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}