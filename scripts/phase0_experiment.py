"""
Phase 0 Experiment: Validate memory retrieval using FTS5 keyword search.

This module runs retrieval experiments to measure how well keyword-based search
(using SQLite FTS5 with BM25 ranking) can retrieve relevant engineering memories
when given a code review context.

Experiment Flow:
    1. Load test case (includes pre-filtered diff and ground truth memory IDs)
    2. Generate search queries via LLM from PR context and diff
    3. Execute queries against memories.db using FTS5 search
    4. Calculate recall: (retrieved ground truth) / (total ground truth)
    5. Save detailed results for analysis

Key Metrics:
    - Recall: Percentage of ground truth memories successfully retrieved
    - Query count: Number of search queries generated by LLM
    - Retrieval: Total unique memories retrieved across all queries

Usage:
    # Run single experiment
    uv run python scripts/phase0_experiment.py data/phase0_test_cases/<file>.json

    # Run all experiments
    uv run python scripts/phase0_experiment.py --all

    # Show help
    uv run python scripts/phase0_experiment.py --help

Requirements:
    - OPENROUTER_API_KEY environment variable
    - Test cases generated by phase0_build_test_cases.py
    - memories.db built by phase0_sqlite_fts.py

Example:
    >>> import os
    >>> os.environ["OPENROUTER_API_KEY"] = "sk-..."
    >>> results = run_experiment("data/phase0_test_cases/pr_123.json")
    >>> print(f"Recall: {results['metrics']['recall']:.1%}")
    Recall: 75.0%
"""

import json
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Set

import sys

import requests

# Add scripts directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))
from phase0_common import load_json, save_json, search_memories

# OpenRouter API configuration
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_API_KEY_ENV = "OPENROUTER_API_KEY"
DEFAULT_MODEL = "anthropic/claude-sonnet-4.5"
DEFAULT_TEMPERATURE = 0.3
DEFAULT_MAX_TOKENS = 1500
DEFAULT_TIMEOUT_S = 120

# Query generation limits (to avoid token overuse)
MAX_CONTEXT_LENGTH = 3000
MAX_DIFF_LENGTH = 12000
MAX_QUERIES_PER_EXPERIMENT = 20

# Search configuration
DEFAULT_SEARCH_LIMIT = 5  # Top K memories per query

# Default paths
DEFAULT_DB_PATH = "data/phase0/memories/memories.db"
DEFAULT_TEST_CASES_DIR = "data/phase0/test_cases"
DEFAULT_RESULTS_DIR = "data/phase0/results"

# Batch execution configuration
DEFAULT_SLEEP_BETWEEN_EXPERIMENTS = 1.0  # seconds


def _call_openrouter(
    api_key: str,
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    timeout_s: int = DEFAULT_TIMEOUT_S,
) -> str:
    """
    Call OpenRouter API to generate LLM response.

    Sends a chat completion request to OpenRouter's unified LLM API.

    Args:
        api_key: OpenRouter API key.
        model: Model identifier (e.g., "anthropic/claude-sonnet-4.5").
        messages: List of message dicts with "role" and "content" keys.
        temperature: Sampling temperature (0.0-1.0). Lower = more deterministic.
        max_tokens: Maximum tokens in response.
        timeout_s: Request timeout in seconds.

    Returns:
        Response content as string.

    Raises:
        requests.HTTPError: If API returns error status.
        requests.Timeout: If request exceeds timeout.

    Example:
        >>> messages = [{"role": "user", "content": "Hello"}]
        >>> response = _call_openrouter(api_key, "anthropic/claude-sonnet-4.5", messages)
    """
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost",
        "X-Title": "memory-retrieval-research",
    }
    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    r = requests.post(OPENROUTER_URL, headers=headers, json=payload, timeout=timeout_s)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"].strip()


def _build_query_generation_prompt(context: str, filtered_diff: str) -> List[Dict[str, str]]:
    """
    Build prompt for LLM to generate search queries from PR context.

    Creates a two-message prompt (system + user) that instructs the LLM to
    analyze a code review and generate keyword search queries that would
    retrieve relevant engineering memories.

    Args:
        context: PR context including description, requirements, and notes.
        filtered_diff: Code diff with build artifacts removed.

    Returns:
        List of message dicts suitable for chat completion API.
        Format: [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]

    Note:
        Truncates inputs if they exceed MAX_CONTEXT_LENGTH or MAX_DIFF_LENGTH
        to avoid token limit issues.
    """
    # Truncate inputs to avoid token limits
    if len(context) > MAX_CONTEXT_LENGTH:
        context = context[:MAX_CONTEXT_LENGTH] + "\n... (truncated)"
    if len(filtered_diff) > MAX_DIFF_LENGTH:
        filtered_diff = filtered_diff[:MAX_DIFF_LENGTH] + "\n... (truncated)"

    system = """Generate search queries to retrieve relevant code review patterns from a vector database.

QUERY GENERATION STRATEGY:

1. Identify the code structure in the diff:
   - Is it a test file, mapper, service, validator, helper, model?
   - What methods/functions are being changed?

2. Identify technical patterns (NOT business logic):
   - Type patterns: optional, nullable, union types, generics
   - Logic patterns: conditionals, loops, error handling
   - Test patterns: edge cases, coverage gaps, assertion issues
   - API patterns: field changes, breaking changes, versioning

3. Identify specific gaps or issues:
   - What's missing? Use phrases like "missing test", "no validation", "lacks error handling"
   - What's inconsistent? Use phrases like "handles X but not Y"
   - What might break? Use phrases like "breaking change", "external dependency"

4. Generate queries that combine structure + pattern + gap:
   - "test file optional parameter missing edge case"
   - "mapper method undefined vs null handling"
   - "service boolean logic missing test coverage"

QUERY RULES:
- 4-8 words per query
- Include code structure when relevant (test file, mapper, service)
- Use technical terms: optional, undefined, null, nullable, edge case, union type
- Describe gaps: "missing", "lacks", "no", "doesn't handle"
- Avoid business domain terms (replace with generic technical terms)
- Generate 5-8 queries from different angles

OUTPUT: JSON array of query strings only.

EXAMPLE OUTPUT:
["test file optional parameter edge case", "mapper method undefined parent object", "missing test completely undefined input", "optional object test coverage nested vs parent", "test suite nullable parameter all scenarios"]"""

    user = f"""PR CONTEXT:
{context}

CODE DIFF:
{filtered_diff}

Generate search queries to retrieve relevant engineering memories. Return ONLY a JSON array of query strings."""

    return [{"role": "system", "content": system}, {"role": "user", "content": user}]


def _parse_queries_from_response(response: str) -> List[str]:
    """
    Extract query list from LLM response.

    Attempts to parse JSON array from response, with fallback to line-based parsing
    if JSON extraction fails.

    Args:
        response: Raw LLM response string.

    Returns:
        List of query strings, capped at MAX_QUERIES_PER_EXPERIMENT.

    Example:
        >>> response = '["query one", "query two"]'
        >>> queries = _parse_queries_from_response(response)
        >>> print(queries)
        ['query one', 'query two']

    Note:
        If JSON parsing fails, falls back to line-by-line parsing with
        basic cleaning (removes bullets, quotes, etc.).
    """
    # Try to parse JSON array
    try:
        match = re.search(r"\[.*\]", response, re.DOTALL)
        if match:
            return json.loads(match.group())[:MAX_QUERIES_PER_EXPERIMENT]
    except json.JSONDecodeError:
        pass

    # Fallback: parse line-by-line
    queries = []
    for line in response.split("\n"):
        # Clean line: remove bullets, quotes, whitespace
        line = line.strip().strip("-").strip("*").strip('"').strip()
        if line and len(line) > 3:
            queries.append(line)

    return queries[:MAX_QUERIES_PER_EXPERIMENT]


def run_experiment(
    test_case_path: str,
    db_path: str = DEFAULT_DB_PATH,
    model: str = DEFAULT_MODEL,
    results_dir: str = DEFAULT_RESULTS_DIR,
) -> Dict[str, Any]:
    """
    Run retrieval experiment for a single test case.

    This is the core experiment function that:
    1. Loads test case with pre-computed ground truth
    2. Generates search queries via LLM
    3. Searches memories.db for each query
    4. Calculates recall metrics
    5. Saves detailed results

    Args:
        test_case_path: Path to test case JSON file.
        db_path: Path to SQLite database with FTS5 index.
                 Defaults to "data/phase0_memories/memories.db".
        model: OpenRouter model identifier.
               Defaults to "anthropic/claude-sonnet-4.5".
        results_dir: Directory where result files should be saved.
                     Defaults to "data/phase0_results".

    Returns:
        Dictionary containing:
            - experiment_id: Timestamp-based unique identifier
            - test_case_id: ID from test case file
            - source_file: Original raw PR filename
            - pr_context: Branch names (source -> target)
            - model: Model used for query generation
            - diff_stats: Original and filtered diff lengths
            - ground_truth: Memory IDs and count
            - queries: Detailed results for each query
            - metrics: Recall, query count, retrieval stats
            - retrieved_ground_truth_ids: Successfully retrieved memories
            - missed_ground_truth_ids: Memories not retrieved

    Raises:
        SystemExit: If OPENROUTER_API_KEY environment variable is not set.
        FileNotFoundError: If test case file or database doesn't exist.

    Side Effects:
        - Prints progress information to stdout
        - Saves result JSON to results_dir
        - Makes API calls to OpenRouter (costs money)

    Example:
        >>> results = run_experiment("data/phase0_test_cases/pr_123.json")
        Test case: tc_pr_123
        Ground truth memories: 5
        Generating queries via anthropic/claude-sonnet-4.5...
        Generated 7 queries
        ...
        RECALL: 80.0%
    """
    # Check API key
    api_key = os.getenv(OPENROUTER_API_KEY_ENV)
    if not api_key:
        raise SystemExit(f"Missing {OPENROUTER_API_KEY_ENV} environment variable")

    # Load test case (pre-processed by phase0_build_test_cases.py)
    test_case = load_json(test_case_path)
    context = test_case.get("pr_context", "")
    filtered_diff = test_case.get("filtered_diff", "")
    meta = test_case.get("metadata", {})
    ground_truth_ids = set(test_case.get("ground_truth_memory_ids", []))
    diff_stats = test_case.get("diff_stats", {})

    # Print test case info
    print(f"Test case: {test_case.get('test_case_id', 'unknown')}")
    print(f"Diff stats: {diff_stats.get('original_length', 0)} -> {diff_stats.get('filtered_length', 0)} chars")
    print(f"Ground truth memories: {len(ground_truth_ids)}")

    # Generate search queries via LLM
    print(f"Generating queries via {model}...")
    prompt = _build_query_generation_prompt(context, filtered_diff)
    response = _call_openrouter(api_key, model, prompt)
    queries = _parse_queries_from_response(response)
    print(f"Generated {len(queries)} queries")

    # Execute each query against FTS5 database
    all_retrieved_ids: Set[str] = set()
    query_results = []

    for query in queries:
        results = search_memories(db_path, query, limit=DEFAULT_SEARCH_LIMIT)
        retrieved_ids = {r["id"] for r in results}
        all_retrieved_ids.update(retrieved_ids)

        # Store detailed query results
        query_results.append({
            "query": query,
            "result_count": len(results),
            "results": [
                {
                    "id": r["id"],
                    "rank": r["rank"],
                    "situation": r["situation_description"][:100],  # Truncate for readability
                    "is_ground_truth": r["id"] in ground_truth_ids,
                }
                for r in results
            ],
        })

    # Calculate recall: (retrieved ground truth) / (total ground truth)
    retrieved_ground_truth = all_retrieved_ids & ground_truth_ids
    recall = len(retrieved_ground_truth) / len(ground_truth_ids) if ground_truth_ids else 0.0

    # Build experiment results
    results = {
        "experiment_id": f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "test_case_id": test_case.get("test_case_id", "unknown"),
        "source_file": test_case.get("source_file", "unknown"),
        "pr_context": f"{meta.get('sourceBranch', '?')} -> {meta.get('targetBranch', '?')}",
        "model": model,
        "diff_stats": diff_stats,
        "ground_truth": {
            "memory_ids": sorted(list(ground_truth_ids)),
            "count": len(ground_truth_ids),
        },
        "queries": query_results,
        "metrics": {
            "total_queries": len(queries),
            "total_unique_retrieved": len(all_retrieved_ids),
            "ground_truth_retrieved": len(retrieved_ground_truth),
            "recall": round(recall, 4),
        },
        "retrieved_ground_truth_ids": sorted(list(retrieved_ground_truth)),
        "missed_ground_truth_ids": sorted(list(ground_truth_ids - all_retrieved_ids)),
    }

    # Save results to JSON file
    Path(results_dir).mkdir(parents=True, exist_ok=True)
    results_path = Path(results_dir) / f"results_{test_case.get('test_case_id', 'unknown')}_{results['experiment_id']}.json"
    save_json(results, results_path)

    # Print summary to console
    print("\n" + "=" * 60)
    print(f"EXPERIMENT RESULTS: {test_case.get('test_case_id', 'unknown')}")
    print("=" * 60)
    print(f"Source: {test_case.get('source_file', 'unknown')}")
    print(f"PR: {results['pr_context']}")
    print(f"Model: {model}")
    print(f"Queries generated: {len(queries)}")
    print(f"Ground truth memories: {len(ground_truth_ids)}")
    print(f"Retrieved (unique): {len(all_retrieved_ids)}")
    print(f"Ground truth retrieved: {len(retrieved_ground_truth)}")
    print(f"\nRECALL: {recall:.1%}")
    print("=" * 60)

    # Print missed memories (helpful for debugging poor recall)
    if ground_truth_ids - all_retrieved_ids:
        print(f"\nMissed {len(ground_truth_ids - all_retrieved_ids)} memories:")
        for mid in sorted(ground_truth_ids - all_retrieved_ids):
            print(f"  - {mid}")

    print(f"\nResults saved to: {results_path}")
    return results


def run_all_experiments(
    test_cases_dir: str = DEFAULT_TEST_CASES_DIR,
    db_path: str = DEFAULT_DB_PATH,
    model: str = DEFAULT_MODEL,
    results_dir: str = DEFAULT_RESULTS_DIR,
    sleep_between: float = DEFAULT_SLEEP_BETWEEN_EXPERIMENTS,
) -> List[Dict[str, Any]]:
    """
    Run experiments for all test cases in directory.

    Batch execution function that processes all test cases and computes
    aggregate statistics across experiments.

    Args:
        test_cases_dir: Directory containing test case JSON files.
                        Defaults to "data/phase0_test_cases".
        db_path: Path to SQLite database with FTS5 index.
                 Defaults to "data/phase0_memories/memories.db".
        model: OpenRouter model identifier.
               Defaults to "anthropic/claude-sonnet-4.5".
        results_dir: Directory where result files should be saved.
                     Defaults to "data/phase0_results".
        sleep_between: Seconds to sleep between experiments (rate limiting).
                       Defaults to 1.0 second.

    Returns:
        List of result dictionaries from each experiment.
        Failed experiments have {"test_case_file": "...", "error": "..."} structure.

    Side Effects:
        - Runs multiple experiments (can take significant time)
        - Makes many API calls (costs money)
        - Prints progress and summary to stdout
        - Saves multiple result files

    Example:
        >>> all_results = run_all_experiments()
        [1/5] Processing pr_123.json
        ...
        OVERALL SUMMARY
        ============================================================
        Experiments run: 5
        Total ground truth memories: 20
        Total retrieved: 15
        Average recall: 75.0%
    """
    test_case_files = sorted(Path(test_cases_dir).glob("*.json"))
    all_results = []

    for i, tc_file in enumerate(test_case_files):
        print(f"\n[{i+1}/{len(test_case_files)}] Processing {tc_file.name}")

        try:
            result = run_experiment(
                str(tc_file),
                db_path=db_path,
                model=model,
                results_dir=results_dir,
            )
            all_results.append(result)
        except Exception as e:
            # Log error but continue with remaining test cases
            print(f"Error processing {tc_file.name}: {e}")
            all_results.append({"test_case_file": tc_file.name, "error": str(e)})

        # Rate limiting: sleep between experiments
        if i < len(test_case_files) - 1:
            time.sleep(sleep_between)

    # Print aggregate summary
    print("\n" + "=" * 60)
    print("OVERALL SUMMARY")
    print("=" * 60)

    successful = [r for r in all_results if "metrics" in r]
    if successful:
        avg_recall = sum(r["metrics"]["recall"] for r in successful) / len(successful)
        total_gt = sum(r["ground_truth"]["count"] for r in successful)
        total_retrieved = sum(r["metrics"]["ground_truth_retrieved"] for r in successful)

        print(f"Experiments run: {len(successful)}")
        print(f"Total ground truth memories: {total_gt}")
        print(f"Total retrieved: {total_retrieved}")
        print(f"Average recall: {avg_recall:.1%}")
    else:
        print("No successful experiments")

    if len(successful) < len(all_results):
        print(f"Failed experiments: {len(all_results) - len(successful)}")

    return all_results


if __name__ == "__main__":
    import sys

    # Command-line interface for running experiments
    if len(sys.argv) < 2 or sys.argv[1] in ["--help", "-h"]:
        print("Phase 0 Retrieval Experiment Runner")
        print()
        print("Usage:")
        print("  uv run python scripts/phase0_experiment.py <test-case.json>")
        print("  uv run python scripts/phase0_experiment.py --all")
        print("  uv run python scripts/phase0_experiment.py --help")
        print()
        print("Description:")
        print("  Runs memory retrieval experiments using FTS5 keyword search with BM25 ranking.")
        print("  Measures recall against pre-computed ground truth memory IDs.")
        print()
        print("Requirements:")
        print(f"  - {OPENROUTER_API_KEY_ENV} environment variable (for LLM API access)")
        print(f"  - Test cases in {DEFAULT_TEST_CASES_DIR}/ (generated by phase0_build_test_cases.py)")
        print(f"  - Memory database at {DEFAULT_DB_PATH} (built by phase0_sqlite_fts.py)")
        print()
        print("Options:")
        print("  <test-case.json>  Run single experiment on specified test case")
        print("  --all             Run experiments on all test cases in directory")
        print("  --help, -h        Show this help message")
        print()
        print("Output:")
        print(f"  Result files saved to {DEFAULT_RESULTS_DIR}/results_*.json")
        print("  Each result includes queries, retrieved memories, and recall metrics")
        print()
        print("Example:")
        print("  export OPENROUTER_API_KEY=sk-or-...")
        print("  uv run python scripts/phase0_experiment.py data/phase0_test_cases/pr_123.json")
        sys.exit(0)

    if sys.argv[1] == "--all":
        run_all_experiments()
    else:
        run_experiment(sys.argv[1])
