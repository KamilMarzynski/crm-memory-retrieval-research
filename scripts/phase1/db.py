"""
SQLite + sqlite-vec database operations for Phase 1 memory retrieval.

This module provides a SQLite-based vector search system for engineering
memories extracted from code reviews. It uses sqlite-vec for vector similarity
search with embeddings generated by a local Ollama model.

Key Features:
    - Vector embeddings via Ollama (mxbai-embed-large, 1024 dimensions)
    - sqlite-vec for efficient vector similarity search
    - Cosine distance ranking (lower = more similar)

Database Structure:
    - memories: Base table storing all memory fields
    - vec_memories: vec0 virtual table for vector similarity search

Usage:
    # Rebuild database from JSONL files (uses latest run)
    uv run python scripts/phase1/db.py --rebuild

    # Use specific run
    uv run python scripts/phase1/db.py --rebuild --run-id run_20260208_143022

    # Programmatic usage
    from phase1.db import search_memories
    results = search_memories("path/to/memories.db", "async function", limit=5)
"""

import json
import sqlite3
import struct
import sys
from contextlib import contextmanager
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional

import ollama
import sqlite_vec

# Add scripts directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from phase1.load_memories import (
    load_memories,
    FIELD_ID,
    FIELD_SITUATION,
    FIELD_LESSON,
    FIELD_METADATA,
    FIELD_SOURCE,
    FIELD_DISTANCE,
)
from common.runs import get_latest_run, get_run, update_run_status, PHASE1

# Ollama configuration
OLLAMA_HOST = None  # Set to custom host URL (e.g., "http://localhost:11434") or None for default
OLLAMA_MODEL = "mxbai-embed-large"
VECTOR_DIMENSIONS = 1024

# Initialize Ollama client (uses default host if OLLAMA_HOST is None)
_ollama_client = ollama.Client(host=OLLAMA_HOST) if OLLAMA_HOST else None

# Default database path (legacy, prefer using run-based paths)
DEFAULT_DB_PATH = "data/phase1/memories/memories.db"
DEFAULT_MEMORIES_DIR = "data/phase1/memories"


def _serialize_f32(vector: List[float]) -> bytes:
    """Serialize a list of floats into raw bytes for sqlite-vec."""
    return struct.pack(f"{len(vector)}f", *vector)


def get_embedding(text: str) -> List[float]:
    """
    Get embedding vector from Ollama for a given text.

    Args:
        text: Text to embed.

    Returns:
        List of floats representing the embedding vector.

    Raises:
        ollama.ResponseError: If Ollama API call fails.
    """
    client = _ollama_client or ollama
    response = client.embed(model=OLLAMA_MODEL, input=text)
    return response["embeddings"][0]


def _load_sqlite_vec(conn: sqlite3.Connection) -> None:
    """Load the sqlite-vec extension into a connection."""
    conn.enable_load_extension(True)
    sqlite_vec.load(conn)
    conn.enable_load_extension(False)


@contextmanager
def get_db_connection(db_path: str) -> Iterator[sqlite3.Connection]:
    """
    Context manager for database connections with sqlite-vec loaded.

    Args:
        db_path: Path to SQLite database file.

    Yields:
        SQLite connection object with sqlite-vec extension loaded.
    """
    conn = sqlite3.connect(db_path)
    _load_sqlite_vec(conn)
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def _serialize_json_field(data: Optional[Dict[str, Any]]) -> str:
    """Serialize dictionary to JSON string for database storage."""
    return json.dumps(data or {}, ensure_ascii=False)


def _deserialize_json_field(json_str: Optional[str]) -> Dict[str, Any]:
    """Deserialize JSON string from database to dictionary."""
    if not json_str:
        return {}
    return json.loads(json_str)


def create_database(db_path: str) -> None:
    """
    Create SQLite database with memories table and sqlite-vec vector index.

    Sets up:
    1. Base table (memories) for storing structured memory data
    2. vec0 virtual table (vec_memories) for vector similarity search

    Args:
        db_path: Path where the SQLite database file should be created.
                 Any existing database at this path will be replaced.
    """
    with get_db_connection(db_path) as conn:
        cur = conn.cursor()

        cur.execute("DROP TABLE IF EXISTS vec_memories")
        cur.execute("DROP TABLE IF EXISTS memories")

        cur.execute(f"""
            CREATE TABLE memories (
                {FIELD_ID} TEXT PRIMARY KEY,
                {FIELD_SITUATION} TEXT NOT NULL,
                {FIELD_LESSON} TEXT NOT NULL,
                {FIELD_METADATA} TEXT,
                {FIELD_SOURCE} TEXT
            )
        """)

        cur.execute(f"""
            CREATE VIRTUAL TABLE vec_memories USING vec0(
                memory_id TEXT PRIMARY KEY,
                situation_embedding float[{VECTOR_DIMENSIONS}]
            )
        """)


def insert_memories(db_path: str, memories: List[Dict[str, Any]]) -> int:
    """
    Insert memories into the database with vector embeddings.

    For each memory, generates an embedding of the situation_description
    via Ollama and stores it in the vec_memories table alongside the
    full memory data in the memories table.

    Args:
        db_path: Path to SQLite database file.
        memories: List of memory dictionaries to insert.

    Returns:
        Count of successfully inserted records.
    """
    with get_db_connection(db_path) as conn:
        cur = conn.cursor()
        inserted = 0

        for i, mem in enumerate(memories):
            mem_id = mem[FIELD_ID]
            situation = mem[FIELD_SITUATION]

            try:
                print(f"  [{i + 1}/{len(memories)}] Embedding {mem_id}...")
                embedding = get_embedding(situation)

                cur.execute(
                    f"""
                    INSERT OR REPLACE INTO memories
                    ({FIELD_ID}, {FIELD_SITUATION}, {FIELD_LESSON}, {FIELD_METADATA}, {FIELD_SOURCE})
                    VALUES (?, ?, ?, ?, ?)
                    """,
                    (
                        mem_id,
                        situation,
                        mem[FIELD_LESSON],
                        _serialize_json_field(mem.get(FIELD_METADATA)),
                        _serialize_json_field(mem.get(FIELD_SOURCE)),
                    ),
                )

                cur.execute(
                    """
                    INSERT OR REPLACE INTO vec_memories (memory_id, situation_embedding)
                    VALUES (?, ?)
                    """,
                    (mem_id, _serialize_f32(embedding)),
                )

                inserted += 1
            except (KeyError, sqlite3.Error, ollama.ResponseError) as e:
                print(f"  Warning: Skipping memory {mem_id}: {e}")

    return inserted


def get_memory_count(db_path: str) -> int:
    """
    Get the total number of memories stored in the database.

    Args:
        db_path: Path to SQLite database file.

    Returns:
        Total count of memory records in the database.
    """
    with get_db_connection(db_path) as conn:
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM memories")
        return cur.fetchone()[0]


def get_sample_memories(db_path: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Get a sample of memories from the database (for inspection/debugging).

    Args:
        db_path: Path to SQLite database file.
        limit: Number of memories to retrieve.

    Returns:
        List of memory dictionaries.
    """
    with get_db_connection(db_path) as conn:
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()

        cur.execute(
            f"""
            SELECT {FIELD_ID}, {FIELD_SITUATION}, {FIELD_LESSON},
                   {FIELD_METADATA}, {FIELD_SOURCE}
            FROM memories
            LIMIT ?
            """,
            (limit,),
        )

        results = []
        for row in cur.fetchall():
            results.append({
                FIELD_ID: row[FIELD_ID],
                FIELD_SITUATION: row[FIELD_SITUATION],
                FIELD_LESSON: row[FIELD_LESSON],
                FIELD_METADATA: _deserialize_json_field(row[FIELD_METADATA]),
                FIELD_SOURCE: _deserialize_json_field(row[FIELD_SOURCE]),
            })

        return results


def get_random_sample_memories(db_path: str, n: int = 5) -> List[Dict[str, Any]]:
    """
    Get random sample of memories to include in query generation prompts.

    This grounds the LLM in actual database vocabulary, improving semantic
    alignment between generated queries and stored memories.

    Args:
        db_path: Path to SQLite database file.
        n: Number of random memories to retrieve.

    Returns:
        List of memory dictionaries with id, situation_description, and lesson.
    """
    with get_db_connection(db_path) as conn:
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()

        cur.execute(
            f"""
            SELECT {FIELD_ID}, {FIELD_SITUATION}, {FIELD_LESSON}
            FROM memories
            ORDER BY RANDOM()
            LIMIT ?
            """,
            (n,),
        )

        return [
            {
                FIELD_ID: row[FIELD_ID],
                FIELD_SITUATION: row[FIELD_SITUATION],
                FIELD_LESSON: row[FIELD_LESSON],
            }
            for row in cur.fetchall()
        ]


def get_confidence_from_distance(distance: float) -> str:
    """
    Convert cosine distance to confidence label.

    Calibrated for mxbai-embed-large with cosine distance.
    Uses embedding distance instead of expensive LLM calls.

    Args:
        distance: Cosine distance from vector search (lower = more similar).

    Returns:
        Confidence label: "high", "medium", "low", or "very_low".
    """
    if distance < 0.5:
        return "high"      # Strong semantic match
    elif distance < 0.8:
        return "medium"    # Moderate match, likely relevant
    elif distance < 1.2:
        return "low"       # Weak match, may be relevant
    else:
        return "very_low"  # Poor match, likely noise


def filter_results_by_confidence(
    results: List[Dict[str, Any]],
    min_confidence: str = "low",
) -> List[Dict[str, Any]]:
    """
    Filter search results by minimum confidence threshold.

    Args:
        results: List of search result dictionaries with 'distance' field.
        min_confidence: Minimum confidence level to include ("high", "medium",
            "low", "very_low").

    Returns:
        Filtered list of results meeting the confidence threshold.
    """
    confidence_order = ["very_low", "low", "medium", "high"]
    min_idx = confidence_order.index(min_confidence)

    return [
        r for r in results
        if confidence_order.index(get_confidence_from_distance(r[FIELD_DISTANCE])) >= min_idx
    ]


def get_memory_by_id(db_path: str, memory_id: str) -> Optional[Dict[str, Any]]:
    """
    Retrieve a specific memory by its unique identifier.

    Args:
        db_path: Path to SQLite database file.
        memory_id: Unique memory identifier.

    Returns:
        Memory dictionary, or None if not found.
    """
    with get_db_connection(db_path) as conn:
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()

        cur.execute(
            f"""
            SELECT {FIELD_ID}, {FIELD_SITUATION}, {FIELD_LESSON},
                   {FIELD_METADATA}, {FIELD_SOURCE}
            FROM memories
            WHERE {FIELD_ID} = ?
            """,
            (memory_id,),
        )

        row = cur.fetchone()
        if row is None:
            return None

        return {
            FIELD_ID: row[FIELD_ID],
            FIELD_SITUATION: row[FIELD_SITUATION],
            FIELD_LESSON: row[FIELD_LESSON],
            FIELD_METADATA: _deserialize_json_field(row[FIELD_METADATA]),
            FIELD_SOURCE: _deserialize_json_field(row[FIELD_SOURCE]),
        }


def search_memories(
    db_path: str = DEFAULT_DB_PATH,
    query: str = "",
    limit: int = 10,
) -> List[Dict[str, Any]]:
    """
    Search memories using vector similarity on situation_description embeddings.

    Embeds the query via Ollama and performs cosine similarity search using
    sqlite-vec's vec0 virtual table.

    Args:
        db_path: Path to SQLite database file.
        query: Search query string (will be embedded via Ollama).
        limit: Maximum number of results to return (default: 10).

    Returns:
        List of memory dictionaries ordered by similarity (closest first).
        Each dict contains:
            - id: Unique memory identifier
            - situation_description: Situation description string
            - lesson: Actionable guidance
            - metadata: Dict with repo, language, severity, confidence
            - source: Dict with original code review context
            - distance: Cosine distance (lower = more similar)
    """
    query_embedding = get_embedding(query)

    conn = sqlite3.connect(db_path)
    _load_sqlite_vec(conn)
    conn.row_factory = sqlite3.Row

    try:
        cur = conn.cursor()
        cur.execute(
            f"""
            SELECT m.{FIELD_ID}, m.{FIELD_SITUATION}, m.{FIELD_LESSON},
                   m.{FIELD_METADATA}, m.{FIELD_SOURCE},
                   v.distance as {FIELD_DISTANCE}
            FROM vec_memories v
            JOIN memories m ON m.{FIELD_ID} = v.memory_id
            WHERE v.situation_embedding MATCH ?
                AND k = ?
            ORDER BY v.distance
            """,
            (_serialize_f32(query_embedding), limit),
        )

        results = []
        for row in cur.fetchall():
            results.append({
                FIELD_ID: row[FIELD_ID],
                FIELD_SITUATION: row[FIELD_SITUATION],
                FIELD_LESSON: row[FIELD_LESSON],
                FIELD_METADATA: _deserialize_json_field(row[FIELD_METADATA]),
                FIELD_SOURCE: _deserialize_json_field(row[FIELD_SOURCE]),
                FIELD_DISTANCE: row[FIELD_DISTANCE],
            })

        return results

    finally:
        conn.close()


def rebuild_database(
    db_path: str = DEFAULT_DB_PATH,
    memories_dir: str = DEFAULT_MEMORIES_DIR,
) -> None:
    """
    Rebuild the entire database from scratch using JSONL source files.

    Args:
        db_path: Path where SQLite database should be created.
        memories_dir: Directory containing memories_*.jsonl files.
    """
    print(f"Creating database at {db_path}...")
    create_database(db_path)

    print(f"Loading memories from {memories_dir}...")
    memories = load_memories(memories_dir)
    print(f"Found {len(memories)} memories")

    print("Inserting memories (generating embeddings via Ollama)...")
    count = insert_memories(db_path, memories)
    print(f"Inserted {count} memories into database")


def _print_usage() -> None:
    """Print CLI usage information."""
    print("SQLite-vec Memory Search Database")
    print()
    print("Usage:")
    print("  uv run python scripts/phase1/db.py --rebuild [--run-id <id>]")
    print("  uv run python scripts/phase1/db.py --search <query> [--limit N] [--run-id <id>]")
    print()
    print("Commands:")
    print("  --rebuild          Rebuild database from memories/*.jsonl files")
    print("  --search <query>   Search memories using vector similarity")
    print()
    print("Options:")
    print("  --run-id <id>      Use specific run (default: latest run)")
    print("  --limit N          Maximum results to return (default: 10)")
    print()
    print("Configuration:")
    print(f"  Ollama host: {OLLAMA_HOST or '(default)'}")
    print(f"  Embedding model: {OLLAMA_MODEL}")
    print(f"  Vector dimensions: {VECTOR_DIMENSIONS}")


def _run_search(query: str, db_path: str, limit: int = 10) -> None:
    """Run a search and print results."""
    print(f"Searching for: {query}")
    print(f"Database: {db_path}")
    print(f"Limit: {limit}")
    print()

    results = search_memories(db_path, query, limit=limit)

    if not results:
        print("No results found.")
        return

    print(f"Found {len(results)} results:\n")
    for i, mem in enumerate(results, 1):
        print(f"[{i}] {mem[FIELD_ID]} (distance: {mem[FIELD_DISTANCE]:.4f})")
        print(f"    Situation: {mem[FIELD_SITUATION]}")
        print(f"    Lesson: {mem[FIELD_LESSON]}")
        print()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="SQLite-vec Memory Search Database",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Rebuild database (uses latest run)
  uv run python scripts/phase1/db.py --rebuild

  # Rebuild database for specific run
  uv run python scripts/phase1/db.py --rebuild --run-id run_20260208_143022

  # Search memories
  uv run python scripts/phase1/db.py --search "async function" --limit 5
        """,
    )
    parser.add_argument(
        "--rebuild",
        action="store_true",
        help="Rebuild database from memories/*.jsonl files",
    )
    parser.add_argument(
        "--search",
        type=str,
        help="Search memories using vector similarity",
    )
    parser.add_argument(
        "--run-id",
        default=None,
        help="Use specific run (default: latest run)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="Maximum results to return (default: 10)",
    )

    args = parser.parse_args()

    if not args.rebuild and not args.search:
        _print_usage()
        sys.exit(1)

    # Determine run directory
    if args.run_id:
        run_dir = get_run(PHASE1, args.run_id)
        print(f"Using run: {args.run_id}")
    else:
        run_dir = get_latest_run(PHASE1)
        print(f"Using latest run: {run_dir.name}")

    memories_dir = str(run_dir / "memories")
    db_path = str(run_dir / "memories" / "memories.db")

    if args.rebuild:
        rebuild_database(db_path=db_path, memories_dir=memories_dir)
        memory_count = get_memory_count(db_path)
        update_run_status(run_dir, "db", {"memory_count": memory_count})
        print(f"\nRun status updated: {run_dir.name}")
    elif args.search:
        _run_search(args.search, db_path, limit=args.limit)
