"""
SQLite + sqlite-vec database operations for Phase 1 memory retrieval.

This module provides a SQLite-based vector search system for engineering
memories extracted from code reviews. It uses sqlite-vec for vector similarity
search with embeddings generated by a local Ollama model.

Key Features:
    - Vector embeddings via Ollama (mxbai-embed-large, 1024 dimensions)
    - sqlite-vec for efficient vector similarity search
    - Cosine distance ranking (lower = more similar)

Database Structure:
    - memories: Base table storing all memory fields
    - vec_memories: vec0 virtual table for vector similarity search

Usage:
    # Rebuild database from JSONL files
    uv run python scripts/phase1/db.py --rebuild

    # Programmatic usage
    from phase1.db import search_memories
    results = search_memories("data/phase1/memories/memories.db", "async function", limit=5)
"""

import json
import sqlite3
import struct
import sys
from contextlib import contextmanager
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional

import requests
import sqlite_vec

# Add scripts directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from phase1.load_memories import (
    load_memories,
    DEFAULT_MEMORIES_DIR,
    FIELD_ID,
    FIELD_SITUATION,
    FIELD_LESSON,
    FIELD_METADATA,
    FIELD_SOURCE,
    FIELD_DISTANCE,
)

# Ollama configuration
OLLAMA_BASE_URL = ""
OLLAMA_MODEL = "mxbai-embed-large"
VECTOR_DIMENSIONS = 1024

# Default database path
DEFAULT_DB_PATH = "data/phase1/memories/memories.db"


def _serialize_f32(vector: List[float]) -> bytes:
    """Serialize a list of floats into raw bytes for sqlite-vec."""
    return struct.pack(f"{len(vector)}f", *vector)


def get_embedding(text: str) -> List[float]:
    """
    Get embedding vector from Ollama for a given text.

    Args:
        text: Text to embed.

    Returns:
        List of floats representing the embedding vector.

    Raises:
        RuntimeError: If Ollama API call fails.
        ValueError: If OLLAMA_BASE_URL is not configured.
    """
    if not OLLAMA_BASE_URL:
        raise ValueError(
            "OLLAMA_BASE_URL is not configured. "
            "Set it in scripts/phase1/db.py before running."
        )

    resp = requests.post(
        f"{OLLAMA_BASE_URL}/api/embed",
        json={"model": OLLAMA_MODEL, "input": text},
        timeout=30,
    )

    if resp.status_code != 200:
        raise RuntimeError(
            f"Ollama API error {resp.status_code}: {resp.text}"
        )

    data = resp.json()
    return data["embeddings"][0]


def _load_sqlite_vec(conn: sqlite3.Connection) -> None:
    """Load the sqlite-vec extension into a connection."""
    conn.enable_load_extension(True)
    sqlite_vec.load(conn)
    conn.enable_load_extension(False)


@contextmanager
def get_db_connection(db_path: str) -> Iterator[sqlite3.Connection]:
    """
    Context manager for database connections with sqlite-vec loaded.

    Args:
        db_path: Path to SQLite database file.

    Yields:
        SQLite connection object with sqlite-vec extension loaded.
    """
    conn = sqlite3.connect(db_path)
    _load_sqlite_vec(conn)
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def _serialize_json_field(data: Optional[Dict[str, Any]]) -> str:
    """Serialize dictionary to JSON string for database storage."""
    return json.dumps(data or {}, ensure_ascii=False)


def _deserialize_json_field(json_str: Optional[str]) -> Dict[str, Any]:
    """Deserialize JSON string from database to dictionary."""
    if not json_str:
        return {}
    return json.loads(json_str)


def create_database(db_path: str) -> None:
    """
    Create SQLite database with memories table and sqlite-vec vector index.

    Sets up:
    1. Base table (memories) for storing structured memory data
    2. vec0 virtual table (vec_memories) for vector similarity search

    Args:
        db_path: Path where the SQLite database file should be created.
                 Any existing database at this path will be replaced.
    """
    with get_db_connection(db_path) as conn:
        cur = conn.cursor()

        cur.execute("DROP TABLE IF EXISTS vec_memories")
        cur.execute("DROP TABLE IF EXISTS memories")

        cur.execute(f"""
            CREATE TABLE memories (
                {FIELD_ID} TEXT PRIMARY KEY,
                {FIELD_SITUATION} TEXT NOT NULL,
                {FIELD_LESSON} TEXT NOT NULL,
                {FIELD_METADATA} TEXT,
                {FIELD_SOURCE} TEXT
            )
        """)

        cur.execute(f"""
            CREATE VIRTUAL TABLE vec_memories USING vec0(
                memory_id TEXT PRIMARY KEY,
                situation_embedding float[{VECTOR_DIMENSIONS}]
            )
        """)


def insert_memories(db_path: str, memories: List[Dict[str, Any]]) -> int:
    """
    Insert memories into the database with vector embeddings.

    For each memory, generates an embedding of the situation_description
    via Ollama and stores it in the vec_memories table alongside the
    full memory data in the memories table.

    Args:
        db_path: Path to SQLite database file.
        memories: List of memory dictionaries to insert.

    Returns:
        Count of successfully inserted records.
    """
    with get_db_connection(db_path) as conn:
        cur = conn.cursor()
        inserted = 0

        for i, mem in enumerate(memories):
            mem_id = mem[FIELD_ID]
            situation = mem[FIELD_SITUATION]

            try:
                print(f"  [{i + 1}/{len(memories)}] Embedding {mem_id}...")
                embedding = get_embedding(situation)

                cur.execute(
                    f"""
                    INSERT OR REPLACE INTO memories
                    ({FIELD_ID}, {FIELD_SITUATION}, {FIELD_LESSON}, {FIELD_METADATA}, {FIELD_SOURCE})
                    VALUES (?, ?, ?, ?, ?)
                    """,
                    (
                        mem_id,
                        situation,
                        mem[FIELD_LESSON],
                        _serialize_json_field(mem.get(FIELD_METADATA)),
                        _serialize_json_field(mem.get(FIELD_SOURCE)),
                    ),
                )

                cur.execute(
                    """
                    INSERT OR REPLACE INTO vec_memories (memory_id, situation_embedding)
                    VALUES (?, ?)
                    """,
                    (mem_id, _serialize_f32(embedding)),
                )

                inserted += 1
            except (KeyError, sqlite3.Error, RuntimeError, ValueError) as e:
                print(f"  Warning: Skipping memory {mem_id}: {e}")

    return inserted


def get_memory_count(db_path: str) -> int:
    """
    Get the total number of memories stored in the database.

    Args:
        db_path: Path to SQLite database file.

    Returns:
        Total count of memory records in the database.
    """
    with get_db_connection(db_path) as conn:
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM memories")
        return cur.fetchone()[0]


def get_memory_by_id(db_path: str, memory_id: str) -> Optional[Dict[str, Any]]:
    """
    Retrieve a specific memory by its unique identifier.

    Args:
        db_path: Path to SQLite database file.
        memory_id: Unique memory identifier.

    Returns:
        Memory dictionary, or None if not found.
    """
    with get_db_connection(db_path) as conn:
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()

        cur.execute(
            f"""
            SELECT {FIELD_ID}, {FIELD_SITUATION}, {FIELD_LESSON},
                   {FIELD_METADATA}, {FIELD_SOURCE}
            FROM memories
            WHERE {FIELD_ID} = ?
            """,
            (memory_id,),
        )

        row = cur.fetchone()
        if row is None:
            return None

        return {
            FIELD_ID: row[FIELD_ID],
            FIELD_SITUATION: row[FIELD_SITUATION],
            FIELD_LESSON: row[FIELD_LESSON],
            FIELD_METADATA: _deserialize_json_field(row[FIELD_METADATA]),
            FIELD_SOURCE: _deserialize_json_field(row[FIELD_SOURCE]),
        }


def search_memories(
    db_path: str = DEFAULT_DB_PATH,
    query: str = "",
    limit: int = 10,
) -> List[Dict[str, Any]]:
    """
    Search memories using vector similarity on situation_description embeddings.

    Embeds the query via Ollama and performs cosine similarity search using
    sqlite-vec's vec0 virtual table.

    Args:
        db_path: Path to SQLite database file.
        query: Search query string (will be embedded via Ollama).
        limit: Maximum number of results to return (default: 10).

    Returns:
        List of memory dictionaries ordered by similarity (closest first).
        Each dict contains:
            - id: Unique memory identifier
            - situation_description: Situation description string
            - lesson: Actionable guidance
            - metadata: Dict with repo, language, severity, confidence
            - source: Dict with original code review context
            - distance: Cosine distance (lower = more similar)
    """
    query_embedding = get_embedding(query)

    conn = sqlite3.connect(db_path)
    _load_sqlite_vec(conn)
    conn.row_factory = sqlite3.Row

    try:
        cur = conn.cursor()
        cur.execute(
            f"""
            SELECT m.{FIELD_ID}, m.{FIELD_SITUATION}, m.{FIELD_LESSON},
                   m.{FIELD_METADATA}, m.{FIELD_SOURCE},
                   v.distance as {FIELD_DISTANCE}
            FROM vec_memories v
            JOIN memories m ON m.{FIELD_ID} = v.memory_id
            WHERE v.situation_embedding MATCH ?
                AND k = ?
            ORDER BY v.distance
            """,
            (_serialize_f32(query_embedding), limit),
        )

        results = []
        for row in cur.fetchall():
            results.append({
                FIELD_ID: row[FIELD_ID],
                FIELD_SITUATION: row[FIELD_SITUATION],
                FIELD_LESSON: row[FIELD_LESSON],
                FIELD_METADATA: _deserialize_json_field(row[FIELD_METADATA]),
                FIELD_SOURCE: _deserialize_json_field(row[FIELD_SOURCE]),
                FIELD_DISTANCE: row[FIELD_DISTANCE],
            })

        return results

    finally:
        conn.close()


def rebuild_database(
    db_path: str = DEFAULT_DB_PATH,
    memories_dir: str = DEFAULT_MEMORIES_DIR,
) -> None:
    """
    Rebuild the entire database from scratch using JSONL source files.

    Args:
        db_path: Path where SQLite database should be created.
        memories_dir: Directory containing memories_*.jsonl files.
    """
    print(f"Creating database at {db_path}...")
    create_database(db_path)

    print(f"Loading memories from {memories_dir}...")
    memories = load_memories(memories_dir)
    print(f"Found {len(memories)} memories")

    print("Inserting memories (generating embeddings via Ollama)...")
    count = insert_memories(db_path, memories)
    print(f"Inserted {count} memories into database")


if __name__ == "__main__":
    if len(sys.argv) < 2 or sys.argv[1] != "--rebuild":
        print("SQLite-vec Memory Search Database Builder")
        print()
        print("Usage:")
        print("  uv run python scripts/phase1/db.py --rebuild")
        print()
        print("Description:")
        print("  Rebuilds the vector search database from memories/*.jsonl files.")
        print("  Generates embeddings via Ollama and stores them in sqlite-vec.")
        print("  This is a destructive operation that replaces the existing database.")
        print()
        print(f"  Database location: {DEFAULT_DB_PATH}")
        print(f"  Source JSONL files: {DEFAULT_MEMORIES_DIR}/memories_*.jsonl")
        print(f"  Ollama URL: {OLLAMA_BASE_URL or '(not configured)'}")
        print(f"  Embedding model: {OLLAMA_MODEL}")
        print(f"  Vector dimensions: {VECTOR_DIMENSIONS}")
        sys.exit(1)

    rebuild_database()
